# Сценарии автоматизации IT-службы с on-premise LLM+RAG

## 📋 Карточки сценариев

---

### **Сценарий 1: Интеллектуальный триаж и диагностика алертов**

**Текущая проблема:**
Дежурные инженеры тратят 15–30 минут на каждый алерт: поиск релевантных метрик в Prometheus, логов в OpenSearch, дашбордов в Grafana, runbook'ов в Confluence. 40% алертов эскалируются на L2/L3 из-за недостатка контекста у L1.

**Как LLM+RAG помогает:**
- **Механизм:** При срабатывании алерта Alertmanager отправляет webhook → LLM-агент автоматически собирает контекст за последние 30–60 минут (метрики, логи, трассы, недавние изменения из Git/Ansible), находит релевантные runbook'и в RAG-базе, формирует первичную диагностику с 3–5 проверочными шагами.
- **Примеры запросов:**
  - "Алерт Redis_High_Latency на prod-redis-03. Какие вероятные причины и что проверить?"
  - "Покажи корреляцию метрик HAProxy и логов Nginx за последние 20 минут по инциденту #4521"
- **Интеграции:** Alertmanager Webhook API, Prometheus HTTP API, OpenSearch Query DSL, Grafana Snapshot API, Git/Ansible inventory.

**Что требуется для реализации:**
- Источники знаний: официальные docs (Redis, Prometheus, Grafana), внутренние runbook'и (Markdown/Confluence export), история инцидентов (PIR).
- API-доступ: Prometheus, Alertmanager, OpenSearch, Grafana (read-only токены).
- Форматы: JSON для метрик, Elasticsearch DSL для логов, Markdown для документации.

**Оценка эффекта:**
- MTTA (Mean Time To Acknowledge): **−40–60%**
- MTTD (Mean Time To Detect root cause): **−35–50%**
- Эскалации L1→L2: **−25–40%**
- SLA по инцидентам: **+5–15%**

**Пример применения:**
Алерт "Ceph OSD nearfull" → ассистент автоматически собирает метрики utilization по OSD, состояния PG, недавние rebalancing операции, находит раздел Ceph docs про управление заполнением, предлагает временные меры (throttle scrub, перераспределение) и план расширения, дает ссылки на внутренний SOP и коммиты последних изменений.

---

### **Сценарий 2: Автоматическая корреляция событий и RCA (Root Cause Analysis)**

**Текущая проблема:**
При сложных мультисервисных инцидентах (цепочка Nginx→HAProxy→Redis→PostgreSQL) инженеры тратят 1–3 часа на построение временной шкалы событий, поиск первопричины и понимание, что изменилось.

**Как LLM+RAG помогает:**
- **Механизм:** Агрегирует таймлайн из алертов (Alertmanager), пиков метрик (Prometheus), аномалий в логах (OpenSearch), событий изменений (Ansible, Git commits), трасс (OpenTelemetry), строит граф зависимостей, выдвигает 3–5 гипотез с подтверждающими/опровергающими фактами.
- **Примеры запросов:**
  - "Почему выросли 5xx ошибки в HAProxy с 14:35? Проверь Nginx upstream, Redis connection pool, Postgres locks."
  - "Построй временную шкалу инцидента #8734 и найди корреляции между изменениями в конфигах и алертами."
- **Интеграции:** Prometheus, OpenSearch, Dataprepper, OTel Collector, Grafana Annotations, Ansible Tower/AWX API, Git webhooks.

**Что требуется для реализации:**
- Единый индекс событий (метрики + логи + изменения) с временными метками.
- Доступ к query API всех систем наблюдаемости.
- Граф зависимостей сервисов (service mesh map или ручная карта).
- RAG-база: архитектурные схемы, описания взаимосвязей компонентов.

**Оценка эффекта:**
- Время на подготовку RCA: **−50–70%**
- Качество расследований: **↑** (больше подтвержденных гипотез)
- Повторные инциденты: **−15–25%** (лучше понимание причин)

**Пример применения:**
Деградация API: LLM обнаруживает всплеск connection refused в Redis → коррелирует с изменением maxclients в конфиге 2 часа назад (commit в Git) → находит лимит в документации Redis → предлагает безопасное увеличение и мониторинг памяти.

---

### **Сценарий 3: Полуавтоматическое исправление через Ansible (Guided Remediation)**

**Текущая проблема:**
Runbook'и существуют, но инженеры теряют время на поиск правильного playbook, подбор параметров, боятся запустить что-то не то. Ручное выполнение команд чревато ошибками.

**Как LLM+RAG помогает:**
- **Механизм:** По сигнатуре алерта/проблемы LLM находит релевантный Ansible playbook в AWX/Tower, подставляет параметры на основе контекста (hostname, метрики), делает dry-run, показывает diff и предупреждения о рисках, запрашивает подтверждение оператора.
- **Примеры запросов:**
  - "Подготовь плейбук для рестарта PostgreSQL-реплики pg-replica-02 с проверкой lag и валидацией."
  - "Запусти в check-mode плейбук очистки диска на ceph-osd-05, покажи что будет удалено."
- **Интеграции:** AWX/Ansible Tower API, Ansible inventory, Git (playbooks), RBAC/Keycloak.

**Что требуется для реализации:**
- Каталог playbook'ов с метаданными (назначение, риск, время выполнения, откат).
- Политики OPA (Open Policy Agent) для контроля допустимых действий.
- Approval gates (подтверждение человеком перед apply).

**Оценка эффекта:**
- Ручной труд на ремедиацию: **−30–50%**
- MTTR (Mean Time To Repair): **−25–40%**
- Ошибки при выполнении: **−40–60%** (благодаря dry-run и валидациям)

**Пример применения:**
RabbitMQ queue growth → LLM выбирает playbook scale_rabbitmq_consumers.yml, предлагает увеличить потребителей на 2 инстанса, показывает dry-run diff, после подтверждения запускает плейбук и мониторит метрики queue depth.

---

### **Сценарий 4: Автогенерация Post-Incident Report (PIR)**

**Текущая проблема:**
Написание постмортемов отнимает 2–4 часа на инцидент, часто откладывается, факты забываются, качество неравномерное.

**Как LLM+RAG помогает:**
- **Механизм:** Автоматически собирает таймлайн из Alertmanager, Grafana annotations, логов OpenSearch, чатов (Mattermost/Slack export), изменений (Git commits, Ansible runs), формирует структурированный черновик PIR (что случилось, impact, timeline, root cause, remediation, follow-ups) с гиперссылками на метрики/дашборды/коммиты.
- **Примеры запросов:**
  - "Сгенерируй PIR для инцидента #0142 с 12:10 до 13:05, добавь ссылки на панели и timeline."
  - "Предложи action items для PIR по деградации Redis на основе анализа причин."
- **Интеграции:** Alertmanager, Prometheus, OpenSearch, Grafana, Git, Mattermost API.

**Что требуется для реализации:**
- Шаблон PIR (Markdown/Confluence).
- Доступ к истории алертов, метрик, логов, коммуникаций.
- Теггирование инцидентов (ID инцидента, временное окно).

**Оценка эффекта:**
- Время на подготовку PIR: **−60–80%**
- Качество документирования: **↑** (полнота, структура)
- База знаний: **↑** (больше PIR → лучше RAG для будущих инцидентов)

**Пример применения:**
После инцидента с Redis OOM (Out Of Memory) LLM автоматически собирает график памяти из Grafana, лог eviction из OpenSearch, коммит с изменением maxmemory-policy, формирует черновик с рекомендациями по настройке eviction и мониторингу.

---

### **Сценарий 5: Ассистент по конфигурациям Nginx/HAProxy**

**Текущая проблема:**
Конфигурационные ошибки в Nginx/HAProxy (неправильные директивы TLS, upstream health checks, sticky sessions) вызывают инциденты. Ревью конфигов занимает много времени, смена версий ломает устаревшие опции.

**Как LLM+RAG помогает:**
- **Механизм:** Анализирует текущие конфиги, объясняет директивы, предлагает улучшения (безопасность, производительность), генерирует патчи (diff), валидирует против официальной документации и корпоративных гайдлайнов (TLS versions, ciphers, headers).
- **Примеры запросов:**
  - "Сгенерируй HAProxy конфиг для mTLS к backend с circuit breaker. Покажи diff и риски."
  - "Проверь nginx.conf на безопасность: HSTS, OCSP stapling, запрещенные ciphers, client body limits."
  - "Адаптируй конфиг HAProxy 2.4 для версии 2.8, покажи deprecated директивы."
- **Интеграции:** Git (конфиги), Nginx/HAProxy docs, линтеры (nginx -t, haproxy -c), CI/CD pipelines.

**Что требуется для реализации:**
- Репозиторий конфигов в Git.
- Парсеры конфигов (nginx-config-parser, haproxy-config).
- Корпоративные baseline-политики (например, Mozilla SSL Config Generator).

**Оценка эффекта:**
- Конфигурационные ошибки: **−30–50%**
- Время ревью конфигов: **−40–60%**
- Инциденты из-за неправильных конфигов: **−20–35%**

**Пример применения:**
Миграция на HTTP/2: LLM генерирует конфиг Nginx с правильными ALPN, server_tokens off, HSTS, безопасными ciphersuites (Mozilla Modern), показывает diff и предлагает план тестирования (проверка HTTP/2 в curl, метрики latency).

---

### **Сценарий 6: Тюнинг производительности баз данных (PostgreSQL/Redis/RabbitMQ)**

**Текущая проблема:**
Оптимизация параметров БД требует глубокой экспертизы. Неправильные настройки приводят к деградации или OOM. Инженеры боятся менять параметры без уверенности.

**Как LLM+RAG помогает:**
- **Механизм:** Анализирует метрики производительности (Prometheus exporters), статистику запросов (pg_stat_statements, Redis slowlog, RabbitMQ queue stats), находит узкие места, предлагает безопасные изменения конфигурации с обоснованием из документации, расчетами и планом rollback.
- **Примеры запросов:**
  - "Где узкое место в PostgreSQL prod-db-01? Дай TOP-10 медленных запросов и рекомендации по work_mem, shared_buffers, autovacuum."
  - "Redis prod-cache-02 имеет высокую latency. Проверь slowlog, memory fragmentation, eviction. Предложи тюнинг."
  - "RabbitMQ queue 'tasks' растет. Проанализируй consumer utilization, prefetch, memory alarms."
- **Интеграции:** Prometheus (postgres_exporter, redis_exporter, rabbitmq_exporter), PostgreSQL pg_stat APIs, Redis INFO/SLOWLOG, RabbitMQ Management API.

**Что требуется для реализации:**
- Доступ к статистике БД (read-only).
- Runbook'и по тюнингу (PostgreSQL wiki, Redis best practices, RabbitMQ tuning).
- Политики безопасных изменений (не более X% от текущего значения, требуется тестирование).

**Оценка эффекта:**
- Улучшение latency p95: **10–30%**
- Алерты по производительности: **−20–40%**
- Время на анализ и тюнинг: **−50–70%**

**Пример применения:**
PostgreSQL медленные запросы: LLM анализирует pg_stat_statements, находит запросы с высоким mean_exec_time, предлагает создание индексов, увеличение work_mem для сортировок, настройку autovacuum для горячих таблиц, генерирует SQL для создания индексов и план проверки (EXPLAIN ANALYZE).

---

### **Сценарий 7: Планирование обновлений и проверка совместимости**

**Текущая проблема:**
Подготовка к обновлению major/minor версий (PostgreSQL 14→15, Ceph Quincy→Reef, Redis 6→7) занимает дни: чтение release notes, проверка breaking changes, deprecated опций, совместимости расширений. Риск пропустить критичное изменение.

**Как LLM+RAG помогает:**
- **Механизм:** Сверяет текущие версии компонентов (из Ansible inventory/CMDB) с целевыми, парсит release notes, находит breaking changes, deprecated features, новые требования (ОС, либы), создает чек-лист pre-upgrade checks, upgrade sequence, post-upgrade validation.
- **Примеры запросов:**
  - "Подготовь план upgrade PostgreSQL 14.5 → 15.3 для кластера prod-db. Какие расширения проверить? Что изменилось в конфигах?"
  - "Что нужно сделать перед upgrade Ceph Quincy→Reef? Какая последовательность upgrade (mon→mgr→osd)?"
  - "Redis 6.2 → 7.0: какие deprecated команды мы используем? Проверь наши конфиги."
- **Интеграции:** Ansible inventory, CMDB, официальные release notes/changelogs, матрицы совместимости.

**Что требуется для реализации:**
- Инвентарь версий компонентов (автоматический сбор через Ansible facts).
- Парсинг конфигов (поиск deprecated директив).
- RAG-база: release notes, upgrade guides, матрицы совместимости.

**Оценка эффекта:**
- Время подготовки к upgrade: **−40–60%**
- Откаты после upgrade: **−30–50%** (меньше пропущенных несовместимостей)
- Простои на upgrade: **−15–25%** (четкий план)

**Пример применения:**
Upgrade PostgreSQL 14→15: LLM находит изменение в pg_stat_statements (новые колонки), предупреждает о deprecated параметре vacuum_defer_cleanup_age, проверяет расширения (pg_stat_kcache совместимо, TimescaleDB требует обновления до 2.10+), генерирует чек-лист (бэкап, проверка репликации lag, тестирование запросов на staging, план rollback).

---

### **Сценарий 8: Снижение шума в алертах и логах**

**Текущая проблема:**
Высокий объем дублирующихся/ложных алертов и шумных логов отвлекает инженеров, важные сигналы теряются (alert fatigue). Настройка правил алертинга и фильтров логов — ручная работа.

**Как LLM+RAG помогает:**
- **Механизм:** Кластеризует повторяющиеся сообщения логов и алертов, объясняет паттерны (например, health-check запросы генерируют 499 в Nginx), предлагает фильтры для OpenSearch, улучшения правил алертинга (inhibition, grouping в Alertmanager), объединяет связанные алерты в один инцидент.
- **Примеры запросов:**
  - "Сгруппируй повторяющиеся ошибки в Nginx access.log за сутки. Предложи фильтры и улучшения лог-формата."
  - "Какие алерты в Alertmanager дублируются или всегда resolved без действий? Предложи правила inhibit."
  - "Найди шумные паттерны в OpenSearch индексе app-logs и создай exclusion filters."
- **Интеграции:** OpenSearch (DSL, aggregations), Alertmanager (config, API), Prometheus (recording rules).

**Что требуется для реализации:**
- Доступ к логам/алертам за длительный период (30–90 дней).
- Механизм обратной связи (инженер отмечает "полезно" / "шум").
- Правила дедупликации и группировки.

**Оценка эффекта:**
- Шумные алерты: **−30–50%**
- Время на просмотр логов/алертов: **−25–40%**
- Скорость обнаружения важных событий: **↑**

**Пример применения:**
Nginx логи содержат тысячи 499 ошибок от Kubernetes liveness probes → LLM группирует их по User-Agent и path, предлагает исключить /healthz из access.log или перевести в отдельный лог-файл, генерирует конфиг map для Nginx.

---

### **Сценарий 9: Генерация дашбордов и правил алертинга (Prometheus/Grafana)**

**Текущая проблема:**
Создание дашбордов Grafana и правил алертинга Prometheus вручную — долго и непоследовательно. Отсутствие стандартов приводит к разнородным панелям и неоптимальным алертам (слишком чувствительные или наоборот).

**Как LLM+RAG помогает:**
- **Механизм:** По описанию сервиса/задачи генерирует PromQL-запросы, JSON-конфиги панелей Grafana, правила Alertmanager, предостерегает от anti-patterns (например, алерты на мгновенные значения вместо rate/increase).
- **Примеры запросов:**
  - "Создай Grafana панель для мониторинга Ceph OSD utilization и алерт при nearfull (>80%) per pool."
  - "Сгенерируй PromQL для p95 latency HAProxy backend по каждому upstream и алерт при превышении 500ms в течение 5 минут."
  - "Дашборд RabbitMQ: consumer lag, queue depth, message rate, memory alarms. Покажи JSON."
- **Интеграции:** Grafana API (dashboard provisioning), Prometheus rule files, Git (infrastructure as code).

**Что требуется для реализации:**
- Каталог доступных метрик (service discovery, метаданные exporters).
- Корпоративные шаблоны дашбордов (baseline для каждого типа сервиса).
- Версии Grafana/Prometheus (для совместимости синтаксиса).

**Оценка эффекта:**
- Время на создание дашборда/алерта: **−50–70%**
- Качество наблюдаемости: **↑** (единые стандарты)
- Ложные/пропущенные алерты: **−20–35%**

**Пример применения:**
Новый сервис на базе Redis: LLM автоматически генерирует дашборд с панелями (ops/sec, latency, memory usage, evictions, hit rate), правила алертинга (latency >100ms 5min, memory >90%, connection refused), экспортирует JSON для Grafana provisioning и YAML для Prometheus rules.

---

### **Сценарий 10: Анализ влияния изменений (Change Impact Analysis)**

**Текущая проблема:**
Изменения в конфигурациях, обновления компонентов часто приводят к неожиданным инцидентам. Риски недооцениваются, тестирование поверхностное.

**Как LLM+RAG помогает:**
- **Механизм:** Читает diff конфигов/кода/Ansible переменных из Pull Request, анализирует потенциальное влияние на зависимые сервисы (на основе service graph и исторических данных), предлагает тестовые сценарии, метрики для мониторинга "до" и "после", план rollback.
- **Примеры запросов:**
  - "Оцени влияние включения HTTP/2 в Nginx для api-gateway. Какие метрики отслеживать? Как быстро откатить?"
  - "Изменение max_connections в PostgreSQL с 200 на 500. Какие риски? Как проверить на staging?"
  - "Diff Ansible vars: увеличен timeout для HAProxy health checks. Что это сломает?"
- **Интеграции:** Git (webhooks на PR), Prometheus (baseline метрик), Grafana (comparison dashboards), Ansible (inventory diff).

**Что требуется для реализации:**
- Интеграция с Git (GitLab/Gitea webhooks).
- Граф зависимостей сервисов.
- Шаблоны change management (ITIL-подобные).

**Оценка эффекта:**
- Инциденты после изменений: **−25–40%**
- Время на ревью изменений: **−30–50%**
- Откаты изменений: **−20–30%**

**Пример применения:**
PR с включением mTLS между HAProxy и Nginx → LLM предупреждает о возможном росте CPU на HAProxy (SSL overhead), предлагает мониторить CPU, TLS handshake latency, connection errors, рекомендует канареечное развертывание (10% трафика), генерирует rollback-команды.

---

### **Сценарий 11: Планирование емкости и прогнозирование (Capacity Planning)**

**Текущая проблема:**
Ручной анализ трендов роста данных/нагрузки, позднее обнаружение приближения лимитов (диск, память, IOPS). Либо покупают избыточное железо, либо сталкиваются с аварийным расширением.

**Как LLM+RAG помогает:**
- **Механизм:** Анализирует исторические тренды метрик (Prometheus, Ceph mgr stats, PostgreSQL размеры БД), строит прогнозы (линейные, сезонные), предлагает capacity plan с временными рамками и бюджетом ресурсов, генерирует алерты на превышение прогнозных threshold.
- **Примеры запросов:**
  - "Спрогнозируй, когда Ceph pool 'images' достигнет nearfull при текущем тренде роста. Дай план расширения (OSD)."
  - "PostgreSQL база растет на X GB/месяц. Когда закончится место? Какие таблицы занимают больше всего?"
  - "Когда Redis достигнет maxmemory при текущем росте ключей? Нужно ли увеличивать память или включить eviction?"
- **Интеграции:** Prometheus (long-term storage, Thanos/VictoriaMetrics), Ceph mgr, PostgreSQL pg_database_size, планировщики (calendar alerts).

**Что требуется для реализации:**
- Доступ к историческим метрикам (минимум 90 дней).
- Политики запасов (headroom, например 20% свободного места).
- SLO по доступности ресурсов.

**Оценка эффекта:**
- Недогруз/перегруз ресурсов: **−15–30%**
- Аварийные расширения: **−40–60%**
- Предсказуемость затрат: **↑**

**Пример применения:**
Ceph кластер: LLM анализирует тренд роста за 6 месяцев, прогнозирует достижение 80% заполнения через 3 месяца, рекомендует добавить 4 OSD (расчет по объему), предлагает план переразметки пулов для равномерной нагрузки.

---

### **Сценарий 12: Проверка безопасности конфигураций (Security Hardening Assistant)**

**Текущая проблема:**
Непоследовательное применение security baseline, пропуски в hardening (слабые TLS ciphers, открытые порты, отсутствие HSTS), сложность поддержания соответствия стандартам (CIS, NIST).

**Как LLM+RAG помогает:**
- **Механизм:** Сверяет конфиги Nginx/HAProxy/PostgreSQL/Redis/RabbitMQ с security baseline (CIS Benchmarks, Mozilla TLS Config, OWASP), находит отклонения, предлагает патчи с обоснованием, генерирует отчеты для аудита.
- **Примеры запросов:**
  - "Проверь Nginx конфиг на соответствие безопасности: TLS 1.2+, безопасные ciphers, HSTS, X-Frame-Options, CSP."
  - "PostgreSQL: проверь аутентификацию (pg_hba.conf), SSL, роли, запрещенные расширения."
  - "HAProxy: валидация mTLS, запрет слабых протоколов, rate limiting, secure headers."
- **Интеграции:** Git (конфиги), линтеры безопасности (ansible-lint, Lynis), корпоративные политики.

**Что требуется для реализации:**
- База стандартов безопасности (CIS, корпоративные политики).
- Доступ к конфигам (read-only Git).
- Механизм предложения patch (PR или issue).

**Оценка эффекта:**
- Несоответствия безопасности: **−40–60%**
- Замечания на аудитах: **−30–50%**
- Риски инцидентов ИБ: **−20–40%**

**Пример применения:**
Nginx конфиг: LLM находит использование TLS 1.1 (deprecated), слабый cipher DHE-RSA-AES128-SHA, отсутствие HSTS и X-Content-Type-Options, генерирует безопасный конфиг (Mozilla Modern profile), предлагает PR с diff и объяснением рисков.

---

### **Сценарий 13: Помощник по восстановлению и DR (Disaster Recovery Assistant)**

**Текущая проблема:**
Процедуры восстановления выполняются редко, инженеры забывают шаги, высокая когнитивная нагрузка в стрессе, ошибки могут усугубить ситуацию.

**Как LLM+RAG помогает:**
- **Механизм:** По типу сбоя (потеря OSD, corruption БД, failover PostgreSQL) выдает пошаговый план восстановления с командами, критериями успеха, валидациями, ссылками на DR-процедуры, предлагает параллельное выполнение шагов (если возможно).
- **Примеры запросов:**
  - "План восстановления PostgreSQL из base backup + WAL-архива для сценария потери primary. Дай команды и проверки."
  - "Потеряны 2 Ceph OSD ноды. Как безопасно восстановить? Какая последовательность, как избежать data loss?"
  - "Redis failover на replica. Как переключить HAProxy backend и проверить консистентность?"
- **Интеграции:** Ansible (DR playbooks), Ceph mgr/CLI, PostgreSQL tools (pg_basebackup, pg_waldump), бэкап-системы (Restic, Borg).

**Что требуется для реализации:**
- Актуальные DR-процедуры (runbook'и, tested playbooks).
- Инвентарь (расположение бэкапов, реплик).
- Тестовый стенд для валидации процедур.

**Оценка эффекта:**
- Время восстановления (RTO): **−30–50%**
- Ошибки при восстановлении: **−40–60%**
- Уверенность инженеров: **↑**

**Пример применения:**
Corruption PostgreSQL primary: LLM предлагает промоут replica в primary (pg_ctl promote), перенаправление приложений через HAProxy, валидацию репликации (SELECT pg_is_in_recovery()), создание нового standby из бэкапа, проверку консистентности (pg_checksums), генерирует Ansible playbook для автоматизации шагов.

---

### **Сценарий 14: Обучение и онбординг инженеров (Training & Knowledge Base)**

**Текущая проблема:**
Долгая адаптация новых инженеров (3–6 месяцев), много повторяющихся вопросов к старшим коллегам, фрагментированная документация (Confluence, Wiki, личные заметки).

**Как LLM+RAG помогает:**
- **Механизм:** QA-ассистент по всему стеку компании и runbook'ам, генерация микро-лабораторий и тренажеров из реальных процедур, квизы для самопроверки, интерактивные туториалы с пояснениями команд.
- **Примеры запросов:**
  - "Объясни новичку pipeline: OpenTelemetry Collector → Dataprepper → OpenSearch. Как добавить новый источник трасс?"
  - "Как работает Prometheus federation в нашей инфраструктуре? Покажи конфиг и дашборды."
  - "Создай лабораторку: развернуть PostgreSQL реплику, настроить streaming replication, проверить lag."
- **Интеграции:** База знаний (Confluence/Wiki export), чаты (Mattermost history), LMS (если есть).

**Что требуется для реализации:**
- Импорт внутренних инструкций, SOP, архитектурных схем.
- Разметка контента по ролям/уровням (L1/L2/L3, junior/senior).
- Сбор FAQ и частых вопросов из чатов.

**Оценка эффекта:**
- Время онбординга: **−30–50%**
- Обращения L1→L3 за помощью: **−25–40%**
- Унификация практик: **↑**

**Пример применения:**
Новый инженер спрашивает "Как понять, почему Redis медленный?" → LLM выдает пошаговую инструкцию: проверить INFO stats (ops/sec, latency), SLOWLOG, memory fragmentation, метрики в Grafana (dashboard link), типичные причины (slowlog команды, fork для RDB, network latency), ссылки на внутренний runbook и Redis docs.

---

### **Сценарий 15: Интеллектуальный поиск по логам и метрикам (Contextual Search)**

**Текущая проблема:**
Поиск в OpenSearch требует знания синтаксиса DSL, инженеры тратят время на подбор фильтров и агрегаций. Трудно найти корреляции между логами разных сервисов.

**Как LLM+RAG помогает:**
- **Механизм:** Преобразует запросы на естественном языке в OpenSearch DSL и PromQL, выполняет поиск, агрегирует результаты, объясняет паттерны, строит корреляции между логами и метриками.
- **Примеры запросов:**
  - "Найди все ошибки 'connection refused' в логах Nginx за последний час, сгруппируй по upstream."
  - "Покажи топ-10 медленных запросов PostgreSQL за сегодня с execution time >1s."
  - "Какие сервисы писали ERROR в логи в промежуток 14:30–15:00? Построй таймлайн."
- **Интеграции:** OpenSearch Query DSL, Prometheus PromQL, Grafana Explore.

**Что требуется для реализации:**
- Индексы OpenSearch с стандартизированными полями (service, level, timestamp, message).
- Шаблоны запросов (query patterns).
- NL2DSL модель (часть LLM).

**Оценка эффекта:**
- Время на поиск в логах/метриках: **−40–60%**
- Барьер входа для L1: **↓** (не нужно учить DSL)
- Качество анализа: **↑** (лучшие запросы)

**Пример применения:**
Инженер: "Покажи все 5xx ошибки HAProxy за последние 30 минут и соответствующие ошибки в логах Nginx" → LLM генерирует OpenSearch query с временным фильтром, service:haproxy, level:error, status:5xx, плюс коррелированный запрос для Nginx по upstream, выдает агрегированную таблицу и график.

---

## 🖥️ Оборудование и стоимость

### Требования
- **Нагрузка:** 10 одновременных запросов × 50 токенов/сек = **500 токенов/сек суммарно**
- **Контекст:** 8k токенов
- **HA:** минимум N+1 (2 активных узла для serving)
- **ИБ:** полностью on-premise, mTLS, изоляция, аудит

### Выбор ПО и моделей

**LLM:**
- **Базовый (минимум):** Qwen2.5-14B-Instruct (Apache 2.0) — отличный баланс качества/скорости, хорошо работает на русском и английском
- **Оптимальный:** Qwen2.5-32B-Instruct (Apache 2.0) + Mixtral-8x7B-Instruct (Apache 2.0) для роутинга сложных/простых запросов
- **Квантование:** AWQ INT4 (для минимизации VRAM и ускорения)

**Embeddings & Rerank:**
- **Embeddings:** bge-m3 (многоязычная, 1024-dim, ONNX для CPU)
- **Reranker:** bge-reranker-v2-m3

**Serving:**
- **LLM:** vLLM (высокая пропускная способность, paged attention, multi-GPU/multi-model support)
- **Load Balancer:** HAProxy (mTLS, health checks)

**RAG:**
- **Vector DB:** OpenSearch (уже в стеке, поддержка kNN/HNSW + BM25 hybrid search)
- **Оркестрация:** Haystack 2.x (модульность, tool calling) или LangGraph (для сложных агентов)

**Безопасность:**
- **SSO/RBAC:** Keycloak
- **Secrets:** HashiCorp Vault
- **Policy:** Open Policy Agent (OPA)
- **UI:** Open WebUI + Mattermost Bot

### Расчет производительности GPU

**NVIDIA L40S 48GB** (оптимальная цена/производительность для инференса):
- Qwen2.5-14B-Instruct AWQ INT4: ~1000–1400 ток/с при 8k контексте
- Qwen2.5-32B-Instruct AWQ INT4: ~500–700 ток/с при 8k контексте
- Mixtral-8x7B-Instruct AWQ INT4: ~700–1000 ток/с при 8k контексте

**NVIDIA A100 80GB** (премиум, +40% производительности):
- Qwen2.5-32B-Instruct AWQ INT4: ~700–1000 ток/с при 8k контексте

### Таблица стоимости оборудования

| **Вариант** | **Компонент** | **Конфигурация** | **Кол-во** | **Производительность** | **Цена за единицу (USD)** | **Итого (USD)** |
|-------------|---------------|------------------|------------|------------------------|---------------------------|-----------------|
| **МИНИМАЛЬНЫЙ** | | | | | | |
| | LLM Serving | 1× L40S 48GB<br>CPU: 32 cores (Xeon/EPYC)<br>RAM: 256GB DDR5<br>SSD: 2× 2TB NVMe<br>NIC: 25GbE<br>ПО: vLLM, Qwen2.5-14B INT4 | **2** | 2000–2800 ток/с<br>(active-active HA) | $16,000–$19,000 | **$32,000–$38,000** |
| | RAG/Embeddings | CPU: 32 cores<br>RAM: 192GB<br>SSD: 2× 1TB NVMe<br>NIC: 10GbE<br>ПО: bge-m3, bge-reranker (CPU/ONNX) | **2** | Инференс эмбеддингов<br>+ rerank (active-standby) | $6,000–$8,000 | **$12,000–$16,000** |
| | OpenSearch | Используется существующий кластер (3+ ноды) | **—** | Vector search + BM25 | $0 (существующий) | **$0** |
| | Сеть/ИБ | HAProxy, Keycloak, Vault, OPA (на существующей инфраструктуре) | **—** | — | $0 (существующее) | **$0** |
| | **ИТОГО МИНИМУМ** | | | **Покрывает 500+ ток/с, HA** | | **$44,000–$54,000** |
| **ОПТИМАЛЬНЫЙ** | | | | | | |
| | LLM Serving (основная модель) | 1× L40S 48GB<br>CPU: 48 cores<br>RAM: 384GB DDR5<br>SSD: 2× 4TB NVMe<br>NIC: 25GbE<br>ПО: vLLM, Qwen2.5-32B INT4 | **2** | 1000–1400 ток/с<br>(active-active) | $18,000–$22,000 | **$36,000–$44,000** |
| | LLM Serving (быстрая модель) | 1× L40S 48GB<br>CPU: 32 cores<br>RAM: 256GB<br>SSD: 2× 2TB NVMe<br>NIC: 25GbE<br>ПО: vLLM, Mixtral-8x7B INT4 | **1** | 700–1000 ток/с<br>(для простых задач) | $16,000–$19,000 | **$16,000–$19,000** |
| | RAG/Embeddings + Rerank | 1× NVIDIA L4 24GB<br>CPU: 32 cores<br>RAM: 192GB<br>SSD: 2× 2TB NVMe<br>NIC: 10GbE<br>ПО: bge-m3 GPU, bge-reranker | **2** | Быстрая индексация<br>+ rerank (active-active) | $8,000–$11,000 | **$16,000–$22,000** |
| | OpenSearch (расширение) | CPU: 32 cores<br>RAM: 256GB<br>SSD: 4× 4TB NVMe (RAID)<br>NIC: 25GbE | **3** | Высокопроизводительный<br>vector search | $8,000–$10,000 | **$24,000–$30,000** |
| | **ИТОГО ОПТИМУМ** | | | **2000–3000+ ток/с, N+1 HA, роутинг запросов** | | **$92,000–$115,000** |

### Комментарии к таблице

**Минимальный вариант ($44k–$54k):**
- ✅ Покрывает требования: 10 запросов × 50 ток/с = 500 ток/с с запасом (~4× overhead)
- ✅ HA: 2 активных LLM-узла (stateless, за HAProxy)
- ✅ Модель Qwen2.5-14B — высокое качество на RU/EN
- ✅ RAG на CPU (достаточно для 10 одновременных пользователей)
- ⚠️ Для пиковых нагрузок может быть тесно

**Оптимальный вариант ($92k–$115k):**
- ✅ Высокое качество: Qwen2.5-32B для сложных задач (RCA, конфиги, тюнинг)
- ✅ Роутинг: Mixtral-8x7B для быстрых ответов (FAQ, поиск)
- ✅ N+1 HA: 3 LLM-узла (2 основных + 1 резерв)
- ✅ GPU-ускорение RAG: быстрая индексация больших объемов доков
- ✅ Расширенный OpenSearch: отдельный высокопроизводительный кластер для векторного поиска
- ✅ Запас на рост (до 20–30 одновременных запросов)

### Альтернатива: NVIDIA A100 80GB (для максимального качества)

Если приоритет — максимальное качество и скорость для сложных моделей:

| Компонент | Конфигурация | Кол-во | Цена за единицу | Итого |
|-----------|--------------|--------|-----------------|-------|
| LLM Serving | 1× A100 80GB, CPU 48c, RAM 512GB, 2× 4TB NVMe, 25GbE | 2 | $28,000–$35,000 | $56,000–$70,000 |
| **ИТОГО (замена L40S на A100)** | Остальное без изменений | | | **$80,000–$100,000 (мин)<br>$130,000–$160,000 (опт)** |

---

## 🔒 Обеспечение HA и ИБ

### Высокая доступность

**LLM Serving:**
- Минимум 2 активных GPU-узла (stateless) за HAProxy
- Health checks (vLLM /health endpoint)
- Канареечные обновления (blue/green deployment)
- Автоматический failover (HAProxy backend switching <2s)

**RAG:**
- 2 инстанса embedding/rerank сервиса (active-active или active-standby)
- OpenSearch: минимум 3 ноды, репликация индексов (replica: 1)
- Снапшоты индексов в Ceph RGW (daily)

**Инфраструктура:**
- HAProxy в HA-режиме (keepalived/VRRP)
- PostgreSQL для метаданных (если нужно) — streaming replication
- Мониторинг всех компонентов через Prometheus + алерты

**SLA:**
- Целевая доступность: **99.5%** (допустимый downtime ~3.6ч/месяц)
- RTO (Recovery Time Objective): **<5 минут** (failover)
- RPO (Recovery Point Objective): **<1 час** (для RAG-индексов)

### Информационная безопасность

**Изоляция:**
- ❌ **Без доступа в Интернет** (air-gapped или строгий egress-контроль)
- Закрытая сеть для LLM/RAG (VLAN/VPC)
- Management plane отдельно от data plane

**Шифрование:**
- 🔐 **mTLS между всеми компонентами** (vLLM ↔ HAProxy ↔ RAG ↔ OpenSearch)
- TLS 1.3, безопасные cipher suites (Mozilla Modern)
- Шифрование на диске (LUKS/dm-crypt для NVMe)

**Аутентификация и авторизация:**
- 🔑 **SSO через Keycloak** (LDAP/AD интеграция)
- **RBAC:** роли L1/L2/L3, разделение прав на инструменты (read-only/execute)
- **API tokens** из Vault с TTL и ротацией

**Секреты:**
- 🗝️ Все секреты в **HashiCorp Vault** (API keys, DB passwords, certificates)
- Автоматическая ротация сертификатов (cert-manager)
- Запрет hardcoded secrets в коде (проверка через gitleaks)

**Политики и контроль:**
- 🛡️ **OPA (Open Policy Agent)** для контроля вызовов инструментов
- Whitelist разрешенных действий (например, только dry-run Ansible без явного approval)
- Rate limiting на API (защита от abuse)

**Аудит:**
- 📝 Полное логирование всех запросов/ответов LLM → OpenSearch (WORM индексы)
- Логирование вызовов инструментов (кто, что, когда, результат)
- Retention: минимум 90 дней, иммутабельные логи (ILM политики)
- SIEM интеграция (алерты на подозрительные паттерны)

**Защита от утечек:**
- 🚫 Запрет на обработку PII (автоматическая детекция и маскирование)
- Тегирование внутреннего контента ("internal_only"), запрет на экспорт
- DLP-правила на выходе из системы

**Обновления:**
- 🔄 Подписанные артефакты (cosign, SBOM)
- Локальный registry для контейнеров и моделей (Harbor/Nexus)
- Сканирование на уязвимости (Trivy, Clair)

---

## 🚀 Быстрый старт: план внедрения за 4 недели

### Неделя 1: Фундамент
- [ ] Поднять vLLM с Qwen2.5-14B на 1 GPU-узле
- [ ] Настроить OpenSearch для RAG (индекс, kNN)
- [ ] Загрузить первую порцию доков (PostgreSQL, Redis, Nginx, Prometheus)
- [ ] Базовый RAG (BM25 + dense search, без rerank)
- [ ] Простой UI (Open WebUI)
- **Результат:** Первые ответы на вопросы по документации

### Неделя 2: Интеграции
- [ ] Подключить Prometheus/Alertmanager (триаж алертов)
- [ ] Подключить OpenSearch логи (контекст для алертов)
- [ ] Добавить Grafana (снапшоты панелей)
- [ ] Включить reranker (bge-reranker)
- [ ] Цитаты в ответах
- **Результат:** Автоматический триаж 1–2 типов алертов

### Неделя 3: Автоматизация
- [ ] Tool calling: Prometheus query, OpenSearch logs, Grafana snapshot
- [ ] Интеграция с Ansible/AWX (dry-run плейбуков)
- [ ] OPA политики (безопасность инструментов)
- [ ] mTLS между компонентами
- **Результат:** Полуавтоматическое ремедиацию для 2–3 сценариев

### Неделя 4: Production
- [ ] HA: 2 GPU-узла + HAProxy
- [ ] Keycloak SSO, Vault для секретов
- [ ] Полный аудит (логи в OpenSearch)
- [ ] Mattermost bot для команды
- [ ] Обучение L1/L2
- [ ] Метрики качества (latency, hit rate, feedback)
- **Результат:** Готовность к production, пилотная группа инженеров

---

## 📊 Ожидаемый эффект (сводка)

| Метрика | Улучшение |
|---------|-----------|
| **MTTA (Mean Time To Acknowledge)** | −40–60% |
| **MTTD (Mean Time To Detect)** | −35–50% |
| **MTTR (Mean Time To Repair)** | −25–40% |
| **Эскалации L1→L2** | −25–40% |
| **Время на PIR** | −60–80% |
| **Конфигурационные ошибки** | −30–50% |
| **Время на тюнинг БД** | −50–70% |
| **Подготовка к обновлениям** | −40–60% |
| **Шумные алерты** | −30–50% |
| **Время онбординга** | −30–50% |
| **Инциденты после изменений** | −25–40% |
| **SLA по инцидентам** | +5–15% |
| **Удовлетворенность инженеров** | ↑↑ (снижение рутины) |

---

## ❓ Уточняющие вопросы для финализации

1. **Приоритетные метрики:** Какие показатели наиболее критичны для вас?
   - Время (MTTA/MTTR)?
   - Качество (SLA, меньше инцидентов)?
   - Экономия ресурсов (FTE, стоимость)?

2. **Языки:** Преимущественно русский/английский или 50/50? (влияет на выбор модели)

3. **Первые сценарии:** С чего начать в первую очередь?
   - Триаж алертов Prometheus?
   - Помощник по конфигам Nginx/HAProxy?
   - Тюнинг PostgreSQL?

4. **Существующая инфраструктура:**
   - Есть ли свободные серверы для пилота?
   - Используется ли уже Ansible Tower/AWX?
   - Есть ли SSO (Keycloak/LDAP)?

5. **Бюджет:** Какой вариант предпочтителен — минимальный ($44k–$54k) или оптимальный ($92k–$115k)?

Готов детализировать любой сценарий, подготовить PoC-план или технический дизайн под ваши требования.
