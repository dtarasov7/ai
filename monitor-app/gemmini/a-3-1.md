Отличная задача! Построение такого дашборда — ключевой шаг к проактивному мониторингу и обеспечению надежности системы.

Вот детальный план по созданию дашборда в Grafana, который будет отображать как общее состояние ИС, так и состояние каждого компонента в отдельности.

### Общая концепция дашборда

Дашборд будет построен по принципу "сверху-вниз":
1.  **Верхний уровень:** Одна большая панель **"Общий статус ИС"**. Она отвечает на вопрос: "Система доступна для конечных пользователей?".
2.  **Средний уровень:** Ряд панелей (Stat Panel) для каждого **типа** компонента. Каждая панель показывает совокупный статус всех экземпляров этого компонента (например, "Кластер Postgres", "Серверы приложений").
3.  **Нижний уровень (детализация):** Графики и таблицы с детальными метриками по каждому компоненту, которые могут быть скрыты по умолчанию и раскрываться для анализа инцидентов. (Этот уровень не был явно запрошен, но является логическим продолжением).

Мы сосредоточимся на первых двух уровнях.

---

### Панель 1: Общий статус ИС (доступность для потребителей)

Эта панель должна отражать опыт конечного пользователя. Лучший способ это сделать — симулировать действия пользователя извне.

*   **Тип панели:** Stat Panel
*   **Название:** `Общий статус доступности ИС`

#### Сбор метрик (Exporter)

**Prometheus Blackbox Exporter.** Этот экспортер позволяет "проверять" HTTP, TCP, ICMP и другие эндпоинты снаружи вашей системы, как это делал бы реальный пользователь.

1.  Разверните Blackbox Exporter.
2.  Настройте в Prometheus `scrape_config` для "прощупывания" ключевых точек входа вашей ИС через Blackbox Exporter. Цели должны быть внешними URL-адресами.
    *   `https://your-main-app-domain.com/` (главная страница)
    *   `https://your-main-app-domain.com/login` (страница аутентификации)
    *   `https://api.your-main-app-domain.com/health` (ключевой эндпоинт API)

#### Ключевые метрики

*   `probe_success{job="blackbox"}`: 1, если проверка прошла успешно, 0 — если нет.
*   `probe_duration_seconds{job="blackbox"}`: Время ответа от эндпоинта.

#### Логика вычисления статуса в Grafana (Stat Panel)

*   **OK (Зеленый):** Все ключевые эндпоинты доступны.
    *   **PromQL:** `min(probe_success{job="blackbox", group="critical"}) == 1`
    *   (Здесь `group="critical"` - это ваша метка в конфигурации Prometheus для самых важных проверок).
*   **Warning (Желтый):** Доступны не все эндпоинты, но основные работают, или время ответа значительно увеличилось.
    *   **PromQL:** `min(probe_success{job="blackbox", group="critical"}) == 0 AND min(probe_success{job="blackbox"}) == 1` (критичные недоступны, но хоть что-то работает) ИЛИ `max(probe_duration_seconds{job="blackbox"}) > 2` (время ответа больше 2 секунд).
*   **Error (Красный):** Основной эндпоинт (или все эндпоинты) недоступен.
    *   **PromQL:** `max(probe_success{job="blackbox"}) == 0`

---

### Панель 2: Статус компонентов ИС

Для каждого компонента создается своя Stat Panel.

#### 1. Nginx в ДМЗ

*   **Сбор метрик:** **nginx-prometheus-exporter**. Устанавливается рядом с каждым Nginx.
*   **Ключевые метрики:**
    *   `up{job="nginx-dmz"}`: Доступность самого экспортера (и, косвенно, сервера).
    *   `nginx_http_requests_total{status=~"5.."}`: Количество 5xx ошибок.
*   **Логика вычисления статуса:**
    *   **OK:** Все экземпляры Nginx доступны, и нет всплеска 5xx ошибок.
        *   `count(up{job="nginx-dmz"}) == <общее_число_nginx_в_дмз>`
    *   **Warning:** Один из серверов Nginx недоступен, но остальные работают. Либо наблюдается повышенный, но не критичный, уровень 5xx ошибок.
        *   `count(up{job="nginx-dmz"}) < <общее_число_nginx_в_дмз> AND count(up{job="nginx-dmz"}) > 0`
        *   ИЛИ `sum(rate(nginx_http_requests_total{job="nginx-dmz", status=~"5.."}[5m])) > 5`
    *   **Error:** Все серверы Nginx недоступны.
        *   `count(up{job="nginx-dmz"}) == 0`

#### 2. Серверы приложений (AS) + Микросервисы + Nginx на AS

Эту группу логично объединить, так как их здоровье тесно связано. Статус "AS" — это совокупный статус микросервисов, работающих на них.

*   **Сбор метрик:**
    *   **Микросервисы:** Используйте клиентскую библиотеку Prometheus для вашего языка/фреймворка (например, Micrometer для Spring Boot, `prom-client` для Node.js). Она будет отдавать метрики (`/metrics`).
    *   **Nginx на AS:** Тот же `nginx-prometheus-exporter`.
*   **Ключевые метрики:**
    *   `up{job="my-microservice"}`: Доступность экземпляра микросервиса.
    *   `http_server_requests_seconds_count{status=~"5.."}`: Счетчик серверных ошибок от микросервиса.
    *   `jvm_memory_used_bytes{area="heap"}` (для Java): Использование памяти.
*   **Логика вычисления статуса:**
    *   **OK:** Все экземпляры всех микросервисов доступны и работают без ошибок.
        *   `count(up{job="my-microservice"}) == <общее_число_AS>`
    *   **Warning:** Часть экземпляров недоступна, но не все. Или наблюдается высокий уровень ошибок / потребления ресурсов на некоторых экземплярах.
        *   `count(up{job="my-microservice"}) < <общее_число_AS> AND count(up{job="my-microservice"}) > 0`
    *   **Error:** Ни один экземпляр микросервиса не отвечает.
        *   `count(up{job="my-microservice"}) == 0`

#### 3. Кластер Redis

*   **Сбор метрик:** **redis_exporter**.
*   **Ключевые метрики:**
    *   `redis_up`: 1, если узел Redis отвечает.
    *   `redis_cluster_state`: 1, если состояние кластера `ok`.
    *   `redis_connected_slaves`: Количество подключенных реплик (если используется).
*   **Логика вычисления статуса:**
    *   **OK:** `redis_cluster_state == 1` и все узлы `redis_up == 1`.
    *   **Warning:** Кластер в целом работает (`redis_cluster_state == 1`), но один из узлов недоступен (`sum(redis_up) < <общее_число_узлов_redis>`).
    *   **Error:** Состояние кластера не `ok` (`redis_cluster_state != 1`) или все узлы недоступны.

#### 4. Кластер RabbitMQ

*   **Сбор метрик:** Встроенный **RabbitMQ Prometheus plugin**.
*   **Ключевые метрики:**
    *   `rabbitmq_running`: 1, если узел запущен.
    *   `rabbitmq_cluster_partitions`: Количество сетевых разделений (должно быть 0).
    *   `rabbitmq_queue_messages_ready`: Количество готовых сообщений в очередях (аномальный рост — проблема).
*   **Логика вычисления статуса:**
    *   **OK:** Все узлы запущены, и нет разделений кластера.
        *   `sum(rabbitmq_running) == <число_узлов_rabbitmq> AND sum(rabbitmq_cluster_partitions) == 0`
    *   **Warning:** Один из узлов упал, но кластер не разделен. Либо какая-то очередь неконтролируемо растет.
        *   `sum(rabbitmq_running) < <число_узлов_rabbitmq> AND sum(rabbitmq_cluster_partitions) == 0`
    *   **Error:** Кластер разделен (`sum(rabbitmq_cluster_partitions) > 0`) или все узлы недоступны.

#### 5. Кластер Kafka + Zookeeper

*   **Сбор метрик:** **JMX Exporter** для Kafka и Zookeeper.
*   **Ключевые метрики:**
    *   `kafka_controller_kafkacontroller_activecontrollercount`: Должен быть равен 1 в кластере.
    *   `kafka_server_brokertopicmetrics_underreplicatedpartitions`: Количество недореплицированных партиций (должно быть 0).
    *   `zookeeper_ruok`: 1, если ZK отвечает `imok`.
*   **Логика вычисления статуса:**
    *   **OK:** Есть активный контроллер, нет недореплицированных партиций, все узлы ZK отвечают.
        *   `sum(kafka_controller_kafkacontroller_activecontrollercount) == 1 AND sum(kafka_server_brokertopicmetrics_underreplicatedpartitions) == 0`
    *   **Warning:** Нет активного контроллера, но брокеры работают. Или есть недореплицированные партиции. Или один из узлов ZK недоступен, но кворум сохранен.
        *   `sum(kafka_controller_kafkacontroller_activecontrollercount) == 0` ИЛИ `sum(kafka_server_brokertopicmetrics_underreplicatedpartitions) > 0`
    *   **Error:** Кворум Zookeeper потерян, или все брокеры Kafka недоступны.

#### 6. Кластер PostgreSQL

*   **Сбор метрик:** **postgres_exporter**.
*   **Ключевые метрики:**
    *   `pg_up`: 1, если экспортер может подключиться к Postgres.
    *   `pg_stat_replication_lag`: Отставание репликации в байтах (для реплик).
    *   `pg_is_in_recovery`: 1, если узел является репликой.
*   **Логика вычисления статуса (для архитектуры master-slave):**
    *   **OK:** Мастер доступен, все реплики доступны и лаг репликации минимален.
        *   `sum(pg_up * (1 - pg_is_in_recovery)) == 1 AND sum(pg_up) == <число_узлов_pg>`
    *   **Warning:** Мастер доступен, но одна из реплик отстает или недоступна.
        *   `sum(pg_up * (1 - pg_is_in_recovery)) == 1 AND sum(pg_up) < <число_узлов_pg>`
        *   ИЛИ `max(pg_stat_replication_lag) > <некий_порог_байт>`
    *   **Error:** Мастер-узел недоступен.
        *   `sum(pg_up * (1 - pg_is_in_recovery)) == 0`

#### 7. Кластер Keycloak

*   **Сбор метрик:** Keycloak (на базе WildFly/Quarkus) имеет встроенную поддержку метрик в формате Prometheus. Эндпоинт `/metrics` должен быть доступен.
*   **Ключевые метрики:**
    *   `up{job="keycloak"}`: Доступность узла.
    *   `keycloak_failed_login_attempts`: Счетчик неудачных попыток входа (всплеск может говорить о проблеме или атаке).
*   **Логика вычисления статуса:**
    *   **OK:** Все узлы Keycloak доступны.
        *   `count(up{job="keycloak"}) == <число_узлов_keycloak>`
    *   **Warning:** Часть узлов недоступна. Или резкий рост ошибок аутентификации.
        *   `count(up{job="keycloak"}) < <число_узлов_keycloak> AND count(up{job="keycloak"}) > 0`
    *   **Error:** Ни один из узлов Keycloak не отвечает.
        *   `count(up{job="keycloak"}) == 0`

#### 8. Кластер Consul

*   **Сбор метрик:** Consul имеет встроенный эндпоинт `/v1/agent/metrics?format=prometheus`.
*   **Ключевые метрики:**
    *   `consul_raft_peers`: Количество известных пиров в Raft-консенсусе. Должно быть равно числу серверов.
    *   `consul_serf_lan_members{status="alive"}`: Количество живых членов кластера.
*   **Логика вычисления статуса:**
    *   **OK:** Количество пиров и живых членов равно ожидаемому.
        *   `min(consul_raft_peers) == <число_серверов_consul>`
    *   **Warning:** Один из серверов отвалился, но кворум (большинство) сохранен.
        *   `min(consul_raft_peers) < <число_серверов_consul> AND min(consul_raft_peers) >= (<число_серверов_consul>/2 + 1)`
    *   **Error:** Потерян кворум.
        *   `min(consul_raft_peers) < (<число_серверов_consul>/2 + 1)`

#### 9. Кластер Ceph

*   **Сбор метрик:** Модуль **Prometheus** для **Ceph Manager (ceph-mgr)**.
*   **Ключевые метрики:**
    *   `ceph_health_status`: Главная метрика здоровья. 0=OK, 1=WARN, 2=ERR.
*   **Логика вычисления статуса:** Эта метрика идеально ложится на нашу задачу.
    *   **OK:** `ceph_health_status == 0`
    *   **Warning:** `ceph_health_status == 1`
    *   **Error:** `ceph_health_status == 2`

#### 10. Кластер OpenSearch

*   **Сбор метрик:** **Elasticsearch Exporter** (полностью совместим с OpenSearch) или специальный плагин для OpenSearch.
*   **Ключевые метрики:**
    *   `opensearch_cluster_health_status`: 'green', 'yellow' или 'red'. Экспортер представляет это как `opensearch_cluster_health_status{color="green"} 1`.
*   **Логика вычисления статуса:**
    *   **OK:** `opensearch_cluster_health_status{color="green"} == 1`
    *   **Warning:** `opensearch_cluster_health_status{color="yellow"} == 1` (Все праймари-шарды доступны, но не все реплики. Данные в безопасности, но нет полной отказоустойчивости).
    *   **Error:** `opensearch_cluster_health_status{color="red"} == 1` (Недоступен как минимум один праймари-шард. Часть данных недоступна для записи/чтения).

### Рекомендации по настройке Grafana

*   **Переменные:** Создайте переменную, например `$job`, чтобы можно было легко фильтровать дашборд по конкретным джобам Prometheus.
*   **Цвета и пороги:** В настройках Stat Panel вы можете задать цвета (зеленый, желтый, красный) на основе значений. Вместо PromQL, который выдает 0 или 1, можно написать запрос, который выдает 0 (OK), 1 (Warning), 2 (Error), и настроить маппинг значений на цвета.
*   **Ссылки (Drill-down):** Настройте на каждой Stat-панели ссылку, которая ведет на другой, более детальный дашборд, посвященный именно этому компоненту. Это позволит быстро перейти от общего статуса к анализу причин проблемы.
