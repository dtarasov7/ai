Отличное и очень правильное задание. Построение такого дашборда — ключевой шаг для эксплуатации комплексной ИС.

Вот подробный план по созданию дашборда "Общее состояние ИС" в Grafana на основе ваших компонентов, с указанием метрик, экспортеров и логики вычисления статусов.

### Общая концепция

1.  **Сбор данных:** Для каждого компонента используется специализированный **Prometheus exporter**, который собирает метрики и предоставляет их Prometheus. В `prometheus.yml` вы должны настроить `scrape_configs` для каждого типа экспортера, используя `job` и `instance` лейблы для идентификации.
2.  **Дашборд в Grafana:**
    *   Создаем новый дашборд.
    *   Для каждого компонента добавляем панель типа **Stat**.
    *   В каждую панель мы пишем PromQL-запрос, который будет возвращать одно числовое значение: **2** для **OK**, **1** для **Warning**, **0** для **Error**.
    *   В настройках панели используем **Value Mappings** (или **Thresholds** в старых версиях) для сопоставления этих чисел с текстом и цветом:
        *   `0` -> `ERROR` (красный)
        *   `1` -> `WARNING` (оранжевый)
        *   `2` -> `OK` (зеленый)
3.  **Общий статус ИС:** Самая верхняя и главная панель. Она показывает, доступна ли система для конечных потребителей.

---

### 1. Общий статус доступности ИС (для пользователей)

Эта панель должна отвечать на главный вопрос: "Работает ли система для внешних пользователей?". Лучший способ это проверить — симуляция реального пользовательского запроса.

*   **Сбор метрик (с помощью чего):** **Prometheus Blackbox Exporter**. Его настраивают на периодическую проверку публичного URL вашей системы (например, `https://your-service.com`). Он проверяет доступность DNS, TCP, TLS и HTTP-кода ответа (должен быть 200 OK).
*   **Ключевые метрики:**
    *   `probe_success`: `1`, если проверка прошла успешно, `0` — если нет.
    *   `probe_http_duration_seconds`: Время ответа. Позволяет отловить "медленную" работу.
*   **Логика вычисления статуса:**

    ```promql
    // Возвращает 2, если проба успешна и быстра
    (
        probe_success{job="blackbox-public"} == 1 
      AND 
        probe_http_duration_seconds{job="blackbox-public"} < 5
    ) * 2
    
    // ИЛИ возвращает 1, если проба успешна, но медленная
    OR ON()
    (
        probe_success{job="blackbox-public"} == 1 
      AND 
        probe_http_duration_seconds{job="blackbox-public"} >= 5
    ) * 1
    
    // Если ничего из вышеперечисленного не сработало, значит probe_success = 0, и вернется 0 (ERROR)
    ```
    *   **OK (2):** Система доступна извне и отвечает быстро (например, менее 5 секунд).
    *   **Warning (1):** Система доступна, но отвечает медленно. Это признак деградации производительности.
    *   **Error (0):** Система недоступна (HTTP-ошибка, таймаут). `probe_success` равен 0.

---

### 2. Nginx в ДМЗ

*   **Сбор метрик:** **Nginx Prometheus Exporter**. Разворачивается рядом с каждым Nginx или имеет доступ к его `stub_status` эндпоинту.
*   **Ключевые метрики:**
    *   `nginx_up{job="nginx-dmz"}`: `1` если экспортер может получить метрики от Nginx, `0` если нет.
*   **Логика вычисления статуса:** Сравниваем количество работающих инстансов с общим количеством.

    ```promql
    // 2 (OK), если все инстансы работают
    (
        sum(nginx_up{job="nginx-dmz"}) == count(nginx_up{job="nginx-dmz"})
    ) * 2
    
    // ИЛИ 1 (Warning), если хотя бы один работает (но не все)
    OR ON()
    (
        sum(nginx_up{job="nginx-dmz"}) > 0 
      AND 
        sum(nginx_up{job="nginx-dmz"}) < count(nginx_up{job="nginx-dmz"})
    ) * 1
    
    // Иначе 0 (Error), если ни один не работает
    ```
    *   **OK (2):** Все серверы Nginx в ДМЗ доступны.
    *   **Warning (1):** Часть серверов Nginx недоступна, но сервис в целом работает за счет оставшихся.
    *   **Error (0):** Ни один сервер Nginx не отвечает.

---

### 3. Сервера приложений (AS) и Микросервисы

Логика для них идентична, просто будут разные `job` в Prometheus. Предположим, что каждый микросервис и AS имеют встроенную возможность отдавать метрики (например, через Spring Boot Actuator, `micrometer`, или клиентскую библиотеку Prometheus).

*   **Сбор метрик:** **Клиентская библиотека Prometheus**, встроенная в код приложения/микросервиса. Она предоставляет эндпоинт `/metrics`.
*   **Ключевые метрики:**
    *   `up{job="application-servers"}`: Стандартная метрика Prometheus, показывающая, удалось ли ему собрать данные с эндпоинта.
*   **Логика вычисления статуса (для AS):**

    ```promql
    ( sum(up{job="application-servers"}) == count(up{job="application-servers"}) ) * 2
    OR ON()
    ( sum(up{job="application-servers"}) > 0 AND sum(up{job="application-servers"}) < count(up{job="application-servers"}) ) * 1
    ```
    *   **Логика вычисления статуса (для Микросервисов, пример для `auth-service`):**

    ```promql
    ( sum(up{job="auth-service"}) == count(up{job="auth-service"}) ) * 2
    OR ON()
    ( sum(up{job="auth-service"}) > 0 AND sum(up{job="auth-service"}) < count(up{job="auth-service"}) ) * 1
    ```
    *   **OK (2):** Все экземпляры доступны.
    *   **Warning (1):** Часть экземпляров недоступна.
    *   **Error (0):** Все экземпляры недоступны.

---

### 4. Внутренний Nginx (на каждом AS)

Та же логика, что и для Nginx в ДМЗ, но с другим `job`.

*   **Сбор метрик:** **Nginx Prometheus Exporter**.
*   **Ключевые метрики:** `nginx_up{job="nginx-internal"}`.
*   **Логика вычисления статуса:**
    ```promql
    ( sum(nginx_up{job="nginx-internal"}) == count(nginx_up{job="nginx-internal"}) ) * 2
    OR ON()
    ( sum(nginx_up{job="nginx-internal"}) > 0 AND sum(nginx_up{job="nginx-internal"}) < count(nginx_up{job="nginx-internal"}) ) * 1
    ```

---

### 5. Кластер Redis

*   **Сбор метрик:** **Redis Exporter**.
*   **Ключевые метрики:**
    *   `redis_up{job="redis"}`: Доступность отдельного инстанса Redis.
    *   `redis_cluster_state{job="redis"}`: Состояние кластера (`1` если ОК, `0` если `fail`).
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если кластер в состоянии ОК и все ноды доступны
    (
        min(redis_cluster_state{job="redis"}) == 1
      AND
        sum(redis_up{job="redis"}) == count(redis_up{job="redis"})
    ) * 2
    
    // ИЛИ 1 (Warning), если кластер в состоянии ОК, но не все ноды доступны
    OR ON()
    (
        min(redis_cluster_state{job="redis"}) == 1
      AND
        sum(redis_up{job="redis"}) < count(redis_up{job="redis"})
    ) * 1
    
    // Иначе 0 (Error), если кластер в статусе fail
    ```
    *   **OK (2):** Кластер здоров, все ноды доступны.
    *   **Warning (1):** Кластер функционален, но одна из нод (например, реплика) отвалилась.
    *   **Error (0):** Кластер находится в состоянии `fail`.

---

### 6. Кластер RabbitMQ

*   **Сбор метрик:** Встроенный в RabbitMQ **Prometheus Plugin**.
*   **Ключевые метрики:**
    *   `rabbitmq_running{job="rabbitmq"}`: `1`, если нода запущена и является частью кластера.
    *   `rabbitmq_partitions{job="rabbitmq"}`: Количество сетевых разделений (partitions), видимых с этой ноды. Должно быть 0.
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если все ноды работают и нет разделений
    (
        sum(rabbitmq_running{job="rabbitmq"}) == count(rabbitmq_running{job="rabbitmq"})
      AND
        sum(rabbitmq_partitions{job="rabbitmq"}) == 0
    ) * 2
    
    // ИЛИ 1 (Warning), если есть разделение или не все ноды работают
    OR ON()
    (
        sum(rabbitmq_running{job="rabbitmq"}) > 0 
      AND 
        (sum(rabbitmq_partitions{job="rabbitmq"}) > 0 OR sum(rabbitmq_running{job="rabbitmq"}) < count(rabbitmq_running{job="rabbitmq"}))
    ) * 1
    
    // Иначе 0 (Error), если ни одна нода не работает
    ```
    *   **OK (2):** Все ноды в кластере работают, нет сетевых разделений.
    *   **Warning (1):** Есть сетевое разделение (очень опасно!) или часть нод отвалилась.
    *   **Error (0):** Все ноды RabbitMQ недоступны.

---

### 7. Кластер Kafka (+ Zookeeper)

Разделим их на две панели, так как это разные сервисы.

#### Kafka
*   **Сбор метрик:** **Kafka Exporter** (например, от Danielqsj).
*   **Ключевые метрики:**
    *   `kafka_brokers`: Количество брокеров, которое видит экспортер.
    *   `kafka_topic_partition_under_replicated_partitions`: Количество "недореплицированных" партиций. Критически важный показатель здоровья. В идеале всегда 0.
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если есть брокеры и нет недореплицированных партиций
    (
        sum(kafka_brokers{job="kafka"}) > 0 
      AND
        sum(kafka_topic_partition_under_replicated_partitions{job="kafka"}) == 0
    ) * 2
    
    // ИЛИ 1 (Warning), если есть недореплицированные партиции
    OR ON()
    (
        sum(kafka_brokers{job="kafka"}) > 0 
      AND 
        sum(kafka_topic_partition_under_replicated_partitions{job="kafka"}) > 0
    ) * 1
    
    // Иначе 0 (Error), если не видно ни одного брокера
    ```
    *   **OK (2):** Все данные надежно реплицированы.
    *   **Warning (1):** Есть проблемы с репликацией. Часть данных под угрозой при падении брокера.
    *   **Error (0):** Кластер Kafka недоступен.

#### Zookeeper
*   **Сбор метрик:** **JMX Exporter** или через команду `mntr` с помощью скрипта-экспортера.
*   **Ключевые метрики:** `up{job="zookeeper"}`. Более сложная проверка — `zk_server_state` (`leader` или `follower`).
*   **Логика вычисления статуса (простой вариант):**
    ```promql
    ( sum(up{job="zookeeper"}) == count(up{job="zookeeper"}) ) * 2
    OR ON()
    ( sum(up{job="zookeeper"}) > 0 AND sum(up{job="zookeeper"}) < count(up{job="zookeeper"}) ) * 1
    ```
    *   **OK (2):** Все ноды ZK доступны.
    *   **Warning (1):** Потеряна часть нод, кворум еще может быть, но надежность снижена.
    *   **Error (0):** Потеряно большинство нод (кворум потерян) или все ноды.

---

### 8. Кластер PostgreSQL

*   **Сбор метрик:** **PostgreSQL Exporter**.
*   **Ключевые метрики:**
    *   `pg_up{job="postgres"}`: Может ли экспортер подключиться к базе.
    *   `pg_stat_replication_lag` (или `pg_replication_lag` в старых версиях): Отставание реплик.
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если все инстансы доступны и лаг репликации мал
    (
        sum(pg_up{job="postgres"}) == count(pg_up{job="postgres"})
      AND
        max(pg_stat_replication_lag{job="postgres"}) < 10000000 -- ~10 MB
    ) * 2
    
    // ИЛИ 1 (Warning), если кто-то отвалился или большой лаг
    OR ON()
    (
        sum(pg_up{job="postgres"}) > 0
      AND
        (sum(pg_up{job="postgres"}) < count(pg_up{job="postgres"}) OR max(pg_stat_replication_lag{job="postgres"}) >= 10000000)
    ) * 1
    
    // Иначе 0 (Error), если все инстансы лежат
    ```
    *   **OK (2):** Все ноды PostgreSQL доступны, репликация работает с минимальным отставанием.
    *   **Warning (1):** Одна из реплик недоступна или сильно отстает.
    *   **Error (0):** Все ноды недоступны (или как минимум мастер).

---

### 9. Кластер Keycloak

*   **Сбор метрик:** Keycloak (Wildfly/JBoss) предоставляет метрики через **JMX**. Используйте **JMX Exporter**.
*   **Ключевые метрики:** `up{job="keycloak"}`.
*   **Логика вычисления статуса:** Аналогична серверам приложений.
    ```promql
    ( sum(up{job="keycloak"}) == count(up{job="keycloak"}) ) * 2
    OR ON()
    ( sum(up{job="keycloak"}) > 0 AND sum(up{job="keycloak"}) < count(up{job="keycloak"}) ) * 1
    ```

---

### 10. Кластер Consul

*   **Сбор метрик:** Consul имеет встроенный эндпоинт метрик Prometheus. Просто добавьте его в `scrape_configs`.
*   **Ключевые метрики:**
    *   `consul_raft_leader{job="consul"}`: `1`, если нода является лидером. Сумма по кластеру должна быть `1`.
    *   `consul_serf_lan_members{job="consul"}`: Количество членов кластера.
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если есть лидер и все ожидаемые члены на месте
    (
        sum(consul_raft_leader{job="consul"}) == 1
      AND
        min(consul_serf_lan_members{job="consul"}) == count(up{job="consul"})
    ) * 2
    
    // ИЛИ 1 (Warning), если есть лидер, но кого-то не хватает
    OR ON()
    (
        sum(consul_raft_leader{job="consul"}) == 1
      AND
        min(consul_serf_lan_members{job="consul"}) < count(up{job="consul"})
    ) * 1
    
    // Иначе 0 (Error), если нет лидера
    ```
    *   **OK (2):** В кластере есть лидер, все ноды на месте.
    *   **Warning (1):** Лидер есть, но часть нод отвалилась.
    *   **Error (0):** В кластере нет лидера. Сервис неработоспособен.

---

### 11. Кластер Ceph

Ceph сам отлично сообщает о своем состоянии.

*   **Сбор метрик:** Встроенный в **Ceph Manager (mgr)** модуль `prometheus`.
*   **Ключевые метрики:** `ceph_health_status`. Эта метрика уже кодирует состояние кластера: `1` для "HEALTH_OK", `2` для "HEALTH_WARN", `3` для "HEALTH_ERR" (значения могут меняться, проверьте в своей версии).
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK)
    (ceph_health_status == 1) * 2
    // 1 (Warning)
    OR ON() (ceph_health_status == 2) * 1
    // 0 (Error) - если статус не 1 и не 2, считаем его за ошибку
    ```
    *   **OK (2):** Ceph сообщает `HEALTH_OK`.
    *   **Warning (1):** Ceph сообщает `HEALTH_WARN`.
    *   **Error (0):** Ceph сообщает `HEALTH_ERR`.

---

### 12. Кластер OpenSearch

Аналогично Ceph, OpenSearch (как и Elasticsearch) сам сообщает о статусе кластера.

*   **Сбор метрик:** **OpenSearch Prometheus Exporter** или плагин.
*   **Ключевые метрики:** `opensearch_cluster_health_status{color="green|yellow|red"}`.
*   **Логика вычисления статуса:**
    ```promql
    // 2 (OK), если статус green
    opensearch_cluster_health_status{job="opensearch", color="green"} * 2
    
    // ИЛИ 1 (Warning), если статус yellow
    OR ON()
    opensearch_cluster_health_status{job="opensearch", color="yellow"} * 1
    
    // Иначе 0 (Error), если статус red
    ```
    *   **OK (2):** Статус `green`. Все первичные и реплика-шарды распределены.
    *   **Warning (1):** Статус `yellow`. Все первичные шарды доступны, но некоторые реплики не распределены. Кластер работает, но отказоустойчивость снижена.
    *   **Error (0):** Статус `red`. Некоторые первичные шарды недоступны. Часть данных потеряна, кластер не работает.

---

### Заключение

Этот дашборд даст вам мгновенное представление о работоспособности всей системы. Для каждого компонента рекомендуется создавать отдельный, более детальный дашборд, на который можно будет перейти по клику с главной панели (через `Data links` в настройках панели), чтобы расследовать причины `Warning` или `Error` статусов.
