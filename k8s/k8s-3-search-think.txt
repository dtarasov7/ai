Для мониторинга кластера Kubernetes через Zabbix (без инфраструктурных метрик) рекомендую следующие проверки:

---

### 1. **Недоступность kube-api через внешний балансировщик**
- **Метод получения**: Запрос от Zabbix server через `curl` к API (`https://<LB_IP>/api/healthz`) с проверкой HTTP-статуса .
- **Порог**: Ошибка 503 (Service Unavailable) или таймаут >5 секунд.
- **Частота**: 1 раз в минуту.
- **Комментарий**: Критическая метрика для управления кластером.

---

### 2. **Состояние системных компонентов Kubernetes (kube-apiserver, etcd, kube-scheduler, kube-controller-manager)**
- **Метод получения**: Запрос к `/healthz` каждого компонента через локальный `curl` на master-нодах (порт 8080 для `kube-apiserver`, 10251 для других) .
- **Порог**: Ответ `500 Internal Server Error` или таймаут.
- **Источник**: Zabbix agent на master-нодах.
- **Комментарий**: Позволяет выявить падение ключевых компонентов.

---

### 3. **Состояние нод кластера (Node Ready/NotReady)**
- **Метод получения**: Скрипт на master-нодах с использованием `kubectl get nodes -o wide` и анализом статуса .
- **Порог**: Любая нода в состоянии `NotReady`.
- **Источник**: Zabbix agent на master-нодах.
- **Частота**: 1 раз в 2 минуты.

---

### 4. **Проблемы сетевой связанности между узлами**
- **Метод получения**: 
  - Пинг между всеми нодами (master, ingress, worker) через `fping` .
  - Проверка TCP-соединения на порт 6443 (API) и 10250 (kubelet) между нодами.
- **Порог**: 3 последовательных провала.
- **Источник**: Zabbix agent на всех хостах.
- **Комментарий**: Обнаруживает сетевые изоляции между компонентами.

---

### 5. **Срок действия сертификатов**
- **Метод получения**: Скрипт на master- и ingress-нодах с проверкой сертификатов через `openssl x509 -in <file> -text -noout` .
- **Порог**: Осталось <7 дней до истечения.
- **Источник**: Zabbix agent на соответствующих хостах.
- **Частота**: 1 раз в день.

---

### 6. **Состояние Ingress-контроллера (Nginx)**
- **Метод получения**: 
  - Проверка доступности портов 80/443 на ingress-нодах.
  - Тестовый запрос к Ingress-пулу с валидацией HTTP-ответа (200 OK) .
- **Порог**: Таймаут или 5xx ошибка.
- **Источник**: Zabbix agent на ingress-нодах.
- **Комментарий**: Указывает на проблемы с маршрутизацией внешнего трафика.

---

### 7. **Состояние Istio Control Plane**
- **Метод получения**: 
  - Проверка статуса подов в `istio-system` через `kubectl get pods`.
  - Проверка доступности Istio Ingress Gateway (порт 15021 `/healthz`) .
- **Порог**: Любая ошибка в подах или недоступность `/healthz`.
- **Источник**: Zabbix agent на master-нодах.

---

### 8. **Состояние кластера etcd**
- **Метод получения**: Запрос к etcd API (`etcdctl cluster-health`) на master-нодах .
- **Порог**: Статус `unhealthy`.
- **Источник**: Zabbix agent на master-нодах.
- **Комментарий**: Критично для работы кластера.

---

### 9. **Доступность внешних сервисов**
- **Метод получения**: 
  - DNS: Проверка резолва через `dig` или `nslookup` к корпоративным DNS-серверам.
  - LDAP: Проверка подключения через `ldapsearch`.
  - Container Registry: Запрос `GET /v2/` с валидацией ответа .
- **Порог**: Таймаут или код ошибки.
- **Источник**: Zabbix agent на master-нодах.
- **Комментарий**: Предотвращает проблемы с аутентификацией и загрузкой образов.

---

### 10. **Срабатывание OOM Killer**
- **Метод получения**: Парсинг логов `/var/log/messages` или `/dmesg` на наличие строк `oom-killer` .
- **Порог**: Обнаружение события.
- **Источник**: Zabbix agent на всех хостах.
- **Частота**: Регулярная проверка логов (1 раз в минуту).

---

### Рекомендации:
- **Исключите дублирующие проверки** (например, отдельно для Istio и Ingress, если они пересекаются).
- **Используйте шаблоны Zabbix** для автоматизации настройки проверок.
- **Настройте зависимости** между алертами (например, если LB недоступен, не проверять внутренние компоненты кластера).
- **Тестирование порогов**: Для минимизации ложных срабатываний используйте **последовательные проверки** (например, 3 неудачных попытки подряд).

Эти метрики позволят дежурной смене оперативно выявлять критические инциденты, сохраняя фокус на ключевых аспектах работы кластера.
