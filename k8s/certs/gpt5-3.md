Ниже — отлаженная стратегия PKI для Deckhouse в изолированном периметре, где корпоративные сертификаты выпускаются только вручную и только вне кластера. Поделю контур на внутренний (полностью автоматизируем) и внешний (минимизируем ручные операции), дам пошаговые плейбуки выпуска/обновления и чек‑листы.

Куда и какие сертификаты
- Внутрикластерные (автономные, не зависят от корп. УЦ):
  - kubernetes-ca (API server/клиенты), front-proxy-ca, etcd-ca.
  - kubelet client/server сертификаты (через CSR API).
  - Ключ(и) подписи сервис‑аккаунтов (не CA).
  - mTLS между сервисами: отдельный кластерный CA (cert-manager/SPIFEE), не смешивать с корп. УЦ.
- Внешние (доверие конечных пользователей/браузеров/корп. хостов):
  - Ingress* сертификаты (TLS для ваших UI, API).
  - Опционально TLS для внешнего доступа к kube-apiserver (чаще — терминация на периметральном LB/прокси).

Оптимальная стратегия при ручном выпуске (вне кластера)
- Главная идея: внутри — максимально короткие сроки и авто‑ротация; снаружи — минимизируем количество “ручных” объектов и автоматизируем доставку/переключение.
- Приоритизация вариантов для внешнего контура:
  1) Если политика допускает: один раз вручную выпустить подчинённый Intermediate (subCA) от корп. УЦ для зоны приложений (*.apps.corp, *.prod.corp) и держать его ключ внутри кластера. Далее все листовые для ingress выдаются автоматически (cert-manager с CA‑issuer). Ручной труд — только при ротации subCA раз в 2–3 года.
  2) Если subCA запрещён: использовать ограниченное число wildcard‑сертификатов (по зонам/средам: *.prod.apps.corp, *.stg.apps.corp). Единичные “штучные” домены — по исключению.
  3) Для kube-apiserver с внешним доступом: TLS‑offload корп. сертификатом на периметральном балансировщике/прокси; внутрь — mTLS на кластерном CA. Так не надо выпускать отдельный корп. cert для самого apiserver.

Сроки и криптопрофили
- Алгоритмы: ECDSA P‑256 (предпочтительно) или RSA‑3072.
- Сроки:
  - Внутренние CAs (kubernetes/front-proxy/etcd): 1–2 года.
  - Листовые control-plane/etcd: 90–180 дней; kubelet: 30–90 дней.
  - subCA (если разрешён): 2–3 года.
  - Внешние листовые (ingress): 180–365 дней при ручном выпуске; если есть subCA — 90–120 дней.
- В изоляции не полагайтесь на OCSP/CRL — компенсируйте короткими сроками и алертами.

Внутрикластерная PKI: создание и авто‑обновление
- Контроль‑плейн (kubeadm под Deckhouse):
  - План: каждые 90–180 дней “kubeadm certs renew all” по одной control‑plane ноде → restart kubelet (статические Pod’ы перезапустятся без простоя кластера).
  - kubelet: включить rotate‑certificates; одобрение CSR — контроллером в kube‑controller‑manager (установите --cluster-signing-duration=90d или согласно вашей политике).
- etcd:
  - По одной ноде кворума: сначала разнести trust‑bundle (oldCA+newCA) при смене CA, перевыпустить server/peer, перезапустить ноду, убедиться в здоровье, переходить дальше.
- ServiceAccount signing keys:
  - Включать одновременно несколько ключей проверки; новый ключ — для подписи, старые — оставлять только на verify до истечения max TTL токенов, затем удалять.

Внешний контур: два проверенных сценария

A) Если политика допускает subCA внутри кластера (наилучший уровень автоматизации)
- Вручную вне кластера:
  1) Сгенерируйте ключ subCA и CSR (офлайн).
  2) Подпишите у корп. УЦ как Intermediate. Получите цепочку до корня.
- В кластере:
  3) Загрузите ключ+цепочку в Secret (namespace с cert-manager), доступ только контроллеру; включите шифрование Secret’ов в etcd.
  4) ClusterIssuer типа CA → cert-manager автоматически выпускает/продлевает листовые для ваших Ingress (объекты Certificate с renewBefore ~30% срока).
- Ротация subCA (раз в 2–3 года):
  - Фаза 1: разнести bundle old+new в доверяющие компоненты; переключить issuer на новый subCA.
  - Фаза 2: перевыпустить все листовые (cert-manager сделает сам, можно поэтапно по namespace).
  - Фаза 3: после прохождения max TTL удалить старый CA из bundle.

B) Если subCA запрещён: wildcard и строгий ручной регламент
- Выдайте вне кластера 1–3 wildcard‑сертификата (на среду/зону): *.prod.apps.corp, *.stg.apps.corp, при необходимости отдельный для админских UI.
- Хранение: в Git как SealedSecret/SOPS (age/GPG), в кластере — как Secret c минимальными правами (PSA=restricted, отдельный namespace, строгое RBAC).
- Обновление без простоя:
  - “Replace‑in‑place”: обновить Secret тем же именем (fullchain.pem + key.pem) — ingress‑контроллер перезагрузит конфиг без даунтайма.
  - Либо “Blue/Green Secret”: создать новый Secret, временно переключить Ingress.tls на него, проверить, удалить старый.
- Штучные домены:
  - Генерация ключа/CSR в изолированной среде → подпись у УЦ → обновление Secret. Сведите такие кейсы к минимуму.

Плейбуки (шаблоны под ручной выпуск)

1) Генерация CSR для wildcard (EC P‑256)
- openssl конфиг csr.conf:
  [ req ]
  default_md = sha256
  prompt = no
  req_extensions = v3_req
  distinguished_name = dn
  [ dn ]
  CN = *.prod.apps.corp
  O = YourOrg
  [ v3_req ]
  keyUsage = critical, digitalSignature, keyEncipherment
  extendedKeyUsage = serverAuth
  subjectAltName = @alt
  [ alt ]
  DNS.1 = *.prod.apps.corp

- Команды:
  openssl genpkey -algorithm EC -pkeyopt ec_paramgen_curve:P-256 -out tls.key
  openssl req -new -key tls.key -out tls.csr -config csr.conf
  # Передать csr в корп. УЦ, получить tls.crt и chain.pem (intermediate(s)+root)

2) Загрузка/обновление в k8s (namespace вашего ingress-контроллера, например d8-ingress-nginx)
- Создание или обновление секретов:
  kubectl -n <ns> create secret tls wildcard-prod \
    --cert=fullchain.pem --key=tls.key \
    -o yaml --dry-run=client | kubectl apply -f -
- Проверки:
  - Ingress контроллер перечитал конфиг (логи/метрики).
  - Клиент видит полную цепочку (openssl s_client -showcerts -connect host:443).

3) Регламент ротации wildcard
- T‑60 дней: выпустить новый сертификат вне кластера.
- T‑30 дней: положить новый fullchain/key в Secret (replace‑in‑place); автоматическая перезагрузка ingress.
- T‑7 дней: повторная проверка цепочки и SAN; удалить старые материалы из хранилищ.

4) Ручной “штучный” домен
- Подготовить openssl конфиг с SAN для всех нужных DNS‑имен.
- Генерировать ключ/CSR → подпись у УЦ → Secret в namespace приложения.
- Обновить манифест Ingress.tls.secretName, если используется другой Secret; дождаться обновления и удалить старый Secret.

5) Kube‑API с внешним доступом (без выпуска корп. cert на apiserver)
- Периметральный LB/прокси: установить корп. cert (fullchain+key), настроить TLS‑offload.
- От LB к apiserver: mTLS на внутреннем kubernetes‑ca, список SAN apiserver должен включать VIP/LB‑имя и сервисные имена.
- Альтернатива: распространить kubernetes‑ca в доверенные хранилища рабочих станций админов (GPO/Ansible/базовый образ).

Мониторинг сроков и алерты (особенно для “ручных” Secret’ов)
- Включите сбор метрик cert-manager (если используется) — он даёт expiry_timestamp по объектам Certificate.
- Для “ручных” TLS‑Secret’ов:
  - CronJob, который раз в 6–12 часов обходит Secrets типа kubernetes.io/tls, извлекает NotAfter и публикует метрику в Prometheus (или пишет в Event/Slack).
  - Алерты: T‑60, T‑30, T‑7 дней до истечения.
- Техпроверки:
  - kubeadm certs check-expiration (control‑plane).
  - openssl x509 -noout -enddate -in <file>.
  - e2e‑проверка внешних хостов: blackbox‑exporter tcp+tls с проверкой цепочки.

Распределение доверия
- Узлы и контейнеры должны доверять корпоративным корням/промежуточным:
  - Добавьте в системные trust‑stores ОС (update-ca-trust/ca-certificates), а для Java — в cacerts (JKS).
- Внутренние кластерные CAs предоставляйте только тем, кто ходит к API/вебхукам напрямую.

Безопасность ключей и бэкапы
- Включите шифрование Secret’ов в etcd (EncryptionConfiguration).
- Храните ключи wildcard/subCA как SealedSecret/SOPS в Git, а исходные ключи — в офлайн‑хранилище/внутреннем Vault; доступ — по принципу наименьших привилегий.
- Регулярные офлайн‑бэкапы etcd и критичных Secret’ов, периодические тесты восстановления.

Чек‑лист SAN и типовых ошибок
- Ingress‑серты: для каждого FQDN, который реально будут вызывать (включая short‑name? лучше всегда FQDN).
- kube‑apiserver: SAN должен содержать kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.<cluster-domain>, ClusterIP API‑сервиса, VIP/LB‑имя, IP/FQDN master‑узлов.
- etcd: у каждого узла в SAN — IP и FQDN ноды (для server и peer).
- Секреты для ingress: используйте full chain (leaf+intermediate), иначе клиенты могут не доверять.
- Наблюдайте за правами: ключи 600, владелец root.

Рекомендуемый ритм работ
- Ежедневно: алерты на истечение (<60/<30/<7 дней).
- Ежеквартально: контрольная ротация листовых control‑plane/etcd (если срок 90–180 дней).
- Ежегодно: учения по ротации CA на dev/стенде (bundle‑подход).
- За 60–90 дней до истечения wildcard/subCA: готовить выпуск и окно внедрения.

Итог
- Внутренняя PKI — автономна и автоматизирована (kubeadm+CSR API, короткие сроки).
- Внешний контур при ручном выпуске: минимизировать количество сертификатов (subCA — лучший вариант; если нельзя — ограниченный набор wildcard), автоматизировать доставку и проверку.
- Все секреты — через GitOps и шифрование, полные цепочки, строгий RBAC/PSA.
- Для смены любых CA — двухфазный bundle‑подход (old+new) → перевыпуск листовых → удаление old.

Если скажете:
- допускается ли subCA внутри кластера;
- какие доменные зоны/среды и какой ingress используете;
- нужен ли внешний доступ к kube‑api,

— пришлю точные YAML (Issuer/Certificate/SealedSecret), CronJob для мониторинга истечений и пошаговый runbook под ваш стек.
