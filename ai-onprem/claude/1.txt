Рассмотрю подробно архитектуру решения для создания on-premise LLM ассистента для управления ИТ-инфраструктурой.

## Архитектура решения

### 1. Выбор LLM модели

**Рекомендуемые модели для CPU-инференса:**

1. **Llama 3.1 8B-Instruct** (приоритетный выбор)
   - Размер: ~4.7GB в GGUF формате (Q4_K_M квантизация)
   - RAM требования: 6-8GB
   - Хорошее качество для технических задач
   - Поддержка контекста до 128K токенов

2. **Mistral 7B-Instruct v0.3**
   - Размер: ~4.1GB (Q4_K_M)
   - RAM требования: 6GB
   - Отличная производительность на CPU

3. **Code Llama 7B-Instruct**
   - Специализирован на программировании
   - Размер: ~4.1GB (Q4_K_M)

### 2. Техническая архитектура

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Web Frontend  │───→│   API Gateway    │───→│   LLM Service   │
│   (Streamlit/   │    │   (FastAPI)      │    │   (llama.cpp)   │
│    Gradio)      │    │                  │    │                 │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │   RAG System     │
                       │   (ChromaDB/     │
                       │    FAISS)        │
                       └──────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │   Knowledge      │
                       │   Base           │
                       └──────────────────┘
```

### 3. Компоненты системы

#### A. LLM Инференс сервис
```python
# Пример с llama-cpp-python
from llama_cpp import Llama

class LLMService:
    def __init__(self):
        self.llm = Llama(
            model_path="./models/llama-3.1-8b-instruct.Q4_K_M.gguf",
            n_ctx=32768,  # контекст
            n_threads=8,  # CPU threads
            n_gpu_layers=0,  # CPU only
            verbose=False
        )
    
    def generate_response(self, prompt, max_tokens=2048):
        response = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=0.1,
            top_p=0.9,
            stop=["<|im_end|>"]
        )
        return response['choices'][0]['text']
```

#### B. RAG система для контекстной информации
```python
import chromadb
from sentence_transformers import SentenceTransformer

class RAGSystem:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./vector_db")
        self.collection = self.client.get_or_create_collection("infrastructure")
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_documents(self, documents, metadata):
        embeddings = self.encoder.encode(documents)
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadata,
            ids=[f"doc_{i}" for i in range(len(documents))]
        )
    
    def search(self, query, n_results=5):
        query_embedding = self.encoder.encode([query])
        results = self.collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=n_results
        )
        return results
```

#### C. Парсер инвентаря и версий
```python
import yaml
import pandas as pd

class InfrastructureParser:
    def __init__(self, inventory_path, versions_csv):
        self.inventory = self.load_ansible_inventory(inventory_path)
        self.versions_df = pd.read_csv(versions_csv)
    
    def load_ansible_inventory(self, path):
        with open(path, 'r') as f:
            return yaml.safe_load(f)
    
    def get_server_info(self, server_name):
        # Извлечение информации о сервере
        server_info = {}
        # Логика парсинга инвентаря
        return server_info
    
    def get_software_versions(self, server_name):
        return self.versions_df[
            self.versions_df['server'] == server_name
        ].to_dict('records')
```

### 4. Предварительная загрузка знаний

#### Скрипт для загрузки документации:
```python
import requests
from bs4 import BeautifulSoup
import concurrent.futures
import time

class DocumentationScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; DocBot/1.0)'
        })
    
    def scrape_product_docs(self, product_configs):
        """
        product_configs = {
            'ceph': {
                'base_url': 'https://docs.ceph.com/',
                'sections': ['install', 'troubleshooting', 'config']
            },
            'kafka': {
                'base_url': 'https://kafka.apache.org/documentation/',
                'sections': ['quickstart', 'config', 'ops']
            }
        }
        """
        all_docs = []
        
        for product, config in product_configs.items():
            docs = self.scrape_single_product(product, config)
            all_docs.extend(docs)
            time.sleep(1)  # Rate limiting
        
        return all_docs
    
    def scrape_single_product(self, product, config):
        docs = []
        for section in config['sections']:
            url = f"{config['base_url']}/{section}"
            try:
                response = self.session.get(url, timeout=30)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Извлечение текста
                text = soup.get_text()
                docs.append({
                    'product': product,
                    'section': section,
                    'url': url,
                    'content': text
                })
            except Exception as e:
                print(f"Error scraping {url}: {e}")
        
        return docs

# Конфигурация для загрузки
DOCUMENTATION_SOURCES = {
    'ceph': {
        'base_url': 'https://docs.ceph.com/en/latest',
        'sections': [
            'install', 'cephadm', 'troubleshooting',
            'rados/configuration', 'mgr'
        ]
    },
    'kafka': {
        'base_url': 'https://kafka.apache.org/documentation',
        'sections': [
            'quickstart', 'configuration', 'ops',
            'security', 'connect'
        ]
    },
    'prometheus': {
        'base_url': 'https://prometheus.io/docs',
        'sections': [
            'introduction', 'installation', 'configuration',
            'querying', 'alerting'
        ]
    },
    'grafana': {
        'base_url': 'https://grafana.com/docs/grafana/latest',
        'sections': [
            'setup-grafana', 'dashboards', 'alerting',
            'administration'
        ]
    },
    'opensearch': {
        'base_url': 'https://opensearch.org/docs/latest',
        'sections': [
            'install-and-configure', 'api-reference',
            'troubleshoot'
        ]
    },
    'nginx': {
        'base_url': 'https://nginx.org/en/docs',
        'sections': [
            'beginners_guide', 'admin_guide',
            'troubleshooting'
        ]
    },
    'kubernetes': {
        'base_url': 'https://kubernetes.io/docs',
        'sections': [
            'setup', 'concepts', 'tasks',
            'troubleshooting', 'reference'
        ]
    }
}
```

#### Скрипт для загрузки StackOverflow:
```python
import stackapi

class StackOverflowScraper:
    def __init__(self):
        self.so = stackapi.StackAPI('stackoverflow')
    
    def get_questions_by_tags(self, tags, max_questions=1000):
        questions = []
        
        for tag in tags:
            try:
                result = self.so.fetch('questions', 
                                     tagged=tag,
                                     sort='votes',
                                     order='desc',
                                     pagesize=100,
                                     max_pages=max_questions//100)
                
                for question in result['items']:
                    # Получаем ответы
                    answers = self.so.fetch('answers',
                                          ids=[question['question_id']],
                                          filter='withbody')
                    
                    questions.append({
                        'title': question['title'],
                        'question': question.get('body', ''),
                        'answers': [a.get('body', '') for a in answers.get('items', [])],
                        'tags': question['tags'],
                        'score': question['score']
                    })
                    
            except Exception as e:
                print(f"Error fetching {tag}: {e}")
        
        return questions

# Теги для загрузки
STACKOVERFLOW_TAGS = [
    'ceph', 'apache-kafka', 'prometheus', 'grafana',
    'opensearch', 'elasticsearch', 'nginx', 'kubernetes',
    'ansible', 'linux-administration'
]
```

### 5. Система обратной связи

```python
class FeedbackSystem:
    def __init__(self):
        self.feedback_db = sqlite3.connect('feedback.db')
        self.init_db()
    
    def init_db(self):
        cursor = self.feedback_db.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS feedback (
                id INTEGER PRIMARY KEY,
                query TEXT,
                response TEXT,
                rating INTEGER,
                feedback_text TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.feedback_db.commit()
    
    def store_feedback(self, query, response, rating, feedback_text):
        cursor = self.feedback_db.cursor()
        cursor.execute('''
            INSERT INTO feedback (query, response, rating, feedback_text)
            VALUES (?, ?, ?, ?)
        ''', (query, response, rating, feedback_text))
        self.feedback_db.commit()
    
    def get_negative_feedback(self):
        cursor = self.feedback_db.cursor()
        cursor.execute('''
            SELECT * FROM feedback WHERE rating <= 2
        ''')
        return cursor.fetchall()
```

### 6. Основной API сервис

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

class QueryRequest(BaseModel):
    query: str
    context: dict = {}

class FeedbackRequest(BaseModel):
    query: str
    response: str
    rating: int
    feedback: str = ""

# Инициализация компонентов
llm_service = LLMService()
rag_system = RAGSystem()
infra_parser = InfrastructureParser('./inventory.yml', './versions.csv')
feedback_system = FeedbackSystem()

@app.post("/query")
async def process_query(request: QueryRequest):
    try:
        # Поиск релевантной информации
        relevant_docs = rag_system.search(request.query)
        
        # Получение информации об инфраструктуре
        infra_context = infra_parser.get_context_for_query(request.query)
        
        # Формирование промпта
        prompt = build_prompt(request.query, relevant_docs, infra_context)
        
        # Генерация ответа
        response = llm_service.generate_response(prompt)
        
        return {"response": response, "context": infra_context}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    feedback_system.store_feedback(
        request.query, request.response, 
        request.rating, request.feedback
    )
    return {"status": "success"}

def build_prompt(query, relevant_docs, infra_context):
    context_text = "\n".join([doc['document'] for doc in relevant_docs['documents']])
    
    prompt = f"""
Ты - экспертный ассистент по управлению ИТ-инфраструктурой.

КОНТЕКСТ ИНФРАСТРУКТУРЫ:
{infra_context}

РЕЛЕВАНТНАЯ ДОКУМЕНТАЦИЯ:
{context_text}

ЗАПРОС ПОЛЬЗОВАТЕЛЯ: {query}

Предоставь детальный ответ, включающий:
1. Анализ проблемы (если применимо)
2. Конкретные команды/скрипты
3. Объяснение решения
4. Меры предосторожности

Ответ должен быть специфичен для данной инфраструктуры.
"""
    return prompt
```

### 7. Требования к оборудованию

**Минимальные требования:**
- **CPU**: 8+ ядер (Intel Xeon или AMD EPYC)
- **RAM**: 16GB (12GB для модели + 4GB для системы)
- **Диск**: 100GB SSD (модели + векторная БД + документация)
- **Сеть**: Gigabit Ethernet

**Рекомендуемые требования:**
- **CPU**: 16+ ядер
- **RAM**: 32GB
- **Диск**: 200GB NVMe SSD
- **Виртуализация**: 2-3 ВМ для разделения компонентов

**Распределение по ВМ:**
1. **LLM Service VM**: 8 ядер, 12GB RAM
2. **RAG + API VM**: 4 ядра, 8GB RAM  
3. **Frontend + DB VM**: 4 ядра, 8GB RAM

### 8. Пошаговая реализация

1. **Подготовка данных** (с доступом к интернету):
   ```bash
   python scrape_documentation.py
   python scrape_stackoverflow.py
   python build_vector_db.py
   ```

2. **Развертывание модели**:
   ```bash
   # Загрузка модели
   wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
   
   # Запуск сервиса
   python llm_service.py
   ```

3. **Настройка RAG системы**:
   ```bash
   python setup_rag.py --docs ./scraped_docs --inventory ./inventory.yml --versions ./versions.csv
   ```

4. **Запуск API**:
   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000
   ```

### 9. Дополнительные возможности

- **Логирование всех запросов** для анализа паттернов
- **Автоматическое обновление** векторной БД при изменении инвентаря
- **Интеграция с системами мониторинга** для получения актуальных метрик
- **Механизм тонкой настройки** модели на основе feedback

Такое решение обеспечит автономную работу LLM-ассистента с контекстом вашей инфраструктуры и возможностью обучения на основе обратной связи.