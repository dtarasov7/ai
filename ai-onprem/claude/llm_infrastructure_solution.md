# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM-—Ä–µ—à–µ–Ω–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ò–¢-–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π

## 1. –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π LLM –º–æ–¥–µ–ª–∏

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è CPU-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –≤—ã–±–æ—Ä: Llama 3.1 8B-Instruct**
- –†–∞–∑–º–µ—Ä: ~4.7GB –≤ GGUF —Ñ–æ—Ä–º–∞—Ç–µ (Q4_K_M –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)
- RAM —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: 6-8GB
- –ö–æ–Ω—Ç–µ–∫—Å—Ç: –¥–æ 128K —Ç–æ–∫–µ–Ω–æ–≤
- –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:**
- **Mistral 7B-Instruct v0.3**: 4.1GB, —Ö–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **Code Llama 7B-Instruct**: —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∫–æ–¥–µ
- **Phi-3-medium-4k-instruct**: –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Microsoft

## 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Web Frontend  ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ   API Gateway    ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ   LLM Service   ‚îÇ
‚îÇ   (Streamlit)   ‚îÇ    ‚îÇ   (FastAPI)      ‚îÇ    ‚îÇ   (llama.cpp)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ  Context Manager ‚îÇ
                       ‚îÇ  + Session Store ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚ñº               ‚ñº               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   RAG System     ‚îÇ ‚îÇ Infrastructure   ‚îÇ ‚îÇ  Feedback        ‚îÇ
    ‚îÇ   (ChromaDB)     ‚îÇ ‚îÇ Context Engine   ‚îÇ ‚îÇ  Learning System ‚îÇ
    ‚îÇ + Version Index  ‚îÇ ‚îÇ                  ‚îÇ ‚îÇ                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                    ‚îÇ                    ‚îÇ
             ‚ñº                    ‚ñº                    ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Versioned Docs   ‚îÇ ‚îÇ Ansible Inventory‚îÇ ‚îÇ Feedback DB      ‚îÇ
    ‚îÇ Knowledge Base   ‚îÇ ‚îÇ + Versions CSV   ‚îÇ ‚îÇ + Learning Rules ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 3. –°–∏—Å—Ç–µ–º–∞ –≤–µ—Ä—Å–∏–æ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π

### 3.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

```
knowledge_base/
‚îú‚îÄ‚îÄ products/
‚îÇ   ‚îú‚îÄ‚îÄ ceph/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 16.2.10/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ installation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configuration/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ troubleshooting/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 17.2.5/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 18.1.0/
‚îÇ   ‚îú‚îÄ‚îÄ kafka/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2.8.1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 3.0.0/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 3.4.0/
‚îÇ   ‚îî‚îÄ‚îÄ kubernetes/
‚îÇ       ‚îú‚îÄ‚îÄ 1.25.8/
‚îÇ       ‚îú‚îÄ‚îÄ 1.26.3/
‚îÇ       ‚îî‚îÄ‚îÄ 1.27.1/
‚îú‚îÄ‚îÄ stackoverflow/
‚îÇ   ‚îú‚îÄ‚îÄ ceph/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version_agnostic/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ version_specific/
‚îÇ   ‚îî‚îÄ‚îÄ kafka/
‚îî‚îÄ‚îÄ metadata/
    ‚îú‚îÄ‚îÄ version_index.json
    ‚îî‚îÄ‚îÄ content_fingerprints.json
```

### 3.2 –°–∏—Å—Ç–µ–º–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤–µ—Ä—Å–∏–π

```python
class VersionedKnowledgeManager:
    def __init__(self, base_path="./knowledge_base"):
        self.base_path = base_path
        self.version_index = self._load_version_index()
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_db = chromadb.PersistentClient(path="./vector_db")
        
    def _load_version_index(self):
        """

## 8. –û—Å–Ω–æ–≤–Ω–æ–π API —Å–µ—Ä–≤–∏—Å

### 8.1 FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import Optional, Dict, List
import asyncio

app = FastAPI(title="Infrastructure LLM Assistant")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class QueryRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    context: Dict = {}

class QueryResponse(BaseModel):
    response: str
    session_id: str
    infrastructure_context: Dict
    sources: List[Dict]

class FeedbackRequest(BaseModel):
    session_id: str
    query: str
    response: str
    rating: int
    feedback_text: str = ""

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
llm_service = LLMService()
knowledge_manager = VersionedKnowledgeManager()
context_engine = InfrastructureContextEngine("./inventory.yml", "./versions.csv")
session_manager = SessionManager()
feedback_system = FeedbackSystem()
learning_engine = LearningEngine(feedback_system, knowledge_manager)

@app.post("/query", response_model=QueryResponse)
async def process_query(request: QueryRequest):
    try:
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é
        session_id = request.session_id or session_manager.create_session()
        session_context = session_manager.get_session_context(session_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        infra_context = context_engine.get_context_for_query(
            request.query, session_context
        )
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –≤–µ—Ä—Å–∏–π
        relevant_docs = await search_versioned_documentation(
            request.query, infra_context
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –ø—Ä–∞–≤–∏–ª –æ–±—É—á–µ–Ω–∏—è
        system_prompt = learning_engine._update_system_prompts()
        prompt = build_contextual_prompt(
            request.query, infra_context, relevant_docs, 
            session_context, system_prompt
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = llm_service.generate_response(prompt)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏
        session_manager.add_to_context(
            session_id, request.query, response, infra_context
        )
        
        return QueryResponse(
            response=response,
            session_id=session_id,
            infrastructure_context=infra_context,
            sources=relevant_docs
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest, background_tasks: BackgroundTasks):
    try:
        feedback_entry = FeedbackEntry(
            query=request.query,
            response=request.response,
            rating=request.rating,
            feedback_text=request.feedback_text,
            context=session_manager.get_session_context(request.session_id),
            timestamp=datetime.now(),
            session_id=request.session_id
        )
        
        feedback_system.store_feedback(feedback_entry)
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        background_tasks.add_task(learning_engine.process_feedback)
        
        return {"status": "success", "message": "Feedback recorded"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/update-knowledge")
async def trigger_knowledge_update(background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    background_tasks.add_task(update_knowledge_base)
    return {"status": "update_started"}

async def search_versioned_documentation(query: str, infra_context: Dict) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –≤–µ—Ä—Å–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
    relevant_docs = []
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    for server_info in infra_context.get('servers', []):
        for software in server_info.get('software_stack', []):
            product = software['product']
            version = software['version']
            
            # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏
            docs = knowledge_manager.search_versioned_docs(
                query, product, version
            )
            
            if docs:
                relevant_docs.extend([{
                    'product': product,
                    'version': version,
                    'content': doc,
                    'source': 'official_docs'
                } for doc in docs['documents']])
    
    # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ –æ–±—â–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–∞—Ö, —É–ø–æ–º—è–Ω—É—Ç—ã—Ö –≤ –∑–∞–ø—Ä–æ—Å–µ
    for product_info in infra_context.get('products', []):
        product = product_info['name']
        
        # –ë–µ—Ä–µ–º –≤—Å–µ –≤–µ—Ä—Å–∏–∏ —ç—Ç–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
        for version in product_info.get('versions', []):
            docs = knowledge_manager.search_versioned_docs(
                query, product, version
            )
            
            if docs:
                relevant_docs.extend([{
                    'product': product,
                    'version': version,
                    'content': doc,
                    'source': 'official_docs'
                } for doc in docs['documents']])
    
    return relevant_docs[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

def build_contextual_prompt(query: str, infra_context: Dict, 
                          relevant_docs: List[Dict], session_context: List[Dict],
                          system_prompt: str) -> str:
    """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    infra_text = format_infrastructure_context(infra_context)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    docs_text = format_documentation_context(relevant_docs)
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏
    session_text = format_session_context(session_context)
    
    prompt = f"""{system_prompt}

CURRENT INFRASTRUCTURE CONTEXT:
{infra_text}

RELEVANT DOCUMENTATION:
{docs_text}

CONVERSATION HISTORY:
{session_text}

USER QUERY: {query}

Provide a detailed response that:
1. Uses the exact versions and configurations from the infrastructure
2. References specific servers when applicable
3. Includes step-by-step instructions if providing commands
4. Explains any risks or prerequisites
5. Cites the relevant documentation versions used

Response:"""

    return prompt

def format_infrastructure_context(infra_context: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
    lines = []
    
    # –°–µ—Ä–≤–µ—Ä—ã
    for server in infra_context.get('servers', []):
        lines.append(f"Server: {server['name']}")
        lines.append(f"  Group: {server['info'].get('group', 'unknown')}")
        
        for software in server.get('software_stack', []):
            lines.append(f"  - {software['product']} v{software['version']}")
        lines.append("")
    
    # –ü—Ä–æ–¥—É–∫—Ç—ã
    for product in infra_context.get('products', []):
        lines.append(f"Product: {product['name']}")
        lines.append(f"  Servers: {', '.join(product['servers'])}")
        lines.append(f"  Versions: {', '.join(product['versions'])}")
        lines.append("")
    
    return "\n".join(lines)

def format_documentation_context(relevant_docs: List[Dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
    lines = []
    
    for doc in relevant_docs:
        lines.append(f"[{doc['product']} v{doc['version']}] {doc['content'][:500]}...")
        lines.append("")
    
    return "\n".join(lines)

def format_session_context(session_context: List[Dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
    if not session_context:
        return "No previous conversation."
    
    lines = []
    for item in session_context[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –æ–±–º–µ–Ω–∞
        lines.append(f"Previous Query: {item['query']}")
        lines.append(f"Previous Response: {item['response'][:200]}...")
        lines.append("---")
    
    return "\n".join(lines)

async def update_knowledge_base():
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        monitor = InfrastructureMonitor("./versions.csv", "./inventory.yml")
        changes = monitor.check_for_changes()
        
        if changes:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏
            version_analyzer = VersionAnalyzer("./versions.csv")
            current_versions = version_analyzer.get_unique_versions()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π
            async with DocumentationScraper() as scraper:
                updater = KnowledgeUpdater(knowledge_manager, scraper)
                await updater.update_for_new_versions(current_versions)
                
        print("Knowledge base update completed")
        
    except Exception as e:
        print(f"Error updating knowledge base: {e}")
```

## 9. LLM –°–µ—Ä–≤–∏—Å

### 9.1 –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

```python
from llama_cpp import Llama
import threading
from queue import Queue
import time

class LLMService:
    def __init__(self, model_path: str = "./models/llama-3.1-8b-instruct.Q4_K_M.gguf"):
        self.model_path = model_path
        self.llm = None
        self.request_queue = Queue()
        self.response_cache = {}
        self.cache_max_size = 100
        self.load_model()
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=32768,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
                n_threads=8,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU –ø–æ—Ç–æ–∫–æ–≤
                n_gpu_layers=0,  # CPU-only
                n_batch=512,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                verbose=False,
                use_mmap=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ memory mapping
                use_mlock=True,  # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –≤ –ø–∞–º—è—Ç–∏
                n_parts=1
            )
            print("Model loaded successfully")
        except Exception as e:
            print(f"Error loading model: {e}")
            raise
    
    def generate_response(self, prompt: str, max_tokens: int = 2048, 
                         temperature: float = 0.1) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        prompt_hash = hash(prompt)
        if prompt_hash in self.response_cache:
            return self.response_cache[prompt_hash]
        
        try:
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.1,
                stop=["<|im_end|>", "Human:", "User:"],
                echo=False
            )
            
            generated_text = response['choices'][0]['text'].strip()
            
            # –ö—ç—à–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            self._cache_response(prompt_hash, generated_text)
            
            return generated_text
            
        except Exception as e:
            print(f"Error generating response: {e}")
            return "I apologize, but I encountered an error processing your request."
    
    def _cache_response(self, prompt_hash: int, response: str):
        """–ö—ç—à–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞"""
        if len(self.response_cache) >= self.cache_max_size:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]
            
        self.response_cache[prompt_hash] = response
    
    def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
        try:
            test_response = self.llm("Test", max_tokens=10)
            return bool(test_response['choices'][0]['text'])
        except:
            return False

## 10. Web –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### 10.1 Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ

```python
import streamlit as st
import requests
import json
from datetime import datetime

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_BASE_URL = "http://localhost:8000"

st.set_page_config(
    page_title="Infrastructure LLM Assistant",
    page_icon="üñ•Ô∏è",
    layout="wide"
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
if 'session_id' not in st.session_state:
    st.session_state.session_id = None
if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

def send_query(query: str):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ API"""
    try:
        response = requests.post(
            f"{API_BASE_URL}/query",
            json={
                "query": query,
                "session_id": st.session_state.session_id
            },
            timeout=120
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            st.error(f"Error: {response.status_code} - {response.text}")
            return None
            
    except requests.exceptions.RequestException as e:
        st.error(f"Connection error: {e}")
        return None

def send_feedback(query: str, response: str, rating: int, feedback_text: str):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""
    try:
        requests.post(
            f"{API_BASE_URL}/feedback",
            json={
                "session_id": st.session_state.session_id,
                "query": query,
                "response": response,
                "rating": rating,
                "feedback_text": feedback_text
            }
        )
        st.success("Feedback submitted successfully!")
        
    except requests.exceptions.RequestException as e:
        st.error(f"Error submitting feedback: {e}")

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("üñ•Ô∏è Infrastructure LLM Assistant")
st.markdown("AI-powered assistant for managing your IT infrastructure")

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("Chat Interface")
    
    # –û–±–ª–∞—Å—Ç—å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    chat_container = st.container()
    
    # –ü–æ–ª–µ –≤–≤–æ–¥–∞ –∑–∞–ø—Ä–æ—Å–∞
    query = st.text_area(
        "Enter your infrastructure query:",
        placeholder="e.g., How to troubleshoot Ceph cluster issues on server-01?",
        height=100
    )
    
    col_send, col_clear = st.columns([1, 1])
    
    with col_send:
        if st.button("Send Query", type="primary"):
            if query.strip():
                with st.spinner("Processing your query..."):
                    result = send_query(query)
                    
                if result:
                    st.session_state.session_id = result['session_id']
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
                    st.session_state.conversation_history.append({
                        'timestamp': datetime.now(),
                        'query': query,
                        'response': result['response'],
                        'infrastructure_context': result['infrastructure_context'],
                        'sources': result['sources']
                    })
                    
                    st.rerun()
    
    with col_clear:
        if st.button("Clear Conversation"):
            st.session_state.conversation_history = []
            st.session_state.session_id = None
            st.rerun()

with col2:
    st.subheader("Infrastructure Context")
    
    if st.session_state.conversation_history:
        latest_context = st.session_state.conversation_history[-1]['infrastructure_context']
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å–µ—Ä–≤–µ—Ä—ã
        if latest_context.get('servers'):
            st.write("**Active Servers:**")
            for server in latest_context['servers']:
                st.write(f"‚Ä¢ {server['name']}")
                for software in server.get('software_stack', []):
                    st.write(f"  - {software['product']} v{software['version']}")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã
        if latest_context.get('products'):
            st.write("**Products:**")
            for product in latest_context['products']:
                st.write(f"‚Ä¢ {product['name']}")
                st.write(f"  Versions: {', '.join(product['versions'])}")

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
with chat_container:
    for i, conversation in enumerate(st.session_state.conversation_history):
        # –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.markdown(f"**üßë‚Äçüíª You ({conversation['timestamp'].strftime('%H:%M:%S')}):**")
        st.markdown(conversation['query'])
        
        # –û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        st.markdown("**ü§ñ Assistant:**")
        st.markdown(conversation['response'])
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        if conversation.get('sources'):
            with st.expander("üìö Sources Used"):
                for source in conversation['sources']:
                    st.write(f"‚Ä¢ {source['product']} v{source['version']}")
        
        # –ö–Ω–æ–ø–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        feedback_col1, feedback_col2, feedback_col3 = st.columns([1, 1, 2])
        
        with feedback_col1:
            rating = st.selectbox(
                "Rating:", 
                options=[1, 2, 3, 4, 5],
                index=4,
                key=f"rating_{i}"
            )
        
        with feedback_col2:
            if st.button("Submit Feedback", key=f"feedback_btn_{i}"):
                feedback_text = st.text_input(
                    "Additional feedback:", 
                    key=f"feedback_text_{i}"
                )
                send_feedback(
                    conversation['query'],
                    conversation['response'],
                    rating,
                    feedback_text
                )
        
        st.divider()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏
with st.sidebar:
    st.subheader("System Information")
    
    # –ö–Ω–æ–ø–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π
    if st.button("Update Knowledge Base"):
        with st.spinner("Updating knowledge base..."):
            try:
                response = requests.post(f"{API_BASE_URL}/update-knowledge")
                if response.status_code == 200:
                    st.success("Knowledge base update started!")
                else:
                    st.error("Failed to start update")
            except:
                st.error("Connection error")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
    if st.session_state.conversation_history:
        st.metric("Queries in Session", len(st.session_state.conversation_history))
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã
        recent_products = set()
        for conv in st.session_state.conversation_history[-5:]:
            for product in conv.get('infrastructure_context', {}).get('products', []):
                recent_products.add(product['name'])
        
        if recent_products:
            st.write("**Recent Products:**")
            for product in recent_products:
                st.write(f"‚Ä¢ {product}")
    
    st.subheader("Quick Actions")
    
    quick_queries = [
        "Show Ceph cluster status",
        "List Kafka topics and partitions", 
        "Check Prometheus targets",
        "Grafana dashboard issues",
        "Kubernetes pod troubleshooting"
    ]
    
    for quick_query in quick_queries:
        if st.button(quick_query):
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–µ –∑–∞–ø—Ä–æ—Å–∞
            st.session_state.quick_query = quick_query
```

## 11. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ

### 11.1 –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

**–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
- **CPU**: 8 —è–¥–µ—Ä (Intel Xeon E5-2670 –∏–ª–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç)
- **RAM**: 16GB DDR4
- **–î–∏—Å–∫**: 100GB SSD
- **–û–°**: Ubuntu 20.04+ –∏–ª–∏ CentOS 8+

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
- **CPU**: 16 —è–¥–µ—Ä (Intel Xeon Gold 6230 –∏–ª–∏ AMD EPYC 7302)
- **RAM**: 32GB DDR4
- **–î–∏—Å–∫**: 200GB NVMe SSD
- **–°–µ—Ç—å**: Gigabit Ethernet

**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º:**

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | CPU | RAM | –î–∏—Å–∫ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|-----------|-----|-----|------|------------|
| LLM Service | 8 —è–¥–µ—Ä | 12GB | 20GB | –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ |
| RAG System | 4 —è–¥—Ä–∞ | 8GB | 50GB | –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î |
| API Service | 2 —è–¥—Ä–∞ | 4GB | 10GB | FastAPI + –ª–æ–≥–∏–∫–∞ |
| Web Interface | 2 —è–¥—Ä–∞ | 4GB | 10GB | Streamlit |
| Knowledge Base | - | 4GB | 100GB | –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è |

### 11.2 –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω–∞—Ö

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Load Balancer VM                         ‚îÇ
‚îÇ                  (nginx, 2 CPU, 4GB RAM)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚ñº           ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM VM     ‚îÇ ‚îÇ  RAG VM     ‚îÇ ‚îÇ  API VM     ‚îÇ
‚îÇ 8 CPU       ‚îÇ ‚îÇ 4 CPU       ‚îÇ ‚îÇ 4 CPU       ‚îÇ
‚îÇ 12GB RAM    ‚îÇ ‚îÇ 8GB RAM     ‚îÇ ‚îÇ 8GB RAM     ‚îÇ
‚îÇ 50GB Disk   ‚îÇ ‚îÇ 100GB Disk  ‚îÇ ‚îÇ 50GB Disk   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 11.3 –°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

```bash
#!/bin/bash
# deploy_infrastructure_llm.sh

set -e

echo "=== Infrastructure LLM Assistant Deployment ==="

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
check_requirements() {
    echo "Checking system requirements..."
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CPU
    cpu_cores=$(nproc)
    if [ "$cpu_cores" -lt 8 ]; then
        echo "Warning: Minimum 8 CPU cores recommended, found $cpu_cores"
    fi
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM
    ram_gb=$(free -g | awk '/^Mem:/{print $2}')
    if [ "$ram_gb" -lt 16 ]; then
        echo "Warning: Minimum 16GB RAM recommended, found ${ram_gb}GB"
    fi
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–∫–∞
    disk_space=$(df -BG / | awk 'NR==2{print $4}' | sed 's/G//')
    if [ "$disk_space" -lt 100 ]; then
        echo "Warning: Minimum 100GB free space recommended, found ${disk_space}GB"
    fi
}

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
install_dependencies() {
    echo "Installing dependencies..."
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    sudo apt update && sudo apt upgrade -y
    
    # Python –∏ pip
    sudo apt install -y python3 python3-pip python3-venv
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    sudo apt install -y build-essential cmake pkg-config
    sudo apt install -y libopenblas-dev liblapack-dev gfortran
    
    # Git
    sudo apt install -y git
    
    # Docker (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    curl -fsSL https://get.docker.com -o get-docker.sh
    sudo sh get-docker.sh
    sudo usermod -aG docker $USER
}

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è
setup_python_environment() {
    echo "Setting up Python environment..."
    
    python3 -m venv venv
    source venv/bin/activate
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pip
    pip install --upgrade pip
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    pip install fastapi uvicorn streamlit
    pip install llama-cpp-python --force-reinstall --no-cache-dir
    pip install sentence-transformers chromadb
    pip install pandas pyyaml requests aiohttp
    pip install beautifulsoup4 stackapi
    pip install python-multipart
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
download_model() {
    echo "Downloading LLM model..."
    
    mkdir -p models
    cd models
    
    # Llama 3.1 8B Instruct GGUF
    wget -c "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" \
         -O "llama-3.1-8b-instruct.Q4_K_M.gguf"
    
    cd ..
}

# –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
create_project_structure() {
    echo "Creating project structure..."
    
    mkdir -p {knowledge_base/products,knowledge_base/stackoverflow,knowledge_base/metadata}
    mkdir -p {vector_db,logs,config}
    mkdir -p scripts
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    cat >–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –≤–µ—Ä—Å–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        index_path = os.path.join(self.base_path, "metadata", "version_index.json")
        if os.path.exists(index_path):
            with open(index_path, 'r') as f:
                return json.load(f)
        return {}
    
    def get_docs_for_version(self, product, version):
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞"""
        collection_name = f"{product}_{version.replace('.', '_')}"
        try:
            collection = self.vector_db.get_collection(collection_name)
            return collection
        except:
            return None
    
    def search_versioned_docs(self, query, product, version, n_results=5):
        """–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        collection = self.get_docs_for_version(product, version)
        if not collection:
            # Fallback –Ω–∞ –±–ª–∏–∂–∞–π—à—É—é –≤–µ—Ä—Å–∏—é
            collection = self._find_closest_version_docs(product, version)
        
        if collection:
            query_embedding = self.embeddings_model.encode([query])
            results = collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=n_results
            )
            return results
        return None
```

## 4. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

### 4.1 –ê–Ω–∞–ª–∏–∑ –≤–µ—Ä—Å–∏–π –≤ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ

```python
class VersionAnalyzer:
    def __init__(self, versions_csv_path):
        self.versions_df = pd.read_csv(versions_csv_path)
        
    def get_unique_versions(self):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        versions_map = {}
        
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É CSV:
        # server,product,version,os_version
        for _, row in self.versions_df.iterrows():
            product = row['product']
            version = row['version']
            
            if product not in versions_map:
                versions_map[product] = set()
            versions_map[product].add(version)
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        for product in versions_map:
            versions_map[product] = sorted(list(versions_map[product]))
            
        return versions_map
    
    def get_server_stack(self, server_name):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Å—Ç–µ–∫ –ü–û –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞"""
        server_data = self.versions_df[
            self.versions_df['server'] == server_name
        ]
        return server_data.to_dict('records')
```

### 4.2 –°–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse

class DocumentationScraper:
    def __init__(self):
        self.session = None
        self.rate_limit = 1  # —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'Infrastructure-LLM-Bot/1.0'}
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            
    async def scrape_product_versions(self, product_config, versions):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π –ø—Ä–æ–¥—É–∫—Ç–∞"""
        all_docs = []
        
        for version in versions:
            print(f"Scraping {product_config['name']} v{version}...")
            version_docs = await self._scrape_version_docs(
                product_config, version
            )
            
            for doc in version_docs:
                doc['version'] = version
                doc['product'] = product_config['name']
                
            all_docs.extend(version_docs)
            await asyncio.sleep(self.rate_limit)
            
        return all_docs
    
    async def _scrape_version_docs(self, config, version):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        docs = []
        base_url = config['url_template'].format(version=version)
        
        for section in config['sections']:
            section_url = urljoin(base_url, section['path'])
            
            try:
                async with self.session.get(section_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        text = self._clean_html_content(soup)
                        
                        docs.append({
                            'section': section['name'],
                            'url': section_url,
                            'content': text,
                            'scraped_at': time.time()
                        })
                        
            except Exception as e:
                print(f"Error scraping {section_url}: {e}")
                
        return docs
    
    def _clean_html_content(self, soup):
        """–û—á–∏—Å—Ç–∫–∞ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏, –Ω–∞–≤–∏–≥–∞—Ü–∏—é
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text()
        
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        lines = [line.strip() for line in text.splitlines()]
        text = '\n'.join([line for line in lines if line])
        
        return text

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
PRODUCT_CONFIGS = {
    'ceph': {
        'name': 'ceph',
        'url_template': 'https://docs.ceph.com/en/{version}',
        'sections': [
            {'name': 'installation', 'path': 'install/'},
            {'name': 'configuration', 'path': 'rados/configuration/'},
            {'name': 'troubleshooting', 'path': 'troubleshooting/'},
            {'name': 'cephadm', 'path': 'cephadm/'},
            {'name': 'monitoring', 'path': 'mgr/'}
        ]
    },
    'kafka': {
        'name': 'kafka',
        'url_template': 'https://kafka.apache.org/{version}/documentation.html',
        'sections': [
            {'name': 'quickstart', 'path': '#quickstart'},
            {'name': 'configuration', 'path': '#configuration'},
            {'name': 'operations', 'path': '#operations'},
            {'name': 'security', 'path': '#security'},
            {'name': 'connect', 'path': '#connect'}
        ]
    },
    'prometheus': {
        'name': 'prometheus',
        'url_template': 'https://prometheus.io/docs/prometheus/{version}',
        'sections': [
            {'name': 'installation', 'path': 'installation/'},
            {'name': 'configuration', 'path': 'configuration/'},
            {'name': 'querying', 'path': 'querying/'},
            {'name': 'alerting', 'path': 'alerting/'}
        ]
    },
    'kubernetes': {
        'name': 'kubernetes',
        'url_template': 'https://kubernetes.io/docs/concepts/',
        'version_path_map': {
            # Kubernetes –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥—Ä—É–≥—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL
            'default': 'https://kubernetes.io/docs'
        },
        'sections': [
            {'name': 'concepts', 'path': 'concepts/'},
            {'name': 'tasks', 'path': 'tasks/'},
            {'name': 'troubleshooting', 'path': 'tasks/debug-application-cluster/'},
            {'name': 'reference', 'path': 'reference/'}
        ]
    }
}
```

### 4.3 –ó–∞–≥—Ä—É–∑–∫–∞ StackOverflow –¥–∞–Ω–Ω—ã—Ö

```python
import stackapi
from datetime import datetime, timedelta

class StackOverflowVersionedScraper:
    def __init__(self):
        self.so = stackapi.StackAPI('stackoverflow')
        
    def scrape_by_product_versions(self, product_versions_map):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã/–æ—Ç–≤–µ—Ç—ã —Å SO –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞"""
        all_questions = {}
        
        for product, versions in product_versions_map.items():
            print(f"Scraping StackOverflow for {product}...")
            
            # –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É
            general_questions = self._get_questions_by_tag(product)
            
            # –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
            version_specific = {}
            for version in versions:
                version_questions = self._get_version_specific_questions(
                    product, version
                )
                if version_questions:
                    version_specific[version] = version_questions
                    
            all_questions[product] = {
                'general': general_questions,
                'version_specific': version_specific
            }
            
            time.sleep(2)  # Rate limiting
            
        return all_questions
    
    def _get_questions_by_tag(self, tag, max_questions=500):
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–≥—É"""
        try:
            questions = self.so.fetch('questions',
                                    tagged=tag,
                                    sort='votes',
                                    order='desc',
                                    pagesize=100,
                                    max_pages=max_questions//100,
                                    filter='withbody')
            
            result = []
            for question in questions['items']:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç—ã
                answers = self.so.fetch('answers',
                                      ids=[question['question_id']],
                                      sort='votes',
                                      order='desc',
                                      filter='withbody')
                
                result.append({
                    'title': question['title'],
                    'question': question.get('body', ''),
                    'answers': [
                        {
                            'body': a.get('body', ''),
                            'score': a.get('score', 0),
                            'accepted': a.get('is_accepted', False)
                        }
                        for a in answers.get('items', [])
                    ],
                    'tags': question['tags'],
                    'score': question['score'],
                    'creation_date': question['creation_date']
                })
                
            return result
            
        except Exception as e:
            print(f"Error fetching questions for {tag}: {e}")
            return []
            
    def _get_version_specific_questions(self, product, version):
        """–ü–æ–∏—Å–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞ –≤–µ—Ä—Å–∏–∏
        search_terms = [
            f"{product} {version}",
            f"{product}-{version}",
            f"{product} version {version}"
        ]
        
        all_results = []
        for term in search_terms:
            try:
                results = self.so.fetch('search/advanced',
                                      q=term,
                                      tagged=product,
                                      sort='votes',
                                      order='desc',
                                      pagesize=50)
                
                if results.get('items'):
                    all_results.extend(results['items'])
                    
            except Exception as e:
                print(f"Error searching for {term}: {e}")
                
        return all_results[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
```

## 5. –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π

### 5.1 –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–µ

```python
import hashlib
from datetime import datetime

class InfrastructureMonitor:
    def __init__(self, versions_csv_path, inventory_path):
        self.versions_csv_path = versions_csv_path
        self.inventory_path = inventory_path
        self.last_checksums = self._calculate_checksums()
        
    def _calculate_checksums(self):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Å—É–º–º—ã —Ñ–∞–π–ª–æ–≤"""
        checksums = {}
        
        for file_path in [self.versions_csv_path, self.inventory_path]:
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    checksums[file_path] = hashlib.md5(f.read()).hexdigest()
                    
        return checksums
    
    def check_for_changes(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–∞—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        current_checksums = self._calculate_checksums()
        changes = {}
        
        for file_path, current_checksum in current_checksums.items():
            old_checksum = self.last_checksums.get(file_path)
            if old_checksum != current_checksum:
                changes[file_path] = {
                    'old_checksum': old_checksum,
                    'new_checksum': current_checksum,
                    'changed_at': datetime.now()
                }
                
        if changes:
            self.last_checksums = current_checksums
            
        return changes
    
    def get_new_versions(self, old_versions_map):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        current_analyzer = VersionAnalyzer(self.versions_csv_path)
        current_versions = current_analyzer.get_unique_versions()
        
        new_versions = {}
        for product, versions in current_versions.items():
            old_product_versions = set(old_versions_map.get(product, []))
            current_product_versions = set(versions)
            
            new_product_versions = current_product_versions - old_product_versions
            if new_product_versions:
                new_versions[product] = list(new_product_versions)
                
        return new_versions

class KnowledgeUpdater:
    def __init__(self, knowledge_manager, scraper):
        self.knowledge_manager = knowledge_manager
        self.scraper = scraper
        
    async def update_for_new_versions(self, new_versions_map):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∑–Ω–∞–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π"""
        for product, versions in new_versions_map.items():
            if product in PRODUCT_CONFIGS:
                config = PRODUCT_CONFIGS[product]
                
                print(f"Updating documentation for {product} versions: {versions}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π
                new_docs = await self.scraper.scrape_product_versions(
                    config, versions
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
                await self.knowledge_manager.add_versioned_docs(
                    product, new_docs
                )
                
                # –û–±–Ω–æ–≤–ª—è–µ–º StackOverflow –¥–∞–Ω–Ω—ã–µ
                await self._update_stackoverflow_data(product, versions)
                
    async def _update_stackoverflow_data(self, product, versions):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ StackOverflow –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π"""
        so_scraper = StackOverflowVersionedScraper()
        
        for version in versions:
            version_questions = so_scraper._get_version_specific_questions(
                product, version
            )
            
            if version_questions:
                await self.knowledge_manager.add_stackoverflow_data(
                    product, version, version_questions
                )
```

## 6. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –¥–≤–∏–∂–æ–∫

### 6.1 –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏—è–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

```python
import uuid
from typing import Dict, List, Any

class SessionManager:
    def __init__(self):
        self.sessions: Dict[str, Dict] = {}
        self.max_context_length = 10  # –ú–∞–∫—Å–∏–º—É–º —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        
    def create_session(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é"""
        session_id = str(uuid.uuid4())
        self.sessions[session_id] = {
            'created_at': datetime.now(),
            'context': [],
            'infrastructure_context': {},
            'last_activity': datetime.now()
        }
        return session_id
    
    def add_to_context(self, session_id: str, query: str, response: str, 
                      infrastructure_context: Dict):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏"""
        if session_id not in self.sessions:
            session_id = self.create_session()
            
        session = self.sessions[session_id]
        session['context'].append({
            'timestamp': datetime.now(),
            'query': query,
            'response': response,
            'infrastructure_context': infrastructure_context
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(session['context']) > self.max_context_length:
            session['context'] = session['context'][-self.max_context_length:]
            
        session['last_activity'] = datetime.now()
        
    def get_session_context(self, session_id: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏"""
        return self.sessions.get(session_id, {}).get('context', [])
        
    def cleanup_old_sessions(self, max_age_hours: int = 24):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏"""
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        sessions_to_remove = [
            sid for sid, session in self.sessions.items()
            if session['last_activity'] < cutoff_time
        ]
        
        for sid in sessions_to_remove:
            del self.sessions[sid]

class InfrastructureContextEngine:
    def __init__(self, inventory_path, versions_csv_path):
        self.inventory_parser = AnsibleInventoryParser(inventory_path)
        self.version_analyzer = VersionAnalyzer(versions_csv_path)
        
    def get_context_for_query(self, query: str, session_context: List[Dict] = None):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        context = {
            'servers': [],
            'products': [],
            'versions': {},
            'related_infrastructure': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–æ–≤/–ø—Ä–æ–¥—É–∫—Ç–æ–≤
        mentioned_servers = self._extract_server_names(query)
        mentioned_products = self._extract_product_names(query)
        
        # –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä—ã —É–ø–æ–º—è–Ω—É—Ç—ã —è–≤–Ω–æ
        if mentioned_servers:
            for server in mentioned_servers:
                server_info = self.inventory_parser.get_server_info(server)
                server_versions = self.version_analyzer.get_server_stack(server)
                
                context['servers'].append({
                    'name': server,
                    'info': server_info,
                    'software_stack': server_versions
                })
                
        # –ï—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç—ã –ø—Ä–æ–¥—É–∫—Ç—ã
        if mentioned_products:
            for product in mentioned_products:
                product_servers = self.inventory_parser.get_servers_by_product(product)
                product_versions = self._get_product_versions(product)
                
                context['products'].append({
                    'name': product,
                    'servers': product_servers,
                    'versions': product_versions
                })
                
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if session_context:
            previous_context = self._extract_session_infrastructure_context(
                session_context
            )
            context['related_infrastructure'].extend(previous_context)
            
        return context
    
    def _extract_server_names(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤
        known_servers = self.inventory_parser.get_all_server_names()
        
        mentioned = []
        query_lower = query.lower()
        
        for server in known_servers:
            if server.lower() in query_lower:
                mentioned.append(server)
                
        return mentioned
    
    def _extract_product_names(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        known_products = ['ceph', 'kafka', 'prometheus', 'grafana', 
                         'opensearch', 'nginx', 'kubernetes']
        
        mentioned = []
        query_lower = query.lower()
        
        for product in known_products:
            if product in query_lower:
                mentioned.append(product)
                
        return mentioned

class AnsibleInventoryParser:
    def __init__(self, inventory_path):
        self.inventory_path = inventory_path
        self.inventory = self._load_inventory()
        
    def _load_inventory(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Ansible inventory"""
        if self.inventory_path.endswith('.yml') or self.inventory_path.endswith('.yaml'):
            with open(self.inventory_path, 'r') as f:
                return yaml.safe_load(f)
        else:
            # –ü–∞—Ä—Å–∏–Ω–≥ INI —Ñ–æ—Ä–º–∞—Ç–∞
            import configparser
            config = configparser.ConfigParser(allow_no_value=True)
            config.read(self.inventory_path)
            return dict(config)
    
    def get_server_info(self, server_name: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Ä–≤–µ—Ä–µ –∏–∑ inventory"""
        # –õ–æ–≥–∏–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ inventory
        # –ü—Ä–∏–º–µ—Ä –¥–ª—è YAML —Ñ–æ—Ä–º–∞—Ç–∞
        for group_name, group_data in self.inventory.get('all', {}).get('children', {}).items():
            if 'hosts' in group_data:
                if server_name in group_data['hosts']:
                    return {
                        'group': group_name,
                        'host_vars': group_data['hosts'][server_name],
                        'group_vars': group_data.get('vars', {})
                    }
        return {}
    
    def get_servers_by_product(self, product: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–¥—É–∫—Ç"""
        servers = []
        
        # –ò—â–µ–º –≥—Ä—É–ø–ø—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º
        for group_name, group_data in self.inventory.get('all', {}).get('children', {}).items():
            if product.lower() in group_name.lower():
                if 'hosts' in group_data:
                    servers.extend(group_data['hosts'].keys())
                    
        return servers
    
    def get_all_server_names(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∏–º–µ–Ω–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑ inventory"""
        servers = []
        
        for group_name, group_data in self.inventory.get('all', {}).get('children', {}).items():
            if 'hosts' in group_data:
                servers.extend(group_data['hosts'].keys())
                
        return list(set(servers))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
```

## 7. –°–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –æ–±—É—á–µ–Ω–∏—è

### 7.1 –°–±–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏

```python
import sqlite3
from typing import Optional, List, Dict
from dataclasses import dataclass
from datetime import datetime

@dataclass
class FeedbackEntry:
    query: str
    response: str
    rating: int  # 1-5
    feedback_text: str
    context: Dict
    timestamp: datetime
    session_id: str

class FeedbackSystem:
    def __init__(self, db_path: str = "./feedback.db"):
        self.db_path = db_path
        self.init_database()
        
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS feedback (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                query TEXT NOT NULL,
                response TEXT NOT NULL,
                rating INTEGER NOT NULL,
                feedback_text TEXT,
                context_json TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                processed BOOLEAN DEFAULT FALSE
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS feedback_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT,
                pattern_description TEXT,
                correction_rule TEXT,
                confidence REAL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        
    def store_feedback(self, feedback: FeedbackEntry):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO feedback 
            (session_id, query, response, rating, feedback_text, context_json, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            feedback.session_id,
            feedback.query,
            feedback.response, 
            feedback.rating,
            feedback.feedback_text,
            json.dumps(feedback.context),
            feedback.timestamp
        ))
        
        conn.commit()
        conn.close()
        
    def get_negative_feedback(self, min_rating: int = 2) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM feedback 
            WHERE rating <= ? AND processed = FALSE
            ORDER BY timestamp DESC
        ''', (min_rating,))
        
        rows = cursor.fetchall()
        conn.close()
        
        return [dict(zip([col[0] for col in cursor.description], row)) for row in rows]
        
    def analyze_feedback_patterns(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        negative_feedback = self.get_negative_feedback()
        
        patterns = {
            'incorrect_commands': [],
            'wrong_versions': [],
            'missing_context': [],
            'configuration_errors': []
        }
        
        for feedback in negative_feedback:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
            feedback_text = feedback['feedback_text'].lower()
            
            if any(word in feedback_text for word in ['command', 'script', 'wrong']):
                patterns['incorrect_commands'].append(feedback)
            elif any(word in feedback_text for word in ['version', 'outdated']):
                patterns['wrong_versions'].append(feedback)
            elif any(word in feedback_text for word in ['context', 'server', 'missing']):
                patterns['missing_context'].append(feedback)
            elif any(word in feedback_text for word in ['config', 'parameter']):
                patterns['configuration_errors'].append(feedback)
                
        return patterns
        
    def create_learning_rules(self, patterns: Dict):
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        rules = []
        
        # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
        if patterns['incorrect_commands']:
            rules.append({
                'type': 'command_validation',
                'description': 'Validate commands against infrastructure context',
                'action': 'cross_reference_with_inventory'
            })
            
        # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
        if patterns['wrong_versions']:
            rules.append({
                'type': 'version_matching',
                'description': 'Always use exact version from infrastructure',
                'action': 'strict_version_lookup'
            })
            
        return rules

class LearningEngine:
    def __init__(self, feedback_system: FeedbackSystem, knowledge_manager):
        self.feedback_system = feedback_system
        self.knowledge_manager = knowledge_manager
        self.learning_rules = []
        
    def process_feedback(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""
        patterns = self.feedback_system.analyze_feedback_patterns()
        new_rules = self.feedback_system.create_learning_rules(patterns)
        
        self.learning_rules.extend(new_rules)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
        self._update_system_prompts()
        
    def _update_system_prompts(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –æ–±—É—á–µ–Ω–∏—è"""
        base_prompt = self._get_base_system_prompt()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        learning_additions = []
        
        for rule in self.learning_rules:
            if rule['type'] == 'command_validation':
                learning_additions.append(
                    "CRITICAL: Always cross-reference generated commands with the actual infrastructure inventory before providing them."
                )
            elif rule['type'] == 'version_matching':
                learning_additions.append(
                    "CRITICAL: Use ONLY the exact software versions specified in the infrastructure version database."
                )
                
        if learning_additions:
            updated_prompt = base_prompt + "\n\nLEARNING RULES:\n" + "\n".join(learning_additions)
            return updated_prompt
            
        return base_prompt
    
    def _get_base_system_prompt(self):
        return """
You are an expert infrastructure management assistant with deep knowledge of open-source technologies.

CORE PRINCIPLES:
1. Always use the specific versions of software deployed in the infrastructure
2. Reference the exact server configurations from the Ansible inventory
3. Provide commands and scripts specific to the actual environment
4. Explain potential risks and prerequisites for any suggested actions
5. When analyzing logs, consider the specific product versions and configurations

INFRASTRUCTURE CONTEXT USAGE:
- Server information comes from Ansible inventory
- Software versions come from the versions database
- Always match documentation to the deployed versions
- Consider dependencies between different components
"""