Рассмотрим такую ситуацию. Есть некоторая ИТ инфраструктура, в которой развернуты различные opensource продукты. Например ceph, kafka, prometheus, grafana, opensearch, nginx, kubernetes.

Соответственно есть  инвентарь для ansible, в котором перечислены сервера, на которых развернуты эти продукты.

Есть csv файл, в котором указаны версии операционной системы и версии установленного ПО на серверах - далее назовем это "база по версиям".

Задача - с помощью on-premise LLM:

- понимать причины сбоев, на основе данных из логов.

- создавать скрипты/команды для выполнения тех или иных действий.

- отвечать на вопросы по продуктам ( конфигурация, сопровождение, и т.д.)

И все это делать именно в контексте этой самой инфраструктуры. То есть при формировании ответа должна использоваться информация из ansible инвентаря и из "базы о версиях" на текущий момент времени.

Решение должно помнить контекст запроса.

Дополнительное условие - в процессе работы нет доступа к интернет, но при настройке решения возможно получение доступа к официальной документации (предварительная загрузка информации) по продуктам, к stackoverflow и т.п ( далее - "информация из Интернет"). 

Опиши как конкретно реализовать такую предварительную загрузку , учитывая, что в инфраструктуре может быть несколько инстансов одного и того же продукта с разными версиями.

Соответственно и документацию при формировании ответа для конкретного инстанса надо брать для версии ПО развернутого на  этом конкретном инстансе.

Соответственно и закачивать официальную документацию нужно для разных версий каждого продукта, а конкретные версии надо брать из "базы о версиях".

Так же опиши как обновлять (дополнять) "информацию из Интернет" в связи с установкой новых версий продуктов и/или появлению новой информации на stackoverflow.

Желательно, чтобы модель имела возможность получать обратную связь от пользователя, содержащую информацию о правильности или неправильности преложенного моделью решения, и в дальнейшем учитывала эту информацию.

Какая LLM для этого оптимальней? Как реализовать такое решение? 

Какое требуется оборудование, исходя из ограничения - есть только виртуальные машины без GPU?

Приведи пожалуйста подробное описание.


