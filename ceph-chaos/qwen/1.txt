Организация **Chaos Engineering** в кластере Ceph — это важная практика для проверки устойчивости, отказоустойчивости и восстановления сервисов. В вашем случае (Ceph 17.2.7, 4 RGW, 5 MON+MGR, 6 OSD) можно реализовать автоматизированный процесс, который будет симулировать различные виды отказов.

---

## 🔧 Цели Chaos Engineering в Ceph:

- Проверка отказоустойчивости Ceph при падении отдельных компонентов.
- Тестирование времени восстановления данных после потери OSD/монитора.
- Уверенность в правильной работе механизма self-healing.
- Проверка поведения клиентских сервисов при недоступности части инфраструктуры.

---

## 📌 Основные типы тестов

### 1. **OSD Failures**
- Остановка одного или нескольких OSD.
- Изоляция сети между OSD и другими нодами.
- Искусственное заполнение диска (например, `dd if=/dev/zero`).
- Запуск IO ошибок (через `scsi_debug`, `tc`, `iptables`).

### 2. **MON/MGR Failures**
- Остановка одного или более мониторов.
- Отказ Quorum: проверка поведения при потере большинства мониторов.
- Задержки в сети между мониторами.
- Коррупция данных в хранилище монитора (для продвинутых случаев).

### 3. **RGW Failures**
- Остановка одной или нескольких RGW нод.
- Сетевые проблемы (затухание, задержки, блокировка портов).
- Перегрузка RGW (нагрузочное тестирование + одновременный отказ).

### 4. **Сетевые сбои**
- Разрыв связи между OSD и клиентами.
- Разделение кластера на "сетевой остров".
- Искусственные задержки и дроп пакетов (`tc netem`).

### 5. **Процессы восстановления**
- Добавление/удаление OSD.
- Перезапуск мониторов.
- Восстановление из резервной копии.

---

## 🛠️ Инструменты для реализации

### ✅ 1. **Chaos Mesh (Kubernetes)**  
Если вы работаете в Kubernetes:
- Поддерживает сетевые, pod, io, ядерные сбои.
- Удобный UI и CRD API.

### ✅ 2. **Pumba (Docker)**  
Для сред без Kubernetes:
- Работает с Docker контейнерами.
- Может останавливать, перезапускать, создавать сетевые сбои.

### ✅ 3. **LitmusChaos**  
Платформа для Chaos Engineering в K8s:
- Поддерживает множество экспериментов.
- Хорошо документирована.

### ✅ 4. **Custom скрипты + Ansible / SaltStack / Bash**

Если вы не используете контейнеры:

```bash
# Пример: остановка случайного OSD
OSD_LIST=("osd0" "osd1" "osd2" "osd3" "osd4" "osd5")
random_osd=${OSD_LIST[$RANDOM % ${#OSD_LIST[@]}]}

systemctl stop ceph-osd@$random_osd
sleep 60
systemctl start ceph-osd@$random_osd
```

Также можно использовать `systemd`, `pcs`, `crm`, если используется кластерный менеджер.

---

## 🧪 Практические рекомендации

### 📋 План действий

1. **Определите матрицу сценариев:**
   - Одиночный отказ OSD.
   - Потеря двух мониторов.
   - Отказ RGW + сеть между OSD.
   - И т.д.

2. **Запишите метрики до и после:**
   - IOPS, латентность.
   - Состояние PG (`ceph -s`, `ceph pg stat`).
   - Клиентские запросы (RGW, RBD).
   - Использование CPU/IO во время восстановления.

3. **Используйте нагрузку во время тестов:**
   - Для RGW: `radosgw-agent`, `s3cmd`, `mc`, JMeter.
   - Для RBD: `fio`, `dd`, `cosbench`.

4. **Фиксируйте результаты:**
   - Автоматические отчеты.
   - Логи Ceph.
   - Графики Prometheus/Grafana.

---

## 🧰 Полезные команды

```bash
# Проверить состояние кластера
ceph -s

# Посмотреть состояние OSD
ceph osd tree

# Проверить состояние мониторов
ceph quorum_status

# Статистика по PG
ceph pg stat

# Остановка OSD
systemctl stop ceph-osd@<id>

# Остановка монитора
systemctl stop ceph-mon@<hostname>

# Остановка MGR
systemctl stop ceph-mgr@<hostname>

# Остановка RGW
systemctl stop ceph-radosgw@rgw.<instance-name>
```

---

## ⚙️ Расписание и автоматизация

Можно написать cron-скрипт или использовать CI/CD pipeline:

```yaml
---
- name: Chaos Test: Stop random OSD
  hosts: osds
  tasks:
    - name: Choose a random OSD to stop
      shell: echo "{{ play_hosts | random }}" > /tmp/osd_to_kill.txt
      delegate_to: localhost

    - name: Read the chosen OSD
      slurp:
        src: /tmp/osd_to_kill.txt
      register: target_osd

    - name: Stop the selected OSD
      become: yes
      systemd:
        name: ceph-osd@{{ target_osd['content'] | b64decode | basename }}
        state: stopped

    - name: Wait for 60 seconds
      pause: 60

    - name: Start the selected OSD
      become: yes
      systemd:
        name: ceph-osd@{{ target_osd['content'] | b64decode | basename }}
        state: started
```

---

## 📈 Мониторинг и анализ

- Prometheus + Grafana: отслеживайте метрики Ceph.
- ELK Stack: анализ логов.
- Alertmanager: оповещение при падении PG, OSD, MON.
- Сравнивайте статистику до/во время/после теста.

---

## 🧠 Важно помнить

- **Не проводите тесты в production без предварительного планирования и согласования.**
- Начните с тестовой среды.
- Все действия должны быть обратимыми.
- Фиксируйте все изменения и результаты.

---

## 🧩 Возможные расширения

- Создание чек-листов для SRE/DevOps.
- Интеграция с CI/CD (запуск тестов перед обновлением версии Ceph).
- Интеграция с PagerDuty/Slack для автоматического уведомления при падении.

---

Хочешь пример конкретного bash-скрипта или playbook'а под свой кластер?
