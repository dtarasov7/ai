Ниже — разбор именно для типичного kubeadm‑кластера (Kubernetes 1.24) с **3 control-plane (мастер) нодами** и **stacked etcd** (etcd “на мастерах”) в виде **static Pod / контейнера**, который запускает kubelet.

---

## Коротко (самое важное)
- **etcd в контейнере на master‑нодах — это нормальная и широко используемая схема** для kubeadm: kubeadm генерирует манифест etcd как *static Pod* и монтирует *dataDir* с хоста через `hostPath`.   
- **Слабое место не “контейнер”, а то, что etcd и control-plane живут на одних и тех же 3 нодах** (stacked topology): любой инцидент, который “гасит” все мастера, одновременно “гасит” и etcd.   
- Если **все 3 master перезагрузятся одновременно**, будет **полный даунтайм control-plane** (API, контроллеры, планировщик) и **etcd потеряет кворум на время простоя**. Подняться обратно обычно сможет, когда ноды вернутся, но пока их нет — изменений в кластере не будет (а запущенные workload’ы часто продолжают работать).   
- База etcd **хранится на диске хоста в `dataDir` (по умолчанию `/var/lib/etcd`)**, а внутри — `member/snap` и `member/wal`.   
- **Бэкап делается снимком (`etcdctl snapshot save`)**: либо изнутри контейнера, либо с хоста (важно иметь правильные TLS‑серты/ключи и подходящую версию утилит).   

---

## 1) Насколько “безопасно” запускать etcd как контейнер на master‑нодах?

### По доступности (HA)
Это стандартная kubeadm‑архитектура “**stacked control plane**”: etcd‑члены и компоненты control-plane (apiserver/controller/scheduler) **сосуществуют на одних и тех же control-plane нодах**. Kubernetes документация прямо описывает два подхода: stacked и external etcd.   

**Плюсы stacked:**
- меньше инфраструктуры, проще разворачивать и обслуживать.

**Минусы stacked (главный):**
- общий “failure domain”: если у вас проблема с control-plane нодами (электричество, сеть, массовая перезагрузка/обновление, общий сторедж/гипервизор), вы одновременно теряете и etcd, и API.

Итого: **сам факт “etcd в контейнере” обычно не ухудшает HA**, но **co-location** (etcd на тех же нодах) ухудшает устойчивость к “массовым” отказам по сравнению с отдельным etcd‑кластером.   

### По безопасности (security)
В kubeadm‑варианте etcd запускается как static Pod: kubelet читает манифесты из `/etc/kubernetes/manifests` и поднимает компоненты ещё до полной готовности сети кластера.   
Для local etcd kubeadm генерирует манифест так, чтобы:
- etcd работал с `HostNetwork=true`
- данные хранились на хосте через `hostPath` (то есть это *не* “эфемерные слои контейнера”).   

С точки зрения security, container vs systemd часто **не решает ключевых рисков**, потому что:
- etcd всё равно имеет доступ к своему dataDir на хосте (hostPath),
- сеть чаще всего hostNetwork,
- безопасность определяется: **TLS, фаерволл/ACL на 2379/2380, контроль доступа на ноды, защита файлов сертификатов, hardening OS**.

---

## 2) Что будет, если одновременно перезагрузятся все 3 master‑ноды?

Разделю на “что будет во время” и “что будет после”.

### Во время одновременной перезагрузки
- etcd‑кластер **полностью недоступен** (0/3), значит **нет кворума**, значит **нельзя коммитить записи**. В etcd прогресс возможен только при наличии большинства (кворума).   
- Kubernetes control-plane тоже недоступен: apiserver/controller/scheduler не работают.

Практический эффект:
- **Новые изменения невозможны**: не создать/обновить ресурсы, не отработают контроллеры, не будет планирования новых Pod’ов.
- **Уже запущенные Pod’ы на worker’ах часто продолжают работать**, потому что контейнеры уже запущены kubelet’ом и не “исчезают” только из‑за падения API. Но любые операции, требующие API/контроллеров, встанут. Kubernetes прямо отмечает сценарий, когда при проблемах с большинством etcd “scheduled pods might continue to run, no new pods can be scheduled / no changes”.   

### После одновременной перезагрузки
Когда ноды вернутся:
- etcd поднимется, проведёт выборы лидера и вернётся в healthy, **когда большинство членов снова доступно**.   
- затем поднимутся apiserver/controller/scheduler (как static pods).

Что может пойти не так:
- если одновременно с ребутом есть проблемы с диском/FS (коррупция dataDir, потеря диска на 2 нодах) — вы рискуете “majority failure”, и тогда восстановление уже через DR/restore. etcd описывает, что при потере большинства кластер не может принимать записи и восстановится только когда вернётся большинство, иначе нужен disaster recovery.   

**Вывод:** “одновременно ребутнуть все 3” — это гарантированный даунтайм control-plane. Данные обычно не теряются, если диски целы, но это сценарий, где **бэкап etcd критичен**.

---

## 3) Где хранится БД etcd в такой схеме?

В kubeadm это почти всегда так:

1) kubeadm хранит static pod манифесты в:
- `/etc/kubernetes/manifests` (kubelet “watch” этот каталог).   

2) Для local etcd kubeadm указывает **dataDir на хосте**, по умолчанию:
- `dataDir: "/var/lib/etcd"`   

3) Внутри dataDir типичная структура etcd:
- `member/` (внутри `snap/` и `wal/`) — snapshots и write-ahead log.   

### Как проверить на ваших нодах (быстро)
На каждой master:
- открыть `/etc/kubernetes/manifests/etcd.yaml` и посмотреть:
  - аргумент `--data-dir=...`
  - `volumes: hostPath: path: ...` для volume типа `etcd-data` (имя может отличаться)

Это важно: если *вдруг* dataDir не вынесен на hostPath (редко для kubeadm, но встречается в “самосборных” вариантах), то данные реально окажутся внутри хранилища container runtime — и это сильно хуже для предсказуемости/бэкапов.

---

## 4) Имеет ли смысл переходить на etcd как сервисы под systemd?

### Если речь про “оставить etcd на тех же трёх master, но вместо контейнера сделать systemd”
Честно: **обычно не даёт принципиального выигрыша** по надёжности/безопасности:
- риск “все три мастера ушли в ребут/даун” остаётся тем же (stacked failure domain),
- dataDir и так на хосте,
- доступ к сети/диску сопоставимый.

При этом вы можете:
- усложнить апгрейды/сопровождение (kubeadm ожидает управлять local etcd как static pod),
- получить “двойное управление” (часть через kubeadm, часть руками).

### Если речь про “перейти на external etcd (отдельные ноды), и там уже systemd”
Вот это — **реальный архитектурный шаг**, который уменьшает связность отказов:
- external topology “decouples control plane and etcd” и делает потерю control plane или потерю etcd‑члена менее разрушительной для общей избыточности (ценой удвоения количества хостов).   
- Kubernetes документация по HA etcd с kubeadm прямо описывает подготовку отдельных хостов под etcd, где обычно systemd как базовая предпосылка окружения.   

**Практический вывод:**  
- “systemd вместо контейнера на тех же нодах” — обычно косметика/вкусовщина.  
- “external etcd на отдельных нодах” — осмысленное улучшение отказоустойчивости (особенно против сценария “все control-plane недоступны”).  

---

## 5) Как делать резервное копирование, если “в контейнере нет ничего кроме etcd”?

### Бэкап №1 (рекомендуемый): `etcdctl snapshot save`
Kubernetes прямо рекомендует регулярно бэкапить etcd и указывает, что:
- все объекты Kubernetes хранятся в etcd,
- снапшот можно делать `etcdctl snapshot save`,
- снапшоты стоит шифровать (там будут Secrets и т.п.).   

Базовая форма команды такая:   
- `ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db`

И вариант с TLS параметрами (типично для kubeadm и production):   
- `--endpoints=https://127.0.0.1:2379 --cacert=... --cert=... --key=... snapshot save ...`

#### Где взять cert/key?
- kubeadm генерирует и хранит PKI под `/etc/kubernetes/pki`, включая etcd‑серты (например, `etcd/peer.*`, `etcd/healthcheck-client.*`).   
Точные файлы зависят от того, чем вам удобнее аутентифицироваться (healthcheck-client часто используют для админских проверок/бэкапов в kubeadm‑кластерах).

#### А если `etcdctl` внутри контейнера нет?
Тогда два типовых пути:

1) **Поставить `etcdctl` на хост** (скачать/установить бинарь) версии, совместимой с вашей etcd.  
   Kubernetes отмечает, что восстановление поддерживается в рамках same major.minor etcd (patch может отличаться).   

2) **Запустить `etcdctl` “временным контейнером”** (не через Kubernetes), используя container runtime (`crictl run` и т.п.). Kubernetes документация прямо упоминает этот подход: “если `etcdctl` недоступен — можно запускать его внутри container image напрямую инструментами runtime, не через Kubernetes”.   

### Бэкап №2: копирование файла БД с диска (только аккуратно)
Kubernetes пишет, что снапшот можно получить и копированием файла `member/snap/db` **из data directory**, но важно, чтобы директория **не использовалась запущенным etcd процессом**.   

etcd дополнительно предупреждает нюанс: копирование `member/snap/db` может упустить данные, которые ещё находятся в WAL (write-ahead log).   

То есть на практике:
- для “горячего” бэкапа лучше `etcdctl snapshot save`,
- файловое копирование — как аварийный/особый вариант и только при понимании консистентности.

### Проверка снапшота
Kubernetes рекомендует проверять снапшоты через `etcdutl snapshot status` (а `etcdctl snapshot status` считается deprecated начиная с v3.5).   

---

## 6) Практический “рецепт” бэкапа для вашей схемы (идея)
Без привязки к вашей ОС/CRI, логика такая:

1) На одной master‑ноде (или по очереди на каждой, но **достаточно одной**) запускаете:
- `etcdctl snapshot save /backup/etcd-$(date ...).db`

2) Файлы снапшотов:
- **шифруете** и **вывозите вне кластера** (S3/MinIO/бэкап‑сервер/лента).

3) Делаете retention (например 7 ежедневных + 4 недельных + 6 месячных).

Почему достаточно одной ноды: снапшот запрашивается у одного endpoint (точка входа одна), это нормальный режим для etcd snapshot.   

---

## 7) Что обычно “улучшает жизнь” больше всего (рядом с вашим вопросом)
1) **Исключить одновременную перезагрузку всех трёх** (процедуры/автоматика):
- rolling reboot “по одной ноде” всегда предпочтительнее — etcd сам рекомендует при работах с несколькими нодами делать изменения по одной.   

2) **Разнести мастера по независимым доменам отказа**:
- разные hypervisor’ы/стойки/зоны,
- разные UPS/питание,
- разные TOR‑коммутаторы, если возможно.

3) **Подумать о 5‑членном etcd**, если вам критична доступность и вы реально ловите “вторую одновременную потерю”. Kubernetes документация прямо говорит, что 5‑членный etcd часто рекомендуют для production на поддерживаемых масштабах (это про надёжность ценой latency).   

4) **Периодически тестировать restore на стенде** (не в проде): бэкап, который ни разу не восстанавливали, — это гипотеза.

---

Если обобщить одним предложением: **контейнерный etcd на 3 мастерах — это нормальная kubeadm‑практика; опасность не в контейнере, а в “stacked” архитектуре и в сценариях массового недоступного control-plane; лечится регулярными снапшотами etcd и снижением вероятности “все три мастера легли одновременно”, а архитектурно — external etcd.**
