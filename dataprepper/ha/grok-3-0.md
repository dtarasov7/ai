Конечно, спасибо за уточнения! Поскольку вы указали, что речь идет о **статической конфигурации** (без Kubernetes или других оркестраторов, например, на виртуальных машинах, Docker-контейнерах или bare-metal хостах), я адаптирую ответ соответственно. Мы сосредоточимся на обработке **трейсов** с необходимостью построения **service-map** в Data Prepper. 

### Важные замечания по service-map и статической среде
- **Service-map в Data Prepper**: Это stateful-процессор (`service_map_stateful`), который анализирует трейсы для построения графа зависимостей между сервисами. Он требует агрегации всех спанов (spans) по ключам вроде `traceId` и `spanId`. Если спаны одного трейса попадут на разные узлы Data Prepper без координации, service-map будет некорректной или неполной. Поэтому **Peer Forwarding** (межузловая пересылка) обязателен: он использует консистентное хеширование для маршрутизации спанов на "ответственный" узел.
- **Статическая конфигурация**: Без динамического обнаружения (как в Kubernetes), для Peer Forwarding используем режим `static` (жесткий список IP-адресов всех узлов Data Prepper). Это означает, что при добавлении/удалении узлов нужно обновлять конфигурацию на всех существующих узлах и перезапускать их. Альтернатива — DNS, если у вас есть DNS-сервер (например, Consul или внутренний DNS), который может разрешать одно имя в несколько IP.
- **HA и масштабирование**: Для высокой доступности (HA) запускайте минимум 3 узла Data Prepper. Горизонтальное масштабирование — добавление узлов с обновлением конфигураций. Все узлы Data Prepper должны отправлять данные в один кластер OpenSearch (который тоже должен быть HA, но это за пределами вопроса).
- **Предположения**: Узлы Data Prepper на хостах с фиксированными IP (например, 192.168.1.10, 192.168.1.11, 192.168.1.12). OTLP использует gRPC (порт 21890 по умолчанию). Включите TLS/mTLS для безопасности в проде.

Я рассмотрю два варианта, как просили, с примерами конфигураций.

---

### Вариант 1: Использование балансировщика нагрузки (Рекомендуемый для статической среды)

**Архитектура:** `otelcol -> Load Balancer -> [Data Prepper 1 (IP1), Data Prepper 2 (IP2), ..., Data Prepper N (IPN)] -> OpenSearch`

Балансировщик (например, NGINX или HAProxy на отдельном хосте) принимает трафик от `otelcol` и распределяет его по узлам Data Prepper (round-robin). Это упрощает конфигурацию `otelcol`, но требует настройки балансировщика.

#### Конфигурация
**1. OpenTelemetry Collector (`otelcol-config.yaml`):**
Укажите один эндпоинт — IP/порт балансировщика.

```yaml
exporters:
  otlp:
    endpoint: "192.168.1.5:21890"  # IP балансировщика (например, NGINX)
    tls:
      insecure: false  # Настройте сертификаты
    sending_queue:
      enabled: true  # Retry для HA

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
```

**2. Балансировщик (пример для NGINX):**
Установите NGINX на отдельном хосте (IP 192.168.1.5). Конфигурация `/etc/nginx/nginx.conf`:

```nginx
http {
    upstream dataprepper {
        server 192.168.1.10:21890;  # Data Prepper 1
        server 192.168.1.11:21890;  # Data Prepper 2
        server 192.168.1.12:21890;  # Data Prepper 3
    }

    server {
        listen 21890 http2;  # Поддержка gRPC (OTLP)
        location / {
            grpc_pass grpc://dataprepper;
            health_check;  # Проверка здоровья узлов
        }
    }
}
```
- Для HAProxy: Аналогично настройте backend с health checks.
- При масштабировании: Добавьте новый IP в upstream и перезагрузите балансировщик.

**3. Data Prepper (`data-prepper-config.yaml` на каждом узле):**
Используйте `static` для peer discovery. Конфигурация одинакова на всех узлах.

```yaml
peer_forwarder:
  discovery_mode: static
  static_endpoints:  # Список всех IP узлов (включая свой собственный)
    - "192.168.1.10:21891"  # Порт для peer forwarding (по умолчанию 21891)
    - "192.168.1.11:21891"
    - "192.168.1.12:21891"
  ssl: true  # Для безопасной пересылки
  ssl_certificate_file: "/path/to/cert.crt"
  ssl_key_file: "/path/to/key.key"

# Пайплайн с service-map (stateful)
entry-pipeline:
  source:
    otlp:
      grpc:
        port: 21890
  processor:
    - service_map_stateful:  # Процессор для построения service-map
        window_duration: 60000  # Время агрегации (ms)
  sink:
    - opensearch:
        hosts: ["https://opensearch-host:9200"]
        index: "traces-index"
        trace_analytics: true  # Включает аналитику трейсов в OpenSearch
```

#### Плюсы/Минусы
- **Плюсы**: `otelcol` не зависит от списка узлов; легко добавлять узлы (только обновить балансировщик); health checks в балансировщике повышают HA.
- **Минусы**: Дополнительный хост/компонент; при добавлении узла обновляйте upstream и перезагружайте балансировщик.
- **Подводные камни**: NGINX/HAProxy должны поддерживать gRPC. Тестируйте service-map в OpenSearch Dashboards — убедитесь, что граф строится корректно даже при сбое узла.

---

### Вариант 2: Указание нескольких эндпоинтов в конфигурации `otelcol`

**Архитектура:** `otelcol -> [Data Prepper 1 (IP1), Data Prepper 2 (IP2), ..., Data Prepper N (IPN)] -> OpenSearch`

`otelcol` сам балансирует нагрузку (round-robin) между списком IP. Нет внешнего балансировщика, но конфигурация `otelcol` жестко привязана к узлам.

#### Конфигурация
**1. OpenTelemetry Collector (`otelcol-config.yaml`):**
Укажите массив `endpoints`.

```yaml
exporters:
  otlp:
    endpoints:
      - "192.168.1.10:21890"
      - "192.168.1.11:21890"
      - "192.168.1.12:21890"
    loadbalancing:
      protocol: otlp
      resolver: static  # Статический список
    tls:
      insecure: false

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
```

**2. Data Prepper (`data-prepper-config.yaml` на каждом узле):**
Та же, как в Варианте 1 — с `static` для Peer Forwarding и процессором `service_map_stateful`.

```yaml
# Аналогично Варианту 1
peer_forwarder:
  discovery_mode: static
  static_endpoints:
    - "192.168.1.10:21891"
    - "192.168.1.11:21891"
    - "192.168.1.12:21891"
  # ... остальные настройки

# Пайплайн с service-map (как выше)
```

#### Плюсы/Минусы
- **Плюсы**: Нет внешнего балансировщика; прямая связь (меньше задержки); `otelcol` автоматически обходит неработающие узлы.
- **Минусы**: При масштабировании обновляйте и перезагружайте все `otelcol`; статическая природа делает ее менее гибкой.
- **Подводные камни**: Если узел упадет, `otelcol` перераспределит трафик, но Peer Forwarding обеспечит, чтобы stateful-данные (для service-map) не потерялись. Тестируйте на большом объеме трейсов.

---

### Сравнение и рекомендации
- **Когда выбрать Вариант 1**: Для сред с частым масштабированием или если `otelcol` много (балансировщик централизует изменения). Лучше для HA в статической среде.
- **Когда выбрать Вариант 2**: Для простых развертываний с фиксированным числом узлов (меньше компонентов).
- **Общие рекомендации**:
  - **Масштабирование**: При добавлении узла (например, новый IP 192.168.1.13) обновите `static_endpoints` на всех Data Prepper и перезапустите их. Для Варианта 1 — также обновите балансировщик; для Варианта 2 — `otelcol`.
  - **HA для service-map**: Установите `window_duration` в процессоре разумно (чтобы агрегация не задерживалась). Мониторьте метрики Data Prepper (Prometheus на порту 4900).
  - **Тестирование**: Генерируйте трейсы (например, с помощью Jaeger) и проверяйте service-map в OpenSearch (плагин Observability). Убедитесь, что при сбое узла данные не теряются.
  - **Альтернативы**: Если статическая среда слишком жесткая, рассмотрите легкую оркестрацию (Docker Compose). Для DNS-based discovery настройте DNS-сервер с A-записями для всех IP.

Если нужны более детальные конфиги или помощь с конкретными инструментами (NGINX, HAProxy), дайте знать!
