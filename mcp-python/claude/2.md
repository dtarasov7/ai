Отлично! Продолжаю создание курса.[1]

***

### `lectures/module05_llm_host.md`

```markdown
# Модуль 5. LLM-хост

## Взаимодействие с хостом

LLM-хост — это приложение, которое управляет взаимодействием пользователя с языковой моделью и координирует работу MCP-клиентов для доступа к внешним инструментам и данным.

### Роль хоста в архитектуре MCP

```
┌─────────────────────────────────────────┐
│           LLM Host Application          │
│  ┌─────────────┐      ┌──────────────┐  │
│  │   UI/UX     │      │  LLM Engine  │  │
│  │  (Chat)     │◄────►│  (Claude,    │  │
│  │             │      │   GPT, etc)  │  │
│  └─────────────┘      └──────┬───────┘  │
│                              │          │
│  ┌───────────────────────────▼────────┐ │
│  │      MCP Client Manager            │ │
│  │  ┌────────┐  ┌────────┐ ┌────────┐│ │
│  │  │Client 1│  │Client 2│ │Client N││ │
│  │  └───┬────┘  └───┬────┘ └───┬────┘│ │
│  └──────┼───────────┼──────────┼─────┘ │
└─────────┼───────────┼──────────┼───────┘
          │           │          │
          ▼           ▼          ▼
    MCP Server  MCP Server  MCP Server
    (Database)   (Files)    (Weather)
```

### Основные обязанности хоста

1. **Управление MCP-клиентами**
   - Запуск и остановка клиентских соединений
   - Конфигурация параметров подключения к серверам
   - Мониторинг состояния соединений

2. **Координация вызовов инструментов**
   - Парсинг запросов LLM на вызов инструментов
   - Маршрутизация вызовов к соответствующим серверам
   - Агрегация результатов от нескольких серверов

3. **Управление контекстом**
   - Предоставление ресурсов LLM как дополнительный контекст
   - Управление историей сообщений
   - Подстановка промптов в диалог

4. **Обработка ошибок**
   - Graceful degradation при недоступности серверов
   - Повторные попытки при временных сбоях
   - Информирование пользователя о проблемах

## Конфигурация хоста

Типичная конфигурация хоста для управления MCP-серверами:

### Формат конфигурации (JSON)

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "python",
      "args": ["/path/to/filesystem_server.py"],
      "env": {
        "ALLOWED_DIRS": "/home/user/projects"
      }
    },
    "database": {
      "command": "python",
      "args": ["/path/to/database_server.py"],
      "env": {
        "DB_CONNECTION": "postgresql://localhost/mydb"
      }
    },
    "remote-api": {
      "url": "https://api.example.com/mcp/sse",
      "headers": {
        "Authorization": "Bearer TOKEN"
      }
    }
  }
}
```

### Загрузка конфигурации в хосте

```python
import json
from pathlib import Path
from typing import Dict, Any
from mcp import StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

class MCPHostConfig:
    """
    Менеджер конфигурации MCP-хоста.
    
    Загружает и парсит конфигурацию серверов из JSON-файла.
    """
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: Путь к JSON-файлу конфигурации
        """
        self.config_path = Path(config_path)
        self.servers_config: Dict[str, Any] = {}
        self._load_config()
    
    def _load_config(self):
        """Загружает конфигурацию из файла."""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            self.servers_config = config.get("mcpServers", {})
    
    def get_stdio_params(self, server_name: str) -> StdioServerParameters:
        """
        Возвращает параметры для stdio-сервера.
        
        Args:
            server_name: Имя сервера из конфигурации
            
        Returns:
            Параметры запуска сервера
        """
        server_config = self.servers_config.get(server_name)
        if not server_config:
            raise ValueError(f"Сервер '{server_name}' не найден в конфигурации")
        
        return StdioServerParameters(
            command=server_config["command"],
            args=server_config.get("args", []),
            env=server_config.get("env", None)
        )
    
    def get_sse_url(self, server_name: str) -> tuple[str, dict]:
        """
        Возвращает URL и заголовки для SSE-сервера.
        
        Args:
            server_name: Имя сервера из конфигурации
            
        Returns:
            Кортеж (url, headers)
        """
        server_config = self.servers_config.get(server_name)
        if not server_config:
            raise ValueError(f"Сервер '{server_name}' не найден в конфигурации")
        
        return (
            server_config["url"],
            server_config.get("headers", {})
        )
```

## Управление множественными клиентами

Хост часто подключается к нескольким MCP-серверам одновременно:

```python
import asyncio
from typing import Dict
from mcp import ClientSession

class MCPClientManager:
    """
    Менеджер множественных MCP-клиентов.
    
    Управляет жизненным циклом всех активных клиентских соединений.
    """
    
    def __init__(self, config: MCPHostConfig):
        """
        Args:
            config: Конфигурация хоста
        """
        self.config = config
        self.sessions: Dict[str, ClientSession] = {}
        self.connections = {}
    
    async def connect_all(self):
        """
        Устанавливает соединения со всеми серверами из конфигурации.
        """
        # Запускаем подключения параллельно
        tasks = []
        for server_name in self.config.servers_config.keys():
            tasks.append(self._connect_server(server_name))
        
        # Ждём завершения всех подключений
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Обрабатываем ошибки
        for server_name, result in zip(self.config.servers_config.keys(), results):
            if isinstance(result, Exception):
                print(f"Ошибка подключения к '{server_name}': {result}")
    
    async def _connect_server(self, server_name: str):
        """
        Подключается к одному серверу.
        
        Args:
            server_name: Имя сервера
        """
        server_config = self.config.servers_config[server_name]
        
        # Определяем тип транспорта
        if "command" in server_config:
            # stdio транспорт
            params = self.config.get_stdio_params(server_name)
            read, write = await stdio_client(params).__aenter__()
        elif "url" in server_config:
            # SSE транспорт
            url, headers = self.config.get_sse_url(server_name)
            read, write = await sse_client(url, headers).__aenter__()
        else:
            raise ValueError(f"Неизвестный тип транспорта для '{server_name}'")
        
        # Сохраняем соединение
        self.connections[server_name] = (read, write)
        
        # Создаём сессию
        session = ClientSession(read, write)
        await session.__aenter__()
        
        # Инициализируем
        await session.initialize()
        
        # Сохраняем сессию
        self.sessions[server_name] = session
        
        print(f"Подключено к серверу '{server_name}'")
    
    async def disconnect_all(self):
        """
        Закрывает все активные соединения.
        """
        for server_name, session in self.sessions.items():
            try:
                await session.__aexit__(None, None, None)
                print(f"Отключено от '{server_name}'")
            except Exception as e:
                print(f"Ошибка при отключении от '{server_name}': {e}")
        
        self.sessions.clear()
        self.connections.clear()
    
    def get_session(self, server_name: str) -> ClientSession:
        """
        Возвращает сессию для указанного сервера.
        
        Args:
            server_name: Имя сервера
            
        Returns:
            Активная клиентская сессия
        """
        if server_name not in self.sessions:
            raise ValueError(f"Нет активного соединения с '{server_name}'")
        return self.sessions[server_name]
```

## Агрегация инструментов из множественных серверов

```python
from typing import List
import mcp.types as types

class ToolAggregator:
    """
    Агрегатор инструментов из нескольких MCP-серверов.
    
    Собирает все доступные инструменты и обеспечивает унифицированный доступ.
    """
    
    def __init__(self, client_manager: MCPClientManager):
        """
        Args:
            client_manager: Менеджер клиентов
        """
        self.client_manager = client_manager
        self.tools_map: Dict[str, str] = {}  # tool_name -> server_name
    
    async def discover_all_tools(self) -> List[types.Tool]:
        """
        Собирает инструменты со всех подключённых серверов.
        
        Returns:
            Список всех доступных инструментов
        """
        all_tools = []
        
        for server_name, session in self.client_manager.sessions.items():
            try:
                # Запрашиваем инструменты у сервера
                result = await session.list_tools()
                
                # Добавляем маппинг инструмент -> сервер
                for tool in result.tools:
                    # Проверяем коллизии имён
                    if tool.name in self.tools_map:
                        # Добавляем префикс сервера к имени
                        original_name = tool.name
                        tool.name = f"{server_name}_{tool.name}"
                        print(f"Предупреждение: конфликт имени '{original_name}', переименовано в '{tool.name}'")
                    
                    self.tools_map[tool.name] = server_name
                    all_tools.append(tool)
                
                print(f"Обнаружено {len(result.tools)} инструментов на '{server_name}'")
                
            except Exception as e:
                print(f"Ошибка получения инструментов от '{server_name}': {e}")
        
        return all_tools
    
    async def call_tool(self, tool_name: str, arguments: dict) -> types.CallToolResult:
        """
        Вызывает инструмент на соответствующем сервере.
        
        Args:
            tool_name: Имя инструмента
            arguments: Аргументы вызова
            
        Returns:
            Результат выполнения инструмента
        """
        # Находим сервер для инструмента
        server_name = self.tools_map.get(tool_name)
        if not server_name:
            raise ValueError(f"Инструмент '{tool_name}' не найден")
        
        # Получаем сессию
        session = self.client_manager.get_session(server_name)
        
        # Вызываем инструмент
        return await session.call_tool(tool_name, arguments)
```

## Интеграция с LLM

Хост координирует взаимодействие между LLM и MCP-инструментами:

```python
from typing import List, Dict, Any

class LLMIntegration:
    """
    Интеграция MCP с языковой моделью.
    
    Управляет потоком: пользователь -> LLM -> инструменты -> LLM -> ответ.
    """
    
    def __init__(
        self,
        tool_aggregator: ToolAggregator,
        llm_client  # Клиент для вашей LLM (OpenAI, Anthropic и т.д.)
    ):
        self.tool_aggregator = tool_aggregator
        self.llm_client = llm_client
        self.conversation_history = []
    
    async def process_user_message(self, user_message: str) -> str:
        """
        Обрабатывает сообщение пользователя через LLM с доступом к инструментам.
        
        Args:
            user_message: Сообщение от пользователя
            
        Returns:
            Ответ ассистента
        """
        # Добавляем сообщение в историю
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })
        
        # Получаем список доступных инструментов
        tools = await self.tool_aggregator.discover_all_tools()
        
        # Конвертируем MCP tools в формат LLM (например, OpenAI function calling)
        llm_tools = self._convert_tools_to_llm_format(tools)
        
        # Основной цикл взаимодействия
        max_iterations = 10
        for iteration in range(max_iterations):
            
            # Запрос к LLM
            response = await self.llm_client.chat_completion(
                messages=self.conversation_history,
                tools=llm_tools
            )
            
            # Проверяем, хочет ли LLM вызвать инструмент
            if response.tool_calls:
                # LLM запрашивает вызов инструмента(ов)
                tool_results = await self._execute_tool_calls(response.tool_calls)
                
                # Добавляем результаты в историю
                self.conversation_history.append({
                    "role": "assistant",
                    "tool_calls": response.tool_calls
                })
                self.conversation_history.append({
                    "role": "tool",
                    "content": tool_results
                })
                
                # Продолжаем цикл для получения финального ответа
                continue
            
            else:
                # LLM дал финальный ответ
                assistant_message = response.content
                self.conversation_history.append({
                    "role": "assistant",
                    "content": assistant_message
                })
                return assistant_message
        
        # Если достигли лимита итераций
        return "Извините, превышен лимит вызовов инструментов."
    
    async def _execute_tool_calls(self, tool_calls: List[Dict]) -> List[Dict]:
        """
        Выполняет запрошенные LLM вызовы инструментов.
        
        Args:
            tool_calls: Список вызовов от LLM
            
        Returns:
            Результаты выполнения
        """
        results = []
        
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            arguments = json.loads(tool_call["function"]["arguments"])
            
            try:
                # Вызываем MCP инструмент
                result = await self.tool_aggregator.call_tool(tool_name, arguments)
                
                # Извлекаем текстовое содержимое
                content = "\n".join([
                    item.text for item in result.content 
                    if hasattr(item, 'text')
                ])
                
                results.append({
                    "tool_call_id": tool_call["id"],
                    "content": content
                })
                
            except Exception as e:
                results.append({
                    "tool_call_id": tool_call["id"],
                    "content": f"Ошибка: {str(e)}"
                })
        
        return results
    
    def _convert_tools_to_llm_format(self, mcp_tools: List[types.Tool]) -> List[Dict]:
        """
        Конвертирует MCP инструменты в формат, понятный LLM.
        
        Args:
            mcp_tools: Список MCP инструментов
            
        Returns:
            Инструменты в формате OpenAI function calling
        """
        llm_tools = []
        
        for tool in mcp_tools:
            llm_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            })
        
        return llm_tools
```

## Обработка ресурсов в хосте

```python
class ResourceManager:
    """
    Менеджер ресурсов MCP для предоставления контекста LLM.
    """
    
    def __init__(self, client_manager: MCPClientManager):
        self.client_manager = client_manager
    
    async def get_all_resources(self) -> List[types.Resource]:
        """
        Собирает все доступные ресурсы со всех серверов.
        """
        all_resources = []
        
        for server_name, session in self.client_manager.sessions.items():
            try:
                result = await session.list_resources()
                all_resources.extend(result.resources)
            except Exception as e:
                print(f"Ошибка получения ресурсов от '{server_name}': {e}")
        
        return all_resources
    
    async def inject_resources_to_context(
        self,
        resource_uris: List[str]
    ) -> str:
        """
        Читает ресурсы и формирует контекст для LLM.
        
        Args:
            resource_uris: Список URI ресурсов для включения
            
        Returns:
            Объединённый контекст из ресурсов
        """
        context_parts = []
        
        for uri in resource_uris:
            # Находим соответствующий сервер
            for server_name, session in self.client_manager.sessions.items():
                try:
                    result = await session.read_resource(uri)
                    
                    for content in result.contents:
                        if hasattr(content, 'text'):
                            context_parts.append(
                                f"--- Ресурс: {uri} ---\n{content.text}\n"
                            )
                    break
                    
                except:
                    continue  # Пробуем следующий сервер
        
        return "\n".join(context_parts)
```

## Резюме

LLM-хост координирует работу множественных MCP-клиентов, агрегирует инструменты и ресурсы, управляет жизненным циклом соединений и обеспечивает интеграцию с языковой моделью через циклы вызова инструментов. Хост загружает конфигурацию серверов, обрабатывает коллизии имён и предоставляет унифицированный интерфейс для LLM [web:12][web:18].

**Следующий модуль**: интеграция с моделями с fine-tuning и без.
```

***

### `lectures/module06_model_integration.md`

```markdown
# Модуль 6. Интеграция с моделями

## Модели с fine-tuning для использования tools

Современные языковые модели с fine-tuning для function calling (например, GPT-4, Claude 3, Gemini) нативно поддерживают работу с инструментами через специальный формат сообщений.

### Формат function calling

#### OpenAI API

```python
import openai
from typing import List, Dict

async def call_openai_with_tools(
    messages: List[Dict],
    tools: List[Dict]
) -> Dict:
    """
    Вызов OpenAI API с поддержкой function calling.
    
    Args:
        messages: История диалога
        tools: Список доступных инструментов в формате OpenAI
        
    Returns:
        Ответ модели с возможными tool_calls
    """
    client = openai.AsyncOpenAI()
    
    response = await client.chat.completions.create(
        model="gpt-4-turbo",
        messages=messages,
        tools=tools,
        tool_choice="auto"  # Модель сама решает, нужен ли инструмент
    )
    
    return response.choices.message
```

**Пример инструмента в формате OpenAI:**

```python
openai_tool = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Получить текущую погоду для указанного города",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "Название города"
                },
                "units": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "Единицы измерения температуры"
                }
            },
            "required": ["city"]
        }
    }
}
```

**Ответ модели с tool_call:**

```python
{
    "role": "assistant",
    "content": null,
    "tool_calls": [
        {
            "id": "call_abc123",
            "type": "function",
            "function": {
                "name": "get_weather",
                "arguments": '{"city": "Moscow", "units": "celsius"}'
            }
        }
    ]
}
```

#### Anthropic Claude API

```python
import anthropic
from typing import List, Dict

async def call_claude_with_tools(
    messages: List[Dict],
    tools: List[Dict]
) -> Dict:
    """
    Вызов Anthropic Claude API с поддержкой tool use.
    
    Args:
        messages: История диалога
        tools: Список инструментов в формате Claude
        
    Returns:
        Ответ модели
    """
    client = anthropic.AsyncAnthropic()
    
    response = await client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=4096,
        messages=messages,
        tools=tools
    )
    
    return response
```

**Пример инструмента в формате Claude:**

```python
claude_tool = {
    "name": "get_weather",
    "description": "Получить текущую погоду для указанного города",
    "input_schema": {
        "type": "object",
        "properties": {
            "city": {
                "type": "string",
                "description": "Название города"
            },
            "units": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "Единицы измерения"
            }
        },
        "required": ["city"]
    }
}
```

### Конвертация MCP tools в формат моделей

```python
import mcp.types as types
from typing import List, Dict

class ToolConverter:
    """
    Конвертер MCP инструментов в форматы различных LLM API.
    """
    
    @staticmethod
    def mcp_to_openai(mcp_tool: types.Tool) -> Dict:
        """
        Конвертирует MCP Tool в формат OpenAI function.
        
        Args:
            mcp_tool: Инструмент MCP
            
        Returns:
            Инструмент в формате OpenAI
        """
        return {
            "type": "function",
            "function": {
                "name": mcp_tool.name,
                "description": mcp_tool.description,
                "parameters": mcp_tool.inputSchema
            }
        }
    
    @staticmethod
    def mcp_to_claude(mcp_tool: types.Tool) -> Dict:
        """
        Конвертирует MCP Tool в формат Claude.
        
        Args:
            mcp_tool: Инструмент MCP
            
        Returns:
            Инструмент в формате Claude
        """
        return {
            "name": mcp_tool.name,
            "description": mcp_tool.description,
            "input_schema": mcp_tool.inputSchema
        }
    
    @staticmethod
    def convert_tools(
        mcp_tools: List[types.Tool],
        target_format: str
    ) -> List[Dict]:
        """
        Массовая конвертация инструментов.
        
        Args:
            mcp_tools: Список MCP инструментов
            target_format: Целевой формат ("openai" или "claude")
            
        Returns:
            Список сконвертированных инструментов
        """
        converters = {
            "openai": ToolConverter.mcp_to_openai,
            "claude": ToolConverter.mcp_to_claude
        }
        
        if target_format not in converters:
            raise ValueError(f"Неподдерживаемый формат: {target_format}")
        
        converter = converters[target_format]
        return [converter(tool) for tool in mcp_tools]
```

### Полный цикл взаимодействия с fine-tuned моделью

```python
import json
from typing import List, Dict, Optional

class FineTunedModelAgent:
    """
    Агент для работы с fine-tuned моделью и MCP инструментами.
    """
    
    def __init__(
        self,
        llm_client,  # OpenAI или Anthropic клиент
        tool_aggregator,  # MCP ToolAggregator
        model_name: str,
        api_type: str = "openai"
    ):
        self.llm_client = llm_client
        self.tool_aggregator = tool_aggregator
        self.model_name = model_name
        self.api_type = api_type
        self.messages = []
    
    async def chat(self, user_message: str) -> str:
        """
        Обрабатывает сообщение пользователя через LLM с MCP инструментами.
        
        Args:
            user_message: Сообщение от пользователя
            
        Returns:
            Ответ ассистента
        """
        # Добавляем сообщение пользователя
        self.messages.append({
            "role": "user",
            "content": user_message
        })
        
        # Получаем MCP инструменты
        mcp_tools = await self.tool_aggregator.discover_all_tools()
        
        # Конвертируем в формат LLM
        llm_tools = ToolConverter.convert_tools(mcp_tools, self.api_type)
        
        # Основной цикл с ограничением итераций
        max_iterations = 15
        
        for iteration in range(max_iterations):
            # Вызываем модель
            if self.api_type == "openai":
                response = await self._call_openai(llm_tools)
            elif self.api_type == "claude":
                response = await self._call_claude(llm_tools)
            else:
                raise ValueError(f"Неподдерживаемый API: {self.api_type}")
            
            # Проверяем, есть ли вызовы инструментов
            tool_calls = self._extract_tool_calls(response)
            
            if not tool_calls:
                # Нет вызовов инструментов - это финальный ответ
                final_text = self._extract_text(response)
                self.messages.append({
                    "role": "assistant",
                    "content": final_text
                })
                return final_text
            
            # Есть вызовы инструментов - выполняем их
            await self._process_tool_calls(tool_calls, response)
        
        return "Превышен лимит итераций вызовов инструментов."
    
    async def _call_openai(self, tools: List[Dict]) -> Dict:
        """Вызов OpenAI API."""
        response = await self.llm_client.chat.completions.create(
            model=self.model_name,
            messages=self.messages,
            tools=tools,
            tool_choice="auto"
        )
        return response.choices.message
    
    async def _call_claude(self, tools: List[Dict]) -> Dict:
        """Вызов Claude API."""
        # Claude не принимает системные сообщения в массиве messages
        system_msg = None
        user_messages = []
        
        for msg in self.messages:
            if msg["role"] == "system":
                system_msg = msg["content"]
            else:
                user_messages.append(msg)
        
        response = await self.llm_client.messages.create(
            model=self.model_name,
            max_tokens=4096,
            system=system_msg,
            messages=user_messages,
            tools=tools
        )
        return response
    
    def _extract_tool_calls(self, response) -> Optional[List]:
        """Извлекает вызовы инструментов из ответа."""
        if self.api_type == "openai":
            return response.tool_calls if hasattr(response, 'tool_calls') else None
        elif self.api_type == "claude":
            # Claude использует content blocks типа tool_use
            tool_uses = [
                block for block in response.content 
                if block.type == "tool_use"
            ]
            return tool_uses if tool_uses else None
    
    def _extract_text(self, response) -> str:
        """Извлекает текстовый ответ."""
        if self.api_type == "openai":
            return response.content or ""
        elif self.api_type == "claude":
            text_blocks = [
                block.text for block in response.content 
                if hasattr(block, 'text')
            ]
            return "\n".join(text_blocks)
    
    async def _process_tool_calls(self, tool_calls: List, response):
        """
        Выполняет вызовы инструментов и добавляет результаты в историю.
        """
        if self.api_type == "openai":
            await self._process_openai_tool_calls(tool_calls, response)
        elif self.api_type == "claude":
            await self._process_claude_tool_calls(tool_calls, response)
    
    async def _process_openai_tool_calls(self, tool_calls: List, response):
        """Обработка tool calls для OpenAI."""
        # Добавляем сообщение ассистента с tool_calls
        self.messages.append({
            "role": "assistant",
            "content": response.content,
            "tool_calls": [
                {
                    "id": tc.id,
                    "type": "function",
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments
                    }
                }
                for tc in tool_calls
            ]
        })
        
        # Выполняем каждый вызов
        for tool_call in tool_calls:
            tool_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)
            
            try:
                # Вызываем MCP инструмент
                result = await self.tool_aggregator.call_tool(tool_name, arguments)
                
                # Извлекаем текст результата
                result_text = "\n".join([
                    item.text for item in result.content 
                    if hasattr(item, 'text')
                ])
                
            except Exception as e:
                result_text = f"Ошибка выполнения инструмента: {str(e)}"
            
            # Добавляем результат в историю
            self.messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": result_text
            })
    
    async def _process_claude_tool_calls(self, tool_uses: List, response):
        """Обработка tool uses для Claude."""
        # Добавляем сообщение ассистента
        self.messages.append({
            "role": "assistant",
            "content": response.content
        })
        
        # Формируем результаты инструментов
        tool_results = []
        
        for tool_use in tool_uses:
            tool_name = tool_use.name
            arguments = tool_use.input
            
            try:
                result = await self.tool_aggregator.call_tool(tool_name, arguments)
                
                result_text = "\n".join([
                    item.text for item in result.content 
                    if hasattr(item, 'text')
                ])
                
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_use.id,
                    "content": result_text
                })
                
            except Exception as e:
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_use.id,
                    "content": f"Ошибка: {str(e)}",
                    "is_error": True
                })
        
        # Добавляем результаты в историю
        self.messages.append({
            "role": "user",
            "content": tool_results
        })
```

## Модели без fine-tuning (через системный промпт)

Модели без специального fine-tuning для function calling могут использовать инструменты через **системный промпт** с описанием формата вызова [web:17][web:20].

### Шаблон системного промпта

```markdown
# Системный промпт для работы с инструментами

Ты ассистент с доступом к набору инструментов. Когда тебе нужно использовать инструмент, 
ответь в следующем формате:

TOOL_CALL: <имя_инструмента>
ARGUMENTS: <JSON с аргументами>

После использования инструмента ты получишь результат, и затем должен продолжить ответ пользователю.

## Доступные инструменты

### get_weather
Описание: Получает текущую погоду для указанного города
Параметры:
- city (string, обязательно): Название города
- units (string, опционально): Единицы измерения (celsius/fahrenheit)

Пример использования:
TOOL_CALL: get_weather
ARGUMENTS: {"city": "Moscow", "units": "celsius"}

### search_web
Описание: Выполняет поиск в интернете
Параметры:
- query (string, обязательно): Поисковый запрос

Пример использования:
TOOL_CALL: search_web
ARGUMENTS: {"query": "Python MCP tutorial"}

## Правила
1. Используй инструменты только когда это необходимо для ответа на вопрос пользователя
2. Всегда указывай корректный JSON в ARGUMENTS
3. Можешь вызывать несколько инструментов последовательно
4. После получения результата инструмента, сформулируй понятный ответ пользователю
```

### Генерация системного промпта из MCP tools

```python
from typing import List
import mcp.types as types
import json

class SystemPromptGenerator:
    """
    Генератор системных промптов для моделей без fine-tuning.
    """
    
    @staticmethod
    def generate_tool_description(tool: types.Tool) -> str:
        """
        Генерирует описание инструмента для промпта.
        
        Args:
            tool: MCP инструмент
            
        Returns:
            Текстовое описание инструмента
        """
        lines = [f"### {tool.name}"]
        lines.append(f"Описание: {tool.description}")
        
        # Парсим параметры из JSON Schema
        schema = tool.inputSchema
        if "properties" in schema:
            lines.append("Параметры:")
            
            required = schema.get("required", [])
            
            for param_name, param_def in schema["properties"].items():
                param_type = param_def.get("type", "any")
                is_required = param_name in required
                required_str = "обязательно" if is_required else "опционально"
                
                param_desc = param_def.get("description", "")
                
                lines.append(
                    f"- {param_name} ({param_type}, {required_str}): {param_desc}"
                )
        
        # Пример использования
        example_args = SystemPromptGenerator._generate_example_args(tool)
        lines.append(f"\nПример использования:")
        lines.append(f"TOOL_CALL: {tool.name}")
        lines.append(f"ARGUMENTS: {json.dumps(example_args, ensure_ascii=False)}")
        
        return "\n".join(lines)
    
    @staticmethod
    def _generate_example_args(tool: types.Tool) -> dict:
        """
        Генерирует пример аргументов для инструмента.
        """
        schema = tool.inputSchema
        example = {}
        
        if "properties" not in schema:
            return {}
        
        for param_name, param_def in schema["properties"].items():
            param_type = param_def.get("type", "string")
            
            # Генерируем примерное значение на основе типа
            if param_type == "string":
                example[param_name] = "example_value"
            elif param_type == "number" or param_type == "integer":
                example[param_name] = 42
            elif param_type == "boolean":
                example[param_name] = True
            elif param_type == "array":
                example[param_name] = []
            elif param_type == "object":
                example[param_name] = {}
        
        return example
    
    @staticmethod
    def generate_system_prompt(
        tools: List[types.Tool],
        additional_instructions: str = ""
    ) -> str:
        """
        Генерирует полный системный промпт со всеми инструментами.
        
        Args:
            tools: Список MCP инструментов
            additional_instructions: Дополнительные инструкции
            
        Returns:
            Системный промпт
        """
        lines = [
            "# Системный промпт для работы с инструментами",
            "",
            "Ты ассистент с доступом к набору инструментов. "
            "Когда тебе нужно использовать инструмент, ответь в следующем формате:",
            "",
            "TOOL_CALL: <имя_инструмента>",
            "ARGUMENTS: <JSON с аргументами>",
            "",
            "После использования инструмента ты получишь результат в формате:",
            "TOOL_RESULT: <результат>",
            "",
            "## Доступные инструменты",
            ""
        ]
        
        # Добавляем описания всех инструментов
        for tool in tools:
            lines.append(SystemPromptGenerator.generate_tool_description(tool))
            lines.append("")
        
        # Правила использования
        lines.extend([
            "## Правила",
            "1. Используй инструменты только когда это необходимо для ответа",
            "2. Всегда указывай корректный JSON в ARGUMENTS",
            "3. Можешь вызывать несколько инструментов последовательно",
            "4. После получения TOOL_RESULT, сформулируй понятный ответ пользователю",
            "5. Не выдумывай результаты - используй только данные из TOOL_RESULT"
        ])
        
        if additional_instructions:
            lines.append("")
            lines.append("## Дополнительные инструкции")
            lines.append(additional_instructions)
        
        return "\n".join(lines)
```

### Парсинг ответов модели без fine-tuning

```python
import re
from typing import Optional, Tuple

class ToolCallParser:
    """
    Парсер вызовов инструментов из текстовых ответов модели.
    """
    
    @staticmethod
    def parse_tool_call(text: str) -> Optional[Tuple[str, dict]]:
        """
        Парсит вызов инструмента из текста модели.
        
        Args:
            text: Ответ модели
            
        Returns:
            Кортеж (tool_name, arguments) или None если вызова нет
        """
        # Ищем паттерн TOOL_CALL и ARGUMENTS
        tool_call_match = re.search(r'TOOL_CALL:\s*(\w+)', text)
        arguments_match = re.search(r'ARGUMENTS:\s*({.*?})', text, re.DOTALL)
        
        if not tool_call_match or not arguments_match:
            return None
        
        tool_name = tool_call_match.group(1)
        arguments_str = arguments_match.group(1)
        
        try:
            arguments = json.loads(arguments_str)
            return (tool_name, arguments)
        except json.JSONDecodeError:
            print(f"Ошибка парсинга аргументов: {arguments_str}")
            return None
```

### Агент для моделей без fine-tuning

```python
class NonFineTunedAgent:
    """
    Агент для работы с моделями без fine-tuning через системные промпты.
    """
    
    def __init__(
        self,
        llm_client,
        tool_aggregator,
        model_name: str
    ):
        self.llm_client = llm_client
        self.tool_aggregator = tool_aggregator
        self.model_name = model_name
        self.system_prompt = ""
        self.messages = []
    
    async def initialize(self):
        """Инициализирует агента и генерирует системный промпт."""
        # Получаем инструменты
        tools = await self.tool_aggregator.discover_all_tools()
        
        # Генерируем системный промпт
        self.system_prompt = SystemPromptGenerator.generate_system_prompt(tools)
    
    async def chat(self, user_message: str) -> str:
        """
        Обрабатывает сообщение пользователя.
        
        Args:
            user_message: Сообщение от пользователя
            
        Returns:
            Ответ ассистента
        """
        self.messages.append({
            "role": "user",
            "content": user_message
        })
        
        max_iterations = 10
        
        for iteration in range(max_iterations):
            # Формируем полный контекст
            full_messages = [
                {"role": "system", "content": self.system_prompt}
            ] + self.messages
            
            # Вызываем модель
            response = await self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=full_messages,
                temperature=0.7
            )
            
            assistant_text = response.choices.message.content
            
            # Парсим возможный вызов инструмента
            tool_call = ToolCallParser.parse_tool_call(assistant_text)
            
            if tool_call is None:
                # Нет вызова инструмента - это финальный ответ
                self.messages.append({
                    "role": "assistant",
                    "content": assistant_text
                })
                return assistant_text
            
            # Есть вызов инструмента
            tool_name, arguments = tool_call
            
            # Добавляем сообщение ассистента
            self.messages.append({
                "role": "assistant",
                "content": assistant_text
            })
            
            # Выполняем инструмент
            try:
                result = await self.tool_aggregator.call_tool(tool_name, arguments)
                
                result_text = "\n".join([
                    item.text for item in result.content 
                    if hasattr(item, 'text')
                ])
                
                # Добавляем результат
                self.messages.append({
                    "role": "user",
                    "content": f"TOOL_RESULT: {result_text}"
                })
                
            except Exception as e:
                self.messages.append({
                    "role": "user",
                    "content": f"TOOL_RESULT: Ошибка - {str(e)}"
                })
        
        return "Превышен лимит итераций."
```

## Резюме

Fine-tuned модели используют нативный function calling через специальные форматы API (OpenAI tool_calls, Claude tool_use), а модели без fine-tuning работают с инструментами через системные промпты с описанием формата вызова и парсинг текстовых ответов. MCP tools конвертируются в соответствующие форматы для каждого типа модели [web:15][web:20].

**Следующий модуль**: практическое применение - построение полного MCP-решения.
```

***

Продолжу создание остальных материалов курса. Нужно ли мне продолжать, или хочешь что-то скорректировать в уже созданных модулях?

[1](https://modelcontextprotocol.io/specification/2024-11-05)
