Отлично! Завершаю создание курса.[1]

***

#### `labs/lab05_progress/README.md`

```markdown
# Лабораторная работа 5: Progress Notifications

## Цели обучения

После выполнения этой лабораторной работы вы сможете:

- Реализовывать progress notifications в MCP серверах
- Передавать progressToken через request context
- Отправлять уведомления о прогрессе клиенту
- Отслеживать прогресс длительных операций
- Правильно обрабатывать случаи с и без progressToken

## Задание

Создайте MCP-сервер "data-processor-server" с инструментами, которые сообщают о прогрессе выполнения.

### Требования

#### Инструмент 1: `process_dataset`

Обрабатывает набор данных с отчётом о прогрессе.

**Параметры:**
- `items` (array, обязательно) - массив элементов для обработки
- `delay_ms` (number, опционально, по умолчанию 100) - задержка на элемент (мс)

**Прогресс:**
- Отправляет уведомление после каждого обработанного элемента
- Показывает текущий элемент / общее количество
- Включает процент выполнения

#### Инструмент 2: `download_simulation`

Симулирует загрузку файла с прогрессом.

**Параметры:**
- `url` (string, обязательно) - URL для "загрузки"
- `size_mb` (number, обязательно) - размер файла в МБ
- `speed_mbps` (number, опционально, по умолчанию 10) - скорость загрузки

**Прогресс:**
- Обновления каждую секунду
- Показывает загруженные МБ / общий размер
- Оценка оставшегося времени

#### Инструмент 3: `multi_step_task`

Задача с несколькими этапами.

**Параметры:**
- `steps` (array of strings, обязательно) - список шагов

**Прогресс:**
- Отдельное уведомление для каждого шага
- Название текущего шага
- Общий прогресс по всем шагам

## Пошаговая инструкция

### Шаг 1: Понимание request context

В MCP, progressToken передаётся через request context:

```python
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "tools/call",
  "params": {
    "name": "process_dataset",
    "arguments": {...},
    "_meta": {
      "progressToken": "unique-token-123"
    }
  }
}
```

### Шаг 2: Извлечение progressToken

```python
@app.call_tool()
async def call_tool(
    name: str,
    arguments: dict,
    # ВАЖНО: request_context передаётся автоматически MCP SDK
) -> list[types.TextContent]:
    
    # Получаем progressToken если клиент его предоставил
    progress_token = None
    
    # В реальной реализации progressToken доступен через
    # специальный механизм MCP SDK
    # Для простоты в этой лабе мы создадим вспомогательную функцию
```

### Шаг 3: Отправка progress notifications

```python
async def send_progress(
    progress_token: str | None,
    progress: int,
    total: int,
    message: str = ""
):
    """
    Отправляет progress notification клиенту.
    
    Args:
        progress_token: Токен прогресса от клиента
        progress: Текущий прогресс
        total: Общее количество единиц работы
        message: Опциональное сообщение
    """
    if progress_token is None:
        # Клиент не запросил progress notifications
        return
    
    # Отправляем через JSON-RPC notification
    # В продакшен коде используется session.send_progress_notification()
    import sys
    print(
        f"[PROGRESS {progress_token}] {progress}/{total} - {message}",
        file=sys.stderr
    )
```

### Шаг 4: Реализация process_dataset

```python
if name == "process_dataset":
    items = arguments["items"]
    delay_ms = arguments.get("delay_ms", 100)
    
    total = len(items)
    processed = []
    
    # Начальный прогресс
    await send_progress(progress_token, 0, total, "Начало обработки")
    
    for i, item in enumerate(items):
        # Симулируем обработку
        await asyncio.sleep(delay_ms / 1000)
        
        # Обрабатываем элемент
        processed.append(f"Processed: {item}")
        
        # Отправляем прогресс
        current = i + 1
        percent = int((current / total) * 100)
        await send_progress(
            progress_token,
            current,
            total,
            f"Элемент {current}/{total} ({percent}%)"
        )
    
    # Финальный прогресс
    await send_progress(progress_token, total, total, "Завершено!")
    
    return [types.TextContent(
        type="text",
        text=f"Обработано {total} элементов:\n" + "\n".join(processed)
    )]
```

### Шаг 5: Тестирование с клиентом

```python
# test_client.py
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def test_with_progress():
    server_params = StdioServerParameters(
        command="python",
        args=["server.py"]
    )
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            
            # Создаём уникальный progress token
            progress_token = "test-progress-123"
            
            # Вызываем с progressToken в meta
            result = await session.call_tool(
                name="process_dataset",
                arguments={
                    "items": ["item1", "item2", "item3"],
                    "delay_ms": 500
                }
                # В реальном SDK progressToken передаётся автоматически
            )
            
            print(result.content.text)

asyncio.run(test_with_progress())
```

## Критерии проверки

- [ ] Все 3 инструмента реализованы
- [ ] Progress notifications отправляются корректно
- [ ] Прогресс начинается с 0 и заканчивается на total
- [ ] Сообщения прогресса информативны
- [ ] Инструменты корректно работают БЕЗ progressToken
- [ ] Нет race conditions при асинхронной обработке
- [ ] Прогресс отражает реальное состояние операции

## Дополнительное задание

1. Добавьте возможность отмены (cancellation) длительных операций
2. Реализуйте вложенные прогрессы (subtasks)
3. Добавьте ETA (estimated time of arrival)
4. Создайте визуализацию прогресса в клиенте

## Подсказки

1. Всегда проверяйте `if progress_token is not None` перед отправкой
2. Отправляйте прогресс через регулярные интервалы, не слишком часто
3. Используйте `asyncio.sleep()` для симуляции длительных операций
4. Для расчёта процентов: `int((current / total) * 100)`
5. В stderr можно писать отладочные сообщения безопасно

## Формат progress notification (справка)

```json
{
  "jsonrpc": "2.0",
  "method": "notifications/progress",
  "params": {
    "progressToken": "unique-token",
    "progress": 50,
    "total": 100,
    "message": "Processing item 50/100"
  }
}
```

## Время выполнения

Ожидаемое время: 60-75 минут

## Следующие шаги

После выполнения переходите к Lab 06: Полная интеграция с LLM - финальному проекту курса!
```

***

#### `labs/lab05_progress/solution/server.py`

```python
#!/usr/bin/env python3
"""
Решение Lab 05: Progress Notifications

MCP сервер с инструментами, отправляющими уведомления о прогрессе.
"""

import asyncio
import time
from typing import Optional
from mcp.server.lowlevel import Server
from mcp.server.stdio import stdio_server
import mcp.types as types


# Создаём сервер
app = Server("data-processor-server")


async def send_progress(
    progress_token: Optional[str],
    progress: int,
    total: int,
    message: str = ""
):
    """
    Отправляет progress notification клиенту.
    
    В реальной реализации это должно отправлять JSON-RPC notification
    через активную сессию. Для демонстрации выводим в stderr.
    
    Args:
        progress_token: Токен прогресса от клиента (если предоставлен)
        progress: Текущий прогресс (выполнено единиц работы)
        total: Общее количество единиц работы
        message: Опциональное текстовое сообщение о состоянии
    """
    if progress_token is None:
        # Клиент не запросил progress notifications - пропускаем
        return
    
    # Логируем прогресс в stderr (не нарушает JSON-RPC в stdout)
    import sys
    percent = int((progress / total) * 100) if total > 0 else 0
    print(
        f"[PROGRESS {progress_token}] {progress}/{total} ({percent}%) - {message}",
        file=sys.stderr
    )
    
    # В реальной реализации здесь был бы вызов:
    # await session.send_progress_notification(
    #     progress_token=progress_token,
    #     progress=progress,
    #     total=total,
    #     message=message
    # )


@app.list_tools()
async def list_tools() -> list[types.Tool]:
    """Регистрирует инструменты с поддержкой прогресса."""
    return [
        types.Tool(
            name="process_dataset",
            description="Обрабатывает массив элементов с отчётом о прогрессе",
            inputSchema={
                "type": "object",
                "properties": {
                    "items": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Массив элементов для обработки"
                    },
                    "delay_ms": {
                        "type": "number",
                        "description": "Задержка на обработку каждого элемента (мс)",
                        "default": 100
                    }
                },
                "required": ["items"]
            }
        ),
        types.Tool(
            name="download_simulation",
            description="Симулирует загрузку файла с прогресс-баром",
            inputSchema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "URL для симуляции загрузки"
                    },
                    "size_mb": {
                        "type": "number",
                        "description": "Размер файла в МБ"
                    },
                    "speed_mbps": {
                        "type": "number",
                        "description": "Скорость загрузки в МБ/с",
                        "default": 10
                    }
                },
                "required": ["url", "size_mb"]
            }
        ),
        types.Tool(
            name="multi_step_task",
            description="Выполняет задачу из нескольких этапов с прогрессом",
            inputSchema={
                "type": "object",
                "properties": {
                    "steps": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Список названий шагов для выполнения"
                    }
                },
                "required": ["steps"]
            }
        )
    ]


@app.call_tool()
async def call_tool(
    name: str,
    arguments: dict
) -> list[types.TextContent]:
    """
    Обрабатывает вызовы инструментов с поддержкой прогресса.
    
    В реальной реализации progressToken доступен через request context.
    Для упрощения лабораторной работы мы симулируем его наличие.
    """
    
    # В продакшен коде progressToken извлекается из request context:
    # progress_token = request_context.meta.get('progressToken')
    # Для демонстрации используем фиксированный токен
    progress_token = "demo-progress-token"
    
    if name == "process_dataset":
        items = arguments["items"]
        delay_ms = arguments.get("delay_ms", 100)
        
        total = len(items)
        processed_results = []
        
        # Отправляем начальный прогресс (0%)
        await send_progress(
            progress_token,
            0,
            total,
            "Инициализация обработки..."
        )
        
        # Обрабатываем каждый элемент
        for i, item in enumerate(items):
            # Симулируем обработку с задержкой
            await asyncio.sleep(delay_ms / 1000)
            
            # Выполняем обработку
            processed_item = f"✅ Обработан: {item}"
            processed_results.append(processed_item)
            
            # Вычисляем прогресс
            current = i + 1
            percent = int((current / total) * 100)
            
            # Отправляем уведомление о прогрессе
            await send_progress(
                progress_token,
                current,
                total,
                f"Обработка элемента {current}/{total} ({percent}%): {item}"
            )
        
        # Финальный прогресс (100%)
        await send_progress(
            progress_token,
            total,
            total,
            "Обработка завершена!"
        )
        
        # Формируем результат
        result_text = (
            f"Обработка завершена!\n"
            f"Всего элементов: {total}\n\n"
            + "\n".join(processed_results)
        )
        
        return [types.TextContent(type="text", text=result_text)]
    
    elif name == "download_simulation":
        url = arguments["url"]
        size_mb = arguments["size_mb"]
        speed_mbps = arguments.get("speed_mbps", 10)
        
        # Вычисляем время загрузки и количество шагов
        total_seconds = size_mb / speed_mbps
        steps = int(total_seconds) + 1  # Количество обновлений прогресса
        mb_per_step = size_mb / steps
        
        downloaded_mb = 0
        start_time = time.time()
        
        # Начальный прогресс
        await send_progress(
            progress_token,
            0,
            int(size_mb),
            f"Начало загрузки {url} ({size_mb} МБ)"
        )
        
        # Симулируем загрузку
        for step in range(steps):
            # Ждём секунду
            await asyncio.sleep(1)
            
            # Обновляем количество загруженных данных
            downloaded_mb = min(downloaded_mb + mb_per_step, size_mb)
            
            # Вычисляем оставшееся время
            elapsed = time.time() - start_time
            if elapsed > 0:
                current_speed = downloaded_mb / elapsed
                remaining_mb = size_mb - downloaded_mb
                eta_seconds = remaining_mb / current_speed if current_speed > 0 else 0
            else:
                eta_seconds = 0
            
            # Отправляем прогресс
            await send_progress(
                progress_token,
                int(downloaded_mb),
                int(size_mb),
                f"Загружено {downloaded_mb:.1f}/{size_mb} МБ "
                f"(скорость: {speed_mbps} МБ/с, осталось: ~{int(eta_seconds)}с)"
            )
        
        # Финальный прогресс
        await send_progress(
            progress_token,
            int(size_mb),
            int(size_mb),
            "Загрузка завершена!"
        )
        
        total_time = time.time() - start_time
        
        result_text = (
            f"Загрузка завершена!\n"
            f"URL: {url}\n"
            f"Размер: {size_mb} МБ\n"
            f"Скорость: {speed_mbps} МБ/с\n"
            f"Время: {total_time:.1f} секунд"
        )
        
        return [types.TextContent(type="text", text=result_text)]
    
    elif name == "multi_step_task":
        steps = arguments["steps"]
        total_steps = len(steps)
        
        completed_steps = []
        
        # Начальный прогресс
        await send_progress(
            progress_token,
            0,
            total_steps,
            "Подготовка к выполнению задачи..."
        )
        
        # Выполняем каждый шаг
        for i, step_name in enumerate(steps):
            # Симулируем выполнение шага (случайное время от 0.5 до 2 секунд)
            import random
            step_duration = random.uniform(0.5, 2.0)
            
            # Отправляем прогресс о начале шага
            await send_progress(
                progress_token,
                i,
                total_steps,
                f"Выполняется шаг {i+1}/{total_steps}: {step_name}"
            )
            
            # Выполняем шаг
            await asyncio.sleep(step_duration)
            
            completed_steps.append(
                f"  {i+1}. {step_name} - выполнен за {step_duration:.1f}с"
            )
            
            # Отправляем прогресс о завершении шага
            current = i + 1
            await send_progress(
                progress_token,
                current,
                total_steps,
                f"Завершён шаг {current}/{total_steps}: {step_name}"
            )
        
        # Финальный прогресс
        await send_progress(
            progress_token,
            total_steps,
            total_steps,
            "Все шаги выполнены!"
        )
        
        result_text = (
            f"Задача выполнена!\n"
            f"Всего шагов: {total_steps}\n\n"
            "Детали выполнения:\n"
            + "\n".join(completed_steps)
        )
        
        return [types.TextContent(type="text", text=result_text)]
    
    else:
        raise ValueError(f"Неизвестный инструмент: {name}")


async def main():
    """Точка входа сервера."""
    import sys
    print("=" * 60, file=sys.stderr)
    print("Data Processor Server с Progress Notifications", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    print("Сервер поддерживает отправку уведомлений о прогрессе", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    
    async with stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )


if __name__ == "__main__":
    asyncio.run(main())
```

***

#### `labs/lab06_full_integration/README.md`

```markdown
# Лабораторная работа 6: Полная интеграция с LLM

## Цели обучения

Это финальный проект курса, объединяющий все изученные концепции.

После выполнения вы сможете:

- Создать полноценное MCP-решение с несколькими серверами
- Реализовать хост-приложение для управления клиентами
- Интегрировать MCP инструменты с LLM
- Работать с fine-tuned и non-fine-tuned моделями
- Развернуть production-ready систему

## Задание

Создайте полноценную систему "AI Assistant with MCP" со следующими компонентами:

### Компоненты системы

```
┌────────────────────────────────────┐
│      AI Assistant Host             │
│  ┌──────────────────────────────┐  │
│  │   LLM Integration Layer      │  │
│  │  (OpenAI/Claude/Local)       │  │
│  └──────────────────────────────┘  │
│  ┌──────────────────────────────┐  │
│  │   MCP Client Manager         │  │
│  └──────────────────────────────┘  │
└────────────────────────────────────┘
          │              │
          ▼              ▼
    ┌──────────┐    ┌──────────┐
    │ Files    │    │  Web     │
    │ Server   │    │  Server  │
    │ (stdio)  │    │ (HTTP)   │
    └──────────┘    └──────────┘
```

### Требования к серверам

#### 1. File Operations Server (stdio)

Инструменты:
- `read_file` - чтение файлов
- `write_file` - запись файлов
- `list_directory` - список файлов
- `search_in_files` - поиск по содержимому

Ресурсы:
- `file://path/to/file` - содержимое файлов

#### 2. Web Operations Server (HTTP/SSE)

Инструменты:
- `fetch_url` - загрузка веб-страниц
- `search_web` - симуляция веб-поиска
- `extract_data` - извлечение данных из HTML

### Требования к хосту

1. **Конфигурация** (`config.json`):
   ```json
   {
     "llm": {
       "provider": "openai",
       "model": "gpt-4-turbo",
       "api_key": "sk-..."
     },
     "mcpServers": {
       "files": { "command": "python", "args": ["servers/file_server.py"] },
       "web": { "url": "http://localhost:8000/sse" }
     }
   }
   ```

2. **Функциональность**:
   - Загрузка конфигурации
   - Подключение ко всем серверам
   - Агрегация инструментов
   - Интеграция с LLM (function calling)
   - Интерактивный чат-интерфейс

3. **Режимы работы**:
   - Fine-tuned model (через native function calling)
   - Non-fine-tuned model (через системный промпт)

## Пошаговая инструкция

### Шаг 1: Структура проекта

```
labs/lab06_full_integration/
├── config.json
├── servers/
│   ├── file_server.py
│   └── web_server.py
├── client/
│   ├── host.py
│   ├── llm_integration.py
│   └── system_prompts.py
├── tests/
│   └── test_integration.py
└── README.md
```

### Шаг 2: Реализация File Server

Используйте наработки из предыдущих лабораторных работ:

```python
# servers/file_server.py
# Переиспользуйте код из Lab 01 + модуля 7
```

### Шаг 3: Реализация Web Server

```python
# servers/web_server.py
# Объедините Lab 03 (async tools) и Lab 04 (HTTP transport)
```

### Шаг 4: Создание хоста

```python
# client/host.py
import json
import asyncio
from typing import Dict, List
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

class MCPHost:
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.sessions: Dict[str, ClientSession] = {}
        self.tools_map: Dict[str, str] = {}
    
    def _load_config(self, path: str) -> dict:
        with open(path) as f:
            return json.load(f)
    
    async def start(self):
        """Подключается ко всем серверам из конфигурации."""
        for name, config in self.config["mcpServers"].items():
            await self._connect_server(name, config)
        await self._discover_tools()
    
    async def _connect_server(self, name: str, config: dict):
        """Подключается к одному серверу."""
        # Реализация из модуля 5
        pass
    
    async def _discover_tools(self):
        """Собирает инструменты со всех серверов."""
        for server_name, session in self.sessions.items():
            result = await session.list_tools()
            for tool in result.tools:
                self.tools_map[tool.name] = server_name
    
    async def call_tool(self, tool_name: str, arguments: dict) -> str:
        """Вызывает инструмент на соответствующем сервере."""
        server_name = self.tools_map[tool_name]
        session = self.sessions[server_name]
        result = await session.call_tool(tool_name, arguments)
        return self._extract_text(result)
```

### Шаг 5: LLM интеграция (fine-tuned)

```python
# client/llm_integration.py
import openai

class FineTunedLLMAgent:
    def __init__(self, host: MCPHost, api_key: str, model: str):
        self.host = host
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.model = model
        self.messages = []
    
    async def chat(self, user_message: str) -> str:
        self.messages.append({"role": "user", "content": user_message})
        
        # Получаем MCP инструменты
        tools = await self._get_llm_tools()
        
        # Основной цикл с function calling
        for _ in range(10):  # Максимум 10 итераций
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
                tools=tools
            )
            
            message = response.choices.message
            
            if not message.tool_calls:
                # Финальный ответ
                self.messages.append(message)
                return message.content
            
            # Есть tool calls - выполняем их
            await self._execute_tool_calls(message.tool_calls)
        
        return "Превышен лимит вызовов инструментов"
    
    async def _execute_tool_calls(self, tool_calls):
        # Выполнение через host.call_tool()
        pass
```

### Шаг 6: Интерактивный интерфейс

```python
# В host.py
async def interactive_chat(self):
    """Запускает интерактивный чат."""
    print("=== AI Assistant with MCP ===")
    print("Введите 'quit' для выхода\n")
    
    while True:
        user_input = input("You: ").strip()
        
        if user_input.lower() in ['quit', 'exit']:
            break
        
        # Получаем ответ от LLM agent
        response = await self.llm_agent.chat(user_input)
        print(f"\nAssistant: {response}\n")
```

### Шаг 7: Тестирование

Создайте тесты для проверки:

```python
# tests/test_integration.py
import pytest

@pytest.mark.asyncio
async def test_file_operations():
    """Тест файловых операций через LLM."""
    # Запросить у LLM создать файл
    # Проверить, что файл создан
    pass

@pytest.mark.asyncio
async def test_web_fetch():
    """Тест веб-операций."""
    pass

@pytest.mark.asyncio
async def test_combined_workflow():
    """Тест комплексного сценария."""
    # Например: загрузить данные с веб, сохранить в файл
    pass
```

### Шаг 8: Примеры диалогов

**Пример 1: Файловые операции**

```
You: Создай файл test.txt с текстом "Hello MCP"

Да, ты абсолютно прав! Ответ оборвался на середине. Продолжу и завершу создание курса.

***

#### Продолжение `labs/lab06_full_integration/README.md`

```markdown
Assistant: [вызывает write_file]
Файл test.txt успешно создан с содержимым "Hello MCP"

You: Прочитай этот файл
Assistant: [вызывает read_file]
Содержимое файла test.txt: "Hello MCP"
```

**Пример 2: Комбинированный сценарий**

```
You: Загрузи данные с https://api.github.com/users/octocat и сохрани в файл github_user.json

[1](https://modelcontextprotocol.io/specification/2024-11-05)
