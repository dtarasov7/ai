Ниже — 14 сценариев, отсортированных от самых простых (чистый LLM без RAG/MCP) до наиболее сложных (LLM + RAG + MCP/интеграции с ИТ‑системами).

---

### Карточка 1. Помощник по ответам на заявки и деловой переписке

1. **Название сценария**  
   Помощник по ответам на заявки и рабочей переписке (L1/L2, инженеры, ИБ).

2. **Текущая задача, проблема или узкое место**  
   - Операторы и инженеры пишут десятки однотипных ответов в день.  
   - Тексты разного качества, нет единого стиля и формулировок.  
   - Много времени уходит на «красивое» изложение, а не на решение сути.

3. **Как LLM помогает (механизм, примеры запросов, интеграции)**  
   - Локальный чат‑интерфейс (web, чат‑бот в корпоративном мессенджере).  
   - Пользователь вставляет текст заявки / черновик ответа и просит:
     - «Сформулируй вежливый ответ пользователю, что его заявка в работе, ожидаем доступ от стороннего подрядчика».  
     - «Сократи и структурируй этот текст для уведомления в рассылку ИТ».  
   - Возможность хранить «тональность» и стиль (формальный/дружелюбный, на “вы” и т.п.).  
   - Все запросы/ответы логируются; доступ к истории — по роли (рук. ИТ, ИБ).

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG / MCP:** не требуется.  
   - Можно задать несколько внутренних шаблонов/примеров текстов (как промпт или системное сообщение), но без отдельной БЗ.  
   - Данные: обычный текст в запросах/ответах.

5. **Оценка эффекта**  
   - Сокращение времени на подготовку ответов на 30–40%.  
   - Повышение качества и единообразия коммуникаций (меньше конфликтов, недопониманий).  
   - Улучшение SLA по времени первого ответа за счёт автоматизации шаблонов.

6. **Пример реального применения или типовой use case**  
   - Оператор L1 получает заявку «VPN не работает!!! Срочно!».  
   - Просит LLM: «Сформулируй ответ пользователю, что мы проверяем VPN и уточни, с какого устройства и когда начались проблемы».  
   - Копирует ответ в ITSM/почту, при необходимости подправляет.

---

### Карточка 2. Генерация скриптов, Ansible‑тасков и шагов в CI

1. **Название сценария**  
   Генерация инфраструктурных скриптов и конфигураций (Bash, PowerShell, Python, Ansible, CI).

2. **Текущая задача, проблема или узкое место**  
   - Инженеры тратят много времени на написание однотипных скриптов и конфигов.  
   - Ошибки синтаксиса, мелкие баги, неучтённые corner‑cases.  
   - Люди часто копируют куски из старых скриптов, таща за собой legacy.

3. **Как LLM помогает**  
   - Чат‑ассистент:  
     - «Напиши bash‑скрипт: раз в день чистить /var/log/app от файлов старше 7 дней, логировать действия».  
     - «Сгенерируй Ansible‑таски для установки nginx на RHEL 8 с проверкой, что SELinux включён».  
     - «Опиши job в GitLab CI, который собирает Java‑приложение и пушит образ в локальный registry».  
   - LLM сразу даёт комментарии в коде и краткую инструкцию по использованию.  
   - Инженер проверяет, адаптирует и коммитит.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG / MCP:** не требуется.  
   - Вход/выход — текст (код, yaml, конфиги).  
   - Можно по желанию дополнительно снабдить LLM примерами «наших стандартных» скриптов как контекст.

5. **Оценка эффекта**  
   - Сокращение времени на написание рутинных скриптов на 30–50%.  
   - Меньше синтаксических ошибок, быстрее code review.  
   - Быстрое онбординг новых сотрудников в «наш стиль» конфигураций.

6. **Пример реального применения**  
   - Системный админ просит:  
     «Сделай пример Ansible role для установки и настройки локального репозитория yum с логированием изменений».  
   - Получает структуру роли, tasks, handlers, пример inventory — дальше дорабатывает по своим стандартам.

---

### Карточка 3. Автоматическое формирование commit message, changelog и release‑нотов

1. **Название сценария**  
   Ассистент по commit‑сообщениям и описанию изменений.

2. **Текущая задача, проблема или узкое место**  
   - Коммиты без внятных описаний, нечитаемые changelog’и.  
   - Release‑менеджеры просматривают десятки diff’ов, чтобы понять «что реально изменилось».

3. **Как LLM помогает**  
   - Интеграция с Git (локальный GitLab, Gitea, Gerrit):  
     - pre‑commit hook или кнопка в веб‑интерфейсе: отправить diff/список файлов в LLM.  
     - LLM возвращает согласованный commit message (по корпоративному шаблону) и короткое описание.  
   - Для релизов: по списку merged MR/коммитов LLM формирует:
     - краткий changelog для внутреннего пользования;  
     - «человеческое» описание для пользователей/бизнеса.  
   - Все вызовы — только к локальной модели; diffs не покидают периметр.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** не обязателен (достаточно контекста диффа).  
   - **MCP:** лёгкая интеграция с Git API для автоматической передачи diff/списка файлов.  
   - Формат: unified diff, JSON списков изменений.

5. **Оценка эффекта**  
   - Экономия времени разработчиков на написание commit‑сообщений до 50–80%.  
   - Качественная трассируемость изменений (облегчает RCA, аудит ИБ).  
   - Ускорение подготовки release‑нотов в разы.

6. **Пример реального применения**  
   - Разработчик запускает `git commit` → клиент предлагает «Сгенерировать сообщение с помощью LLM?» → diff уходит на локальный сервис LLM.  
   - Полученный текст соответствует шаблону: `feat(core): add retry logic for DB connections (INC-1234)` и кратко объясняет причину.

---

### Карточка 4. Быстрый статический анализ кода и конфигов с точки зрения ИБ

1. **Название сценария**  
   LLM‑ревьюер кода и конфигураций на базовые ошибки и нарушения ИБ.

2. **Текущая задача, проблема или узкое место**  
   - Безопасники физически не успевают просматривать все изменения в codebase и конфигурациях.  
   - Повторяющиеся ошибки: жёстко прописанные пароли, широкие ACL, слабые шифры, открытые порты.  

3. **Как LLM помогает**  
   - Инженер или безопасник отправляет фрагмент кода/конфига с запросами типа:  
     - «Найди в этом фрагменте потенциальные уязвимости и жестко прописанные секреты».  
     - «Проверь, не нарушают ли эти iptables‑правила принцип минимальных прав».  
     - «Есть ли тут очевидные проблемы безопасности в Python‑скрипте, который работает с файлами?»  
   - Модель возвращает список найденных проблем + пояснения, почему это риск.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG / MCP:** не требуется.  
   - Вход: текст кода/конфига.  
   - Можно добавить фиксированный системный промпт с вашими политиками (например, «SSH только по ключам, root login запрещён»).

5. **Оценка эффекта**  
   - Отлавливание 20–40% очевидных проблем ИБ до стадии code review и формального аудита.  
   - Снижение нагрузки на команду ИБ за счёт «самоочистки» кода у разработчиков.  
   - Ускорение согласования изменений, меньше возвратов.

6. **Пример реального применения**  
   - DevOps перед merge просит:  
     «Проверь этот nginx.conf на типичные проблемы безопасности и несоответствия нашим стандартам (TLS, заголовки безопасности, лимиты)».  
   - LLM указывает, где не выставлены заголовки `X-Frame-Options`, `Content-Security-Policy`, слабые шифросuites и т.д.

---

### Карточка 5. Разбор логов, stacktrace и ошибок «по запросу инженера»

1. **Название сценария**  
   LLM‑помощник по расшифровке логов и ошибок.

2. **Текущая задача, проблема или узкое место**  
   - При сложных инцидентах инженер получает многострочные stacktrace и логи из разных систем.  
   - Время тратится на поиск корня ошибки и её «человеческое» объяснение.

3. **Как LLM помогает**  
   - Инженер копирует фрагмент лога / stacktrace и задаёт вопросы:
     - «Объясни на русском, что значит эта ошибка и где потенциальная причина».  
     - «Сгруппируй эти логи по типам проблем и выведи топ‑5 аномальных сообщений».  
     - «Предложи план диагностики по этим логам: какие команды/проверки выполнить дальше».  
   - LLM умеет:  
     - подсветить ключевые строки;  
     - предложить дальнейшие шаги;  
     - при необходимости — нагенерить команды диагностики (`journalctl`, `kubectl`, `netstat`, etc.).

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG / MCP:** не требуется на базовом уровне.  
   - Вход: текст логов, возможно в формате `timestamp level message`.  
   - Возможен дальнейший шаг — интеграция с log‑storage, но это уже следующий по сложности сценарий (см. ниже).

5. **Оценка эффекта**  
   - Снижение времени на понимание причин ошибки в 1,5–2 раза, особенно для дежурных и «junior» специалистов.  
   - Повышение качества описания инцидентов и пост‑мортемов.  
   - Опосредованное снижение MTTR.

6. **Пример реального применения**  
   - Дежурный SRE копирует 200 строк Java stacktrace и пишет:  
     «Найди причинную ошибку (root cause) и предложи, что мне проверить в первую очередь на сервере и в БД».  
   - LLM выделяет `Caused by ...` и формирует короткий список действий.

---

## Сценарии с RAG (LLM + внутренняя база знаний)

---

### Карточка 6. Ассистент по внутренним регламентам, политикам ИБ и процедурам

1. **Название сценария**  
   Поисковый ассистент по регламентам ИТ и ИБ (RAG).

2. **Текущая задача, проблема или узкое место**  
   - Сотни страниц регламентов в wiki/доках, сотрудники просто «не знают, где искать».  
   - Частые нарушения процедур из‑за незнания/забытого порядка действий.  
   - SOC и ИТ‑служба формально отвечают за соблюдение, но инструментов помощи мало.

3. **Как LLM помогает**  
   - Строится RAG‑индекс по:  
     - внутренним политиками ИБ, ИТ‑регламентам, инструкциям, ГОСТ/приказам в адаптированной форме;  
     - процедурам доступа, резервирования, эксплуатации, реагирования.  
   - Пользователь задаёт естественные вопросы:
     - «Как у нас оформляется доступ к прод‑БД для разработчика?»  
     - «Опиши шаги процедуры реагирования на утечку пароля доменной учётной записи».  
   - LLM даёт ответ + ссылки/цитаты из соответствующих документов.  
   - Доступ к фрагментам — с учётом ролей (например, конфиденциальные части только для ИБ/руководства).

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** да, индекс по Confluence/Wiki, PDF, DOCX, Markdown.  
   - **MCP:** опционально, для онлайн‑доступа к системам документооборота.  
   - Форматы: извлечение текста, метаданные (гриф, владелец, даты).

5. **Оценка эффекта**  
   - Снижение времени на поиск нужного регламента минимум на 30%.  
   - Уменьшение количества нарушений процедур, связанных с «не знал/не нашёл».  
   - Ускорение онбординга новых сотрудников.

6. **Пример реального применения**  
   - Новому администратору нужно оформить временный доступ подрядчику.  
   - Он спрашивает ассистента: «Опиши в 5 шагах, как выдать временный доступ подрядчику к серверу в DMZ с учётом политик ИБ».  
   - LLM возвращает краткий алгоритм + ссылки на детальный регламент и шаблон заявки.

---

### Карточка 7. Проектирование и сравнение вариантов инфраструктурных решений

1. **Название сценария**  
   LLM‑архитектор с учётом внутренних стандартов (RAG + открытая документация).

2. **Текущая задача, проблема или узкое место**  
   - Архитекторы тратят много времени на чтение vendor‑доков, сравнение вариантов и проверку соответствия корпоративным стандартам.  
   - Решения проектируются «с нуля», даже если похожий паттерн уже есть.

3. **Как LLM помогает**  
   - В RAG‑индекс загружаются:  
     - внутренние архитектурные стандарты, ADR, шаблонные решения;  
     - открытая документация по технологиям (Kubernetes, PostgreSQL, Nginx, Kafka и т.п.) — с учётом лицензий;  
     - ограничения: импортозамещение, локальные вендоры и т.п.  
   - Архитектор задаёт:
     - «Предложи 2–3 варианта отказоустойчивой схемы PostgreSQL на 2 ЦОД с RPO=0, RTO<15 мин, только on‑prem и российский стек по максимуму».  
     - «Сравни Kafka и RabbitMQ для логирования событий ИБ, учитывая наши стандарты и требования по latency».  
   - LLM формирует варианты, таблицы сравнения, указывает соответствие внутренним стандартам.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** обязателен (арх.стандарты + внешние доки).  
   - **MCP:** не обязателен; на продвинутом этапе можно тянуть данные из CMDB/инвентаря (для учёта текущей инфраструктуры).  
   - Форматы: Markdown, PDF, диаграммы в текстовом виде (PlantUML) и т.п.

5. **Оценка эффекта**  
   - Сокращение времени на подготовку «чернового» архитектурного решения на 30–40%.  
   - Повышение качества и унификации решений (меньше «самодеятельности»).  
   - Быстрее защита решений на архитектурных комитетах.

6. **Пример реального применения**  
   - Архитектор пишет:  
     «Сравни 2 варианта: балансировка L7 через Nginx на bare‑metal vs ingress‑контроллер в Kubernetes, с учётом наших стандартов безопасности и сетевой архитектуры. Сформируй таблицу + рекомендации».  
   - Получает структурированный документ, который потом дорабатывает и выносит на согласование.

---

### Карточка 8. База знаний по инцидентам и проблемам (Problem Management Copilot)

1. **Название сценария**  
   Ассистент по прошлым инцидентам, RCA и runbook’ам (RAG).

2. **Текущая задача, проблема или узкое место**  
   - Опыт по решению инцидентов и проблем «размазан» по Jira, wiki, письмам.  
   - Новые дежурные не знают, что аналогичный инцидент уже был и как его тогда решали.

3. **Как LLM помогает**  
   - В RAG‑индекс заносятся:  
     - записи об инцидентах (ITSM), проблемах, post‑mortem отчётах;  
     - runbook’и, check‑list’ы, стандартные операционные процедуры (SOP).  
   - При новом инциденте инженер задаёт:
     - «Найди похожие инциденты по признакам: timeout подключения к БД, сервис X, дата‑центр Y»;  
     - «Какие шаги помогли решить аналогичные инциденты в прошлом?»  
   - LLM находит похожие кейсы, выжимает конкретные действия (команды, настройки) и предупреждает, что решение требует валидации.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** да, на ITSM, wiki, папки с отчетами.  
   - **MCP:** опционально — для онлайн-доступа к ITSM (Jira/Redmine и т.п.) по API.  
   - Форматы: JSON выгрузок ITSM, Markdown/HTML отчетов, ссылки на тикеты.

5. **Оценка эффекта**  
   - Снижение времени на решение повторяющихся инцидентов на 20–50%.  
   - Сокращение количества «переизобретений велосипеда».  
   - Повышение качества RCA (есть контекст прошлых проблем).

6. **Пример реального применения**  
   - Ночной инцидент: очередное «диск переполнен на PROD‑БД».  
   - Дежурный спрашивает: «Какие у нас были похожие инциденты по этой БД и какие меры тогда предприняли (в том числе долгосрочные)?»  
   - LLM находит 3 случая, выводит шаги: временная чистка, изменение параметров мониторинга, изменение политики логирования.

---

### Карточка 9. Ассистент SOC по правилам корреляции, источникам логов и плейбукам

1. **Название сценария**  
   SOC‑ассистент по правилам SIEM и процедурам реагирования (RAG).

2. **Текущая задача, проблема или узкое место**  
   - Правил в SIEM много, документация к ним либо устарела, либо сложна для L1.  
   - Онбординг аналитиков SOC длительный; много времени уходит на объяснение «что значит этот алерт».

3. **Как LLM помогает**  
   - RAG‑индекс по:  
     - правилам корреляции SIEM (описания, условия, источники);  
     - плейбукам SOC, внутренним инструкциям, кейсам расследований;  
     - открытым материалам (например, MITRE ATT&CK, если разрешено).  
   - Аналитик задаёт:
     - «Поясни, что означает срабатывание правила RULE_12345, какие источники задействованы и какой возможный TTP по MITRE».  
     - «Дай краткий плейбук действий по этому типу алерта для уровня L1».  
   - LLM даёт пояснения + ссылку на официальный плейбук.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** да (SIEM rules, SOC‑wiki, прошлые инциденты ИБ).  
   - **MCP:** не обязателен на этом этапе (чистое справочно‑объяснительное использование).  
   - Форматы: YAML/JSON правил, wiki‑страницы, отчёты по инцидентам.

5. **Оценка эффекта**  
   - Сокращение времени L1 на понимание контекста алерта на 30–40%.  
   - Ускорение онбординга новых SOC‑аналитиков.  
   - Снижение вероятности некорректной эскалации (из‑за непонимания важности сигнала).

6. **Пример реального применения**  
   - L1 видит алерт по необычной активности PowerShell.  
   - Спрашивает ассистента: «Объясни правило, какие логи участвуют, какая типичная ложноположительная сработка, какие первые шаги проверки».  
   - Получает структурированный ответ и чек‑лист действий.

---

## Сценарии с MCP/интеграциями (LLM вызывает API и инструменты)

---

### Карточка 10. Автоклассификация и обогащение заявок в ITSM

1. **Название сценария**  
   LLM‑триажер заявок (MCP + опционально RAG по каталогу услуг).

2. **Текущая задача, проблема или узкое место**  
   - Пользователи пишут всё «в свободной форме».  
   - Операторы тратят время на выбор категории, приоритета, CI, исполнителя.  
   - Ошибочная маршрутизация → потеря времени и SLA.

3. **Как LLM помогает**  
   - Через MCP настроены инструменты:
     - `get_ticket_details(ticket_id)` — получить текст заявки из ITSM;  
     - `list_services()` — из каталога услуг / CMDB;  
     - `update_ticket_classification()` — записать предложенные категорию/приоритет/группу.  
   - Поток:
     1. Новая заявка → триггерится вызов LLM‑сервиса.  
     2. LLM анализирует текст и через RAG по каталогу услуг/CI предлагает:
        - категорию, услугу, CI, приоритет, группу исполнителей;  
        - короткое «очищенное» summary заявки.  
     3. Оператор L1 видит рекомендации и подтверждает/корректирует.  
   - Все действия логируются (кто утвердил, какие поля изменены).

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** желательно по каталогу услуг, описанию CI, типовым шаблонам заявок.  
   - **MCP:** обязателен (ITSM API, CMDB API).  
   - Форматы: JSON тикетов, сущностей CMDB.

5. **Оценка эффекта**  
   - Сокращение времени L1 на первичную обработку заявки на 30–60%.  
   - Повышение доли «правильной маршрутизации» с первого раза.  
   - Улучшение SLA по времени реакции и решению.

6. **Пример реального применения**  
   - Приходит заявка: «Не подключается VPN с домашнего ПК, вчера всё работало, сейчас ошибка 809».  
   - LLM предлагает:  
     - Категория: «Удалённый доступ / VPN»;  
     - Сервис: «Корпоративный VPN»;  
     - CI: «VPN‑шлюз №2»;  
     - Приоритет: P3;  
     - Группа: «Сетевая команда».  
   - Оператор подтверждает — заявка сразу у нужной команды.

---

### Карточка 11. Copilot для мониторинга: анализ алертов и метрик

1. **Название сценария**  
   LLM‑помощник по алертам мониторинга (MCP + возможно RAG по известным паттернам).

2. **Текущая задача, проблема или узкое место**  
   - Дежурный при срабатывании алерта ходит по нескольким консолям (Prometheus/Zabbix, Grafana, логи) и вручную собирает картину.  
   - Высокое когнитивное давление в ночные смены, риск пропуска важных деталей.

3. **Как LLM помогает**  
   - MCP‑инструменты:  
     - `get_alert(alert_id)` из системы алертинга;  
     - `get_metrics(target, window)` из Prometheus/Zabbix;  
     - `get_related_alerts(target, window)` для корреляции;  
     - опционально: `get_logs(query, window)` из log‑storage.  
   - Поток:
     1. Алерт создаёт тикет/инцидент.  
     2. LLM по `alert_id` собирает ключевые метрики за N минут до/после события, смотрит связанные алерты.  
     3. Формирует краткий отчёт:
        - какие метрики вышли за пределы;  
        - есть ли похожие недавние случаи (через RAG по инцидентам);  
        - предположительная причина и список шагов диагностики.  
   - Результат прикладывается к тикету, виден дежурному.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** по базе прошлых инцидентов/паттернов (опционально).  
   - **MCP:** да — мониторинг, алертинг, логирование.  
   - Форматы: JSON алертов, временные ряды (time‑series), текст логов (усечённый).

5. **Оценка эффекта**  
   - Сокращение времени на первичный анализ алерта в 1,5–2 раза.  
   - Меньше «ручной рутины» для SRE/дежурных, выше фокус на принятии решений.  
   - Лучше качество описания инцидентов и аргументации при RCA.

6. **Пример реального применения**  
   - Алерт: «Высокий p95 latency у сервиса X > 2 сек».  
   - LLM собирает:
     - графики RPS, error rate, CPU/IO, списки связанных алертов (DB latency, GC pauses);  
     - выдаёт текст: «Вероятно, проблема в росте времени ответа БД; аналогичные случаи были 2024‑05‑01, 2024‑07‑10; предлагаемые шаги...».

---

### Карточка 12. Автоматизированный помощник SOC (оркестрация шагов расследования)

1. **Название сценария**  
   SOC Copilot: полуавтоматическое расследование алертов (MCP + RAG).

2. **Текущая задача, проблема или узкое место**  
   - Аналитики тратят много времени на однотипные действия:  
     поиск событий в SIEM, проверка хоста в EDR, просмотр сетевой активности, поиск по TI‑источникам.  
   - Часть шагов пропускается в спешке, качество документов страдает.

3. **Как LLM помогает**  
   - MCP‑инструменты:
     - `get_siem_alert(alert_id)` — детали алерта;  
     - `search_logs(query, window)` — выборка событий;  
     - `query_edr(host_or_user)` — статус, последние алерты;  
     - `lookup_threat_intel(ioc)` — рез-ты по хешу/IP/домену из локальных TI;  
     - `create_or_update_incident()` — оформление карточки инцидента.  
   - RAG‑индекс — по плейбукам SOC, прошлым инцидентам, MITRE‑методикам.  
   - Поток:
     1. Создаётся алерт в SIEM.  
     2. LLM‑агент выполняет типовой плейбук: собирает логи, TI, EDR‑статус.  
     3. Формирует:
        - таймлайн событий;  
        - гипотезу по TTP (например, попытка lateral movement);  
        - предложенные containment‑меры;  
        - черновик отчёта.  
     4. Аналитик просматривает, вносит правки и принимает/отклоняет рекомендации.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** обязателен по плейбукам/кейсам.  
   - **MCP:** обязателен — SIEM, EDR, TI, возможно SOAR.  
   - Форматы: JSON событий, IOC, отчёты.  
   - Все действия агента жёстко логируются (кто вызвал, какие запросы, какие артефакты использованы).

5. **Оценка эффекта**  
   - Сокращение времени на первичное расследование и обогащение алерта на 30–60%.  
   - Повышение единообразия расследований (следование плейбукам).  
   - Возможность обрабатывать больше событий без линейного роста штата SOC.

6. **Пример реального применения**  
   - Алерт SIEM: «Подозрительный вход в AD из необычной геолокации».  
   - SOC Copilot автоматически:
     - запрашивает логи логина, VPN, proxy, EDR‑события с рабочего места;  
     - проверяет IP по TI;  
     - строит таймлайн и даёт рекомендации: сброс пароля, блокировка сессии, запрос подтверждения у пользователя.  
   - Аналитик просматривает и запускает действия через существующий SOAR (или вручную).

---

### Карточка 13. Генерация и проверка Ansible‑ролей и CI/CD pipeline с автотестами

1. **Название сценария**  
   Copilot для инфраструктурного as‑code: роли, пайплайны, тесты (MCP + RAG).

2. **Текущая задача, проблема или узкое место**  
   - При добавлении новых сервисов/компонентов каждый раз заново пишутся роли, пайплайны, тесты.  
   - Инфраструктурный код (Ansible, Terraform, CI) не всегда соответствует корпоративным стандартам и требованиям ИБ.

3. **Как LLM помогает**  
   - RAG‑индекс по «золотым шаблонам»:  
     - эталонные Ansible‑роли;  
     - стандартные GitLab CI/TeamCity пайплайны;  
     - чек‑листы ИБ по hardening.  
   - MCP‑инструменты:
     - `get_repo_structure(project)` / `create_merge_request()` в GitLab/Gitea;  
     - `trigger_ci_pipeline()` и `get_ci_results()`.  
   - Поток:
     1. Инженер описывает требования: «Нужна роль для установки и харденига PostgreSQL X версии под стандарт ИБ Y, с пайплайном тестов на стейдже».  
     2. LLM генерирует:
        - структуру роли, tasks, handlers;  
        - пайплайн CI;  
        - Molecule‑тесты (или аналог).  
     3. Через MCP создаётся ветка, MR с сгенерированным кодом.  
     4. Запускается CI, LLM читает результаты и комментирует, что исправить.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** да — для стандартов и шаблонов.  
   - **MCP:** да — API Git/CI.  
   - Форматы: YAML, Jinja2, файлы ролей, JSON результатов CI.

5. **Оценка эффекта**  
   - Снижение трудозатрат на создание базовых ролей/пайплайнов на 30–50%.  
   - Больше соответствия корпоративным стандартам и ИБ‑требованиям «по умолчанию».  
   - Меньше ручных ошибок в инфраструктурном коде.

6. **Пример реального применения**  
   - Команда внедряет новый сервис логирования.  
   - Лид DevOps формулирует требования, запускает сценарий генерации роли и пайплайна.  
   - Полученный MR проходит обычный review, но стартовая точка уже выровнена по стандартам.

---

### Карточка 14. Автоматическое построение архитектурных и сетевых диаграмм

1. **Название сценария**  
   LLM‑генератор и обновлятор архитектурных/сетевых диаграмм (MCP + RAG).

2. **Текущая задача, проблема или узкое место**  
   - Диаграммы (сетевые, архитектурные) быстро устаревают, их редко обновляют.  
   - При аудите или расследовании инцидента невозможно быстро понять реальную топологию.

3. **Как LLM помогает**  
   - MCP‑инструменты:
     - `get_cmdb_service_topology(service_id)` — из CMDB/Discovery;  
     - `get_k8s_resources(namespace)` — из Kubernetes;  
     - `get_firewall_rules(segment)` — из шлюзов/NGFW.  
   - RAG‑база по:
     - стандартам оформления диаграмм;  
     - типовым шаблонам (уровни L1/L2/L3, DMZ, сегментация и т.д.).  
   - Поток:
     1. Архитектор запрашивает: «Построй актуальную L3‑диаграмму для сервиса X с указанием основных зависимостей и точек входа/выхода».  
     2. LLM собирает данные из CMDB/K8s/файрволов.  
     3. Генерирует PlantUML/Graphviz/Draw.io XML + краткое текстовое описание.  
     4. Через MCP сохраняет диаграмму в репозиторий или wiki.

4. **Требуется ли подключение источников знаний, API, форматы данных**  
   - **RAG:** да — для корпоративных шаблонов и стандартов условных обозначений.  
   - **MCP:** да — CMDB, Kubernetes, виртуализация, сетевое оборудование.  
   - Форматы: JSON/ YAML инвентаря, текст диаграмм (PlantUML, Graphviz), файлы Draw.io.

5. **Оценка эффекта**  
   - Ускорение подготовки и актуализации диаграмм в 3–5 раз.  
   - Улучшение качества аудитов ИБ, снижается риск «скрытых» связей между сегментами.  
   - Облегчение анализа инцидентов (видно, какие системы связаны с пострадавшим узлом).

6. **Пример реального применения**  
   - Перед большим изменением (миграция сервиса в новый ЦОД) архитектор запускает генерацию диаграммы «как есть».  
   - На основании актуальной схемы планируются DRT‑меры, изменения в ACL, маршрутизации, проверяются точки ИБ‑контроля.

---

Если нужно, могу следующей итерацией:

- сгруппировать сценарии в «дорожную карту внедрения» по годам/кварталам;  
- или для 2–3 приоритетных сценариев расписать конкретную архитектуру решения (какую open‑source LLM‑платформу, какой векторный стор, как организовать RBAC и журналирование).
