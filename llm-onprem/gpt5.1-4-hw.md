Ниже — конкретный, «железо‑приземлённый» вариант стека под **1 пользователя**, **RTX 3090 24 ГБ**, с **моделью от Сбера** и минимальным количеством лишних сервисов.

---

## 1. Основная модель от Сбера

### 1.1. Что брать в качестве LLM

**Рекомендация:**  
**ruGPT‑3.5‑13B** от Sber / ai‑forever:

- Hugging Face: `ai-forever/ruGPT-3.5-13B`   
- 13B параметров, контекст 2048 токенов, MIT‑лицензия.  
- Это базовая языковая модель, на ней обучали первую версию GigaChat.   

Для локального запуска на 3090 удобнее использовать уже готовую квантизованную версию в GGUF:

- **`oblivious/ruGPT-3.5-13B-GGUF`** — набор квантовок (Q2…Q8) в формате GGUF, официально заточен под `llama.cpp`, есть пример команды запуска.   

**Рекомендуемая конфигурация для 3090:**

- Файл: `ruGPT-3.5-13B-Q4_K_M.gguf`
  - Размер ≈ 8.4 ГБ (весов). 
  - На 24 ГБ VRAM этого более чем достаточно, ещё остаётся большой запас под KV‑кэш.
- Ожидаемая скорость генерации:  
  ориентировочно **10–20 токенов/сек** для 13B Q4 на 3090 (по аналогии с LLaMA‑13B Q4 на 3090, где замеряли ~21 ток/сек).   
  Для одного пользователя — комфортно.

Если нужно «максимум качества», можно попробовать `Q5_K_M` (≈9.7 ГБ), но стартовать я бы советовал с **Q4_K_M**.

---

### 1.2. Нужен ли chat‑тюнинг / Instruct‑поведение

Базовый `ruGPT‑3.5‑13B` — **просто языковая модель**, без инструктивного тюнинга. Чтобы она вела себя более как ассистент, используют дообучение (LoRA):

- **LoRA‑адаптер под ассистента (Saiga‑2‑стиль):**  
  `evilfreelancer/ruGPT-3.5-13B-lora` и специализированный адаптер `ruGPT3.5-13B-lora-saiga2`.   
- Есть детальный гайд по дообучению ruGPT‑3.5 13B с LoRA и последующей конвертацией в GGML/GGUF.   

**На старте**, чтобы не усложнять жизнь:

- Можно **не заморачиваться с LoRA** и использовать базовый `ruGPT‑3.5‑13B` с хорошим системным промптом в стиле:  
  «Ты – строгий русскоязычный технический ассистент…»  
- А когда станет не хватать качества/послушности — вернуться к варианту с LoRA (обучить или взять готовый saiga‑адаптер, слить веса и переквантизовать в GGUF).

---

### 1.3. Более лёгкая альтернатива (если захочется быстроты)

Если окажется, что 13B — «тяжело» (например, на ноутбуке / CPU), можно использовать более лёгкую модель от Сбера:

- **mGPT‑1.3B** или **mGPT‑13B** (`ai-forever/mGPT`) — многоязычная GPT‑модель (61 язык, включая русский), 1.3B и 13B параметров.   

Для RTX 3090 под 1 пользователя 13B в Q4 вполне комфортны, так что я бы сразу шёл в **ruGPT‑3.5‑13B Q4_K_M** как основной вариант.

---

## 2. Инференс: vLLM vs llama.cpp для одного пользователя

### 2.1. vLLM

**Плюсы vLLM:**

- Оптимизирован под **многопользовательский** режим, высокую пропускную способность, батчинг.   
- Поддерживает `GPT2LMHeadModel`, т.е. с ruGPT (GPT‑2‑архитектура) он совместим.   

**Минусы в вашем сценарии:**

- Нормально «живёт» с **FP16/BF16** весами — а 13B FP16 (≈26 ГБ + оверхед) **не влезет** в 24 ГБ VRAM без трюков/offload.
- Поддержка разных квантовок для GPT‑2‑архитектуры сложнее и менее «из коробки», чем для LLaMA‑подобных моделей.
- Для 1 пользователя его основное преимущество (масштабирование) просто не нужно.

### 2.2. llama.cpp (+ GGUF)

**Плюсы `llama.cpp` в вашей ситуации:**

- Умеет работать с **GGUF**‑квантовками многих архитектур, в т.ч. GPT‑2 (ruGPT‑3.5‑13B‑GGUF как раз под него).   
- Очень эффективно использует GPU (CUDA build) и VRAM, 13B Q4 на 3090 летает.
- Легко завернуть в HTTP‑сервер или подключить к WebUI (Open WebUI, text-generation-webui, etc.).
- Стек простой: один бинарник + модельный файл.

**Минусы:**

- Часть продвинутых фич (LoRA «на лету», сложный multi‑GPU) придётся реализовывать руками.
- Конфиги немного более low‑level.

### 2.3. Вывод по инференсу

**Для одного пользователя на RTX 3090 и ruGPT‑3.5:**

> **Выбирайте `llama.cpp` + GGUF (oblivious/ruGPT-3.5-13B-GGUF).**  
> vLLM имеет смысл только если вы потом будете поднимать многопользовательский сервис с десятками одновременных запросов.

---

## 3. Конкретный минимальный стек под одного пользователя

### 3.1. Компоненты

1. **OS:** Linux (Ubuntu 22.04 / Debian / Rocky — что привычнее).
2. **GPU:** RTX 3090 24 ГБ (CUDA 12.x, последние драйверы).
3. **Инференс‑движок:** `llama.cpp` (собран с поддержкой CUDA).
4. **Модель:**  
   - `oblivious/ruGPT-3.5-13B-GGUF`, файл `ruGPT-3.5-13B-Q4_K_M.gguf`.   
5. **Web‑интерфейс (опция):**
   - Вариант А: `llama.cpp` встроенный `server` (простенький REST).
   - Вариант Б (я бы так сделал): **Open WebUI** + `llama-cpp-python` как backend.
6. **RAG (при необходимости):**  
   - PostgreSQL + расширение `pgvector` **или** Qdrant (см. ниже).  
   - Эмбеддинги: `ai-forever/sbert_large_nlu_ru` (Sber, BERT‑модель для русских sentence‑эмбеддингов).   

---

### 3.2. Шаги по развёртыванию (в общих чертах)

#### 3.2.1. Установка llama.cpp с CUDA

```bash
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make -j$(nproc) LLAMA_CUBLAS=1
```

Проверяете, что `./main` собирается без ошибок и видит GPU (`nvidia-smi` + запуск тестовой маленькой модели).

#### 3.2.2. Загрузка модели ruGPT‑3.5‑13B в GGUF

```bash
mkdir -p ./models/rugpt35
cd ./models/rugpt35

# через huggingface-cli или просто wget/curl
huggingface-cli download oblivious/ruGPT-3.5-13B-GGUF \
  --include "ruGPT-3.5-13B-Q4_K_M.gguf" \
  --local-dir .
```

(или скачиваете файл руками из веб‑интерфейса HF)

#### 3.2.3. Тестовый запуск из CLI

Пример из карточки модели на HF: 

```bash
cd ~/llama.cpp

./main \
  -m ./models/rugpt35/ruGPT-3.5-13B-Q4_K_M.gguf \
  -c 2048 \
  -n 256 \
  -p 'Стих про программиста может быть таким:'
```

Если видите в консоли связный русскоязычный текст — всё работает.

#### 3.2.4. HTTP‑сервер (для подключения WebUI или своего бэкенда)

```bash
./server \
  -m ./models/rugpt35/ruGPT-3.5-13B-Q4_K_M.gguf \
  -c 2048 \
  --host 0.0.0.0 \
  --port 8000
```

С этого момента у вас есть простой REST API.

#### 3.2.5. Web‑интерфейс (например, Open WebUI)

Дальше вариантов много, один из самых простых — **Open WebUI** в Docker и подключение к `llama.cpp` server как «external OpenAI‑совместимая модель» (через небольшой прокси или `llama-cpp-python`).  

Если нужно, могу в следующем ответе расписать конкретный `docker-compose.yml` под 3090 + Open WebUI + llama.cpp‑backend.

---

## 4. RAG: Qdrant vs pgvector «для одного пользователя»

### 4.1. Эмбеддинги от Сбера

Для RAG вам нужна модель для векторизации текстов (параграфы документации, регламенты и т.п.). У Сбера есть хорошо документированная модель:

- **`ai-forever/sbert_large_nlu_ru`** — BERT‑large sentence embeddings для русского языка.   

Работает через `transformers` или `sentence-transformers`:

```python
from transformers import AutoTokenizer, AutoModel
import torch

tok = AutoTokenizer.from_pretrained("ai-forever/sbert_large_nlu_ru")
model = AutoModel.from_pretrained("ai-forever/sbert_large_nlu_ru").cuda()

def embed(texts):
    enc = tok(texts, padding=True, truncation=True, max_length=512, return_tensors="pt").to("cuda")
    with torch.no_grad():
        out = model(**enc)[0]           # [batch, seq, hidden]
    mask = enc["attention_mask"].unsqueeze(-1)
    pooled = (out * mask).sum(1) / mask.sum(1)  # mean pooling
    return pooled.cpu().numpy()
```

---

### 4.2. Qdrant vs pgvector — что выбрать на старте

**Оба варианта нормальные, разница — в операционке и будущем масштабе.**

#### Вариант 1. `pgvector` (PostgreSQL + расширение)

**Плюсы:**

- Если у вас уже есть PostgreSQL — это просто ещё одна таблица, **без отдельного сервиса**.
- Удобно хранить **текст + метаданные + вектор** в одном месте.
- Для PoC/1 пользователя по объёму (десятки–сотни тысяч документов) — более чем достаточно.

**Минусы:**

- Qdrant при больших коллекциях и сложных фильтрах даёт лучшую производительность «из коробки».
- Векторные фичи менее богаты (хотя сейчас в `pgvector` уже есть и HNSW, и некоторые оптимизации).

**Когда выбирать pgvector:**  

> если Postgres у вас и так «в каждом углу», и не хочется плодить новые сервисы — **берите pgvector** для начала.

#### Вариант 2. Qdrant

**Плюсы:**

- Специализированное векторное хранилище (HNSW, фильтрация, payload, шардирование и т.п.) из коробки.
- Очень простой REST/gRPC API, хорошая документация, есть клиенты на Python/Go/Rust.
- Легко масштабировать, если потом RAG станет «боевым» сервисом.

**Минусы:**

- Отдельный сервис (контейнер), мониторинг, бэкапы.
- Для сценария «один пользователь, десятки тысяч документов» он честно говоря избыточен.

**Когда выбирать Qdrant:**  

> если вы изначально планируете RAG как отдельный сервис, который потом пойдёт в продакшн и будет жить отдельно от приложений — **берите Qdrant**.

---

### 4.3. Мой практический совет для вашего кейса

Сейчас у вас:

- **1 пользователь**, RTX 3090,
- цель — локальный ассистент + RAG поверх своих документов.

Я бы делал так:

1. **Этап 1: PoC и внутренняя обкатка**
   - `llama.cpp` + `ruGPT-3.5-13B-Q4_K_M.gguf`.
   - RAG на **PostgreSQL + pgvector** (если уже есть PG; если нет — можно поднять контейнером).
   - Эмбеддинги — `ai-forever/sbert_large_nlu_ru`.

2. **Этап 2: когда и если RAG/LLM начнёт использоваться несколькими людьми**
   - Вынести векторное хранилище в **Qdrant** и сделать из него отдельный сервис.
   - Задуматься о переходе на vLLM или иной inference‑сервер, если начнутся проблемы с concurrency.

---

## 5. Краткое резюме выбора стека

**Модели от Сбера:**

- LLM: **ruGPT‑3.5‑13B**  
  → брать квантизованную GGUF‑версию `oblivious/ruGPT-3.5-13B-GGUF` (Q4_K_M) для `llama.cpp`.   
- Эмбеддинги для RAG: **`ai-forever/sbert_large_nlu_ru`** (SberDevices).   

**Инференс‑движок:**

- Под ваш кейс (**1 пользователь, RTX 3090**) — **`llama.cpp`** с CUDA, а не vLLM.

**Векторное хранилище:**

- Если важна простота и Postgres уже есть → **PostgreSQL + pgvector**.
- Если хотите более «векторо‑ориентированный» сервис с перспективой на продакшн → **Qdrant**.

Если хочешь, дальше могу:

- набросать конкретный `docker-compose.yml` (llama.cpp server + Open WebUI + Postgres+pgvector),  
- или расписать, как поверх всего этого быстро собрать простой RAG‑API (FastAPI + LangChain/LlamaIndex) под ваши регламенты/документацию.
