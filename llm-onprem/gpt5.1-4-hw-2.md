GigaChat-20B-A3B-instruct-v1.5 как раз очень хорошо ложится на твой сценарий «1 пользователь + RTX 3090 + российская модель». Её **имеет смысл брать вместо ruGPT‑3.5‑13B**, но с оговоркой: только в квантизованном виде.

Разберёмся по пунктам.

---

## 1. Что это за модель и чем она лучше ruGPT‑3.5‑13B

- **Размер:** ~20–21B параметров.
- **Тип:** диалоговая instruct‑модель семейства GigaChat, дообучена от `GigaChat-20B-A3B-base`.  
- **Лицензия:** MIT (открытая).   
- **Контекст:** до **131k токенов** (реально на 3090 придётся ограничиться гораздо меньшим контекстом).   
- Есть **готовые bf16, int8 и GGUF (q4, q5, q6, q8)** варианты, а также сборки под Ollama.   

По бенчмаркам (MERA, ru‑MMLU, Shlepa) она заметно сильнее типичных 7–13B‑моделей на русском и позиционируется как «почти‑продовая» студийная GigaChat‑модель.   

**Критично:** в отличие от ruGPT‑3.5‑13B, это уже **инструктивный чат‑ассистент**, без необходимости ставить отдельный LoRA‑адаптер, с хорошим «alignment» (следование инструкциям).

Для твоих задач (проектирование решений, SOC‑плейбуки, анализ кода/логов, генерация документации) это **однозначно более удачный выбор**, чем базовый ruGPT‑3.5‑13B:

- лучше держит диалог,
- лучше следует инструкциям,
- лучше рассуждает (MERA, MMLU, код‑бенчи).

---

## 2. Можно ли её запустить на RTX 3090 (24 ГБ)

### 2.1. Полные веса (bf16/fp32)

- bf16‑вариант (`ai-sage/GigaChat-20B-A3B-instruct-v1.5-bf16`) весит десятки ГБ и **целиком в 24 ГБ VRAM не влезет**, особенно с учётом KV‑кэша и оверхеда.   
- vLLM в режиме bf16 (`vllm serve ... --dtype bfloat16`) на 3090 без жёсткого offload’а на CPU будет слишком тяжёлым.   

Итог: **чистый bf16 на одной 3090 — нет**, если не городить сложный offload и жить с очень медленным откликом.

### 2.2. Int8 через transformers/bitsandbytes

Есть готовая int8‑квантовка: `ai-sage/GigaChat-20B-A3B-instruct-v1.5-int8`.   

Плюсы:

- Держится через `transformers + bitsandbytes` практически «из коробки».
- Удобно интегрировать в Python‑код, если хочешь писать свои пайплайны/сервисы.

Минусы на 3090:

- 21B параметров × 1 байт ≈ **21 ГБ под веса** → с KV‑кэшем и оверхедом **впритык** для 24 ГБ VRAM.
- Почти наверняка придётся частично offload’ить на оперативку (`device_map="auto", max_memory=...`), что заметно ударит по скорости.
- Для комфортной работы придётся ограничивать контекст разумными 4–8k токенами, не использовать всю «теоретическую» 131k.

Использовать можно, но **для одного пользователя проще и, как правило, быстрее будет GGUF + llama.cpp**, чем int8+bitsandbytes.

### 2.3. GGUF + llama.cpp на RTX 3090

Есть официальный GGUF‑репозиторий: `ai-sage/GigaChat-20B-A3B-instruct-v1.5-GGUF`, а также альтернативная квантизация `mradermacher/...`.   

Размеры (GGUF):   

- **Q4_K_M ≈ 12.5 ГБ** (рекомендуемый баланс «качество/скорость»).
- Q4_K_S ≈ 11.7 ГБ (чуть быстрее, немного хуже).
- Q5_K_M ≈ 14.6 ГБ (лучше качество, медленнее).
- Q6_K ≈ 16.9 ГБ (почти максимум качества для 24ГБ, но медленнее всего).
- Есть Q3/Q2, если вдруг захочешь максимальную скорость ценой качества.

На RTX 3090:

- **Q4_K_M** идеально: 12.5 ГБ под веса + остаётся приличный запас под KV‑кэш и прочий оверхед.
- Реалистичный **контекст 4–8k токенов полностью в VRAM**; можно и больше с частичным offload’ом на RAM, но скорость просядет.
- Скорость генерации в реальности будет порядка **~8–15 токенов/сек** (зависит от настроек, ОС, драйверов), что для одного пользователя комфортно.

---

## 3. Что бы я выбрал на твоём железе

Если цель — **один пользователь, 3090, комфортно работать в интерактиве**, я бы делал так:

### Вариант 1 (рекомендуемый): GGUF + llama.cpp

1. Ставишь `llama.cpp` с CUDA:
   ```bash
   git clone https://github.com/ggerganov/llama.cpp.git
   cd llama.cpp
   make -j$(nproc) LLAMA_CUBLAS=1
   ```

2. Качаешь модель (Q4_K_M) из GGUF‑репо:
   ```bash
   mkdir -p ./models/gigachat20b
   cd ./models/gigachat20b

   huggingface-cli download ai-sage/GigaChat-20B-A3B-instruct-v1.5-GGUF \
     --include "GigaChat-20B-A3B-instruct-v1.5-q4_K_M.gguf" \
     --local-dir .
   ```

3. Тестовый запуск:
   ```bash
   cd ~/llama.cpp

   ./main \
     -m ./models/gigachat20b/GigaChat-20B-A3B-instruct-v1.5-q4_K_M.gguf \
     -c 4096 \
     -n 256 \
     -p "Ты технический ассистент. Объясни, как устроен Kubernetes deployment."
   ```

4. Поднимаешь HTTP‑сервер:
   ```bash
   ./server \
     -m ./models/gigachat20b/GigaChat-20B-A3B-instruct-v1.5-q4_K_M.gguf \
     -c 8192 \
     --host 0.0.0.0 \
     --port 8000
   ```

5. Сверху можешь уже подвесить:
   - Open WebUI / text-generation-webui как UI,
   - или свой микро‑backend (FastAPI), который ходит в `llama.cpp`‑сервер.

Плюсы:

- Достаточно шустро для 1 человека.
- Простая эксплуатация (один бинарник + один файл модели).
- Instruct‑поведение и чат‑шаблон уже вшиты в GGUF (llama.cpp его умеет читать).

### Вариант 2: transformers + int8

Если хочется именно **через transformers** (для плотной интеграции в Python‑код) и не страшна скорость:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

model_name = "ai-sage/GigaChat-20B-A3B-instruct-v1.5-int8"
tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    device_map="auto",      # часть уйдёт в CPU
)
model.generation_config = GenerationConfig.from_pretrained(model_name)

messages = [{"role": "user", "content": "Проанализируй этот ansible-playbook на ИБ-ошибки"}]
x = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
out = model.generate(x.to(model.device), max_new_tokens=512)
print(tok.decode(out[0][x.shape[1]:], skip_special_tokens=False))
```

Но тут:

- будет **заметно медленнее**, чем GGUF + llama.cpp;
- VRAM будет забита почти под завязку, придётся следить за `max_new_tokens` и контекстом.

---

## 4. Сравнение: GigaChat‑20B vs ruGPT‑3.5‑13B на 3090

**Когда брать GigaChat‑20B-A3B‑instruct‑v1.5 (рекомендован в твоём случае):**

- Нужен «настоящий» ассистент с хорошим следованием инструкциям.
- Много задач на **анализ кода/логов**, проектирование архитектуры, помощь SOC — там она реально умнее.
- Важен русский язык и поддержка очень длинного контекста (даже если на 3090 ты физически будешь использовать не 131k, а, скажем, 8k).

**Когда есть смысл остаться на ruGPT‑3.5‑13B:**

- Очень критична скорость (хочешь условные 20–30 ток/сек и выше).
- Сильно ограничен по VRAM/оперативке.
- Готов мириться с тем, что это не чистый instruct‑ассистент и придётся сильнее «жонглировать» промптами.

Для **одного пользователя с 3090** баланс, по сути, в пользу GigaChat‑20B‑Q4_K_M: скорость в районе **8–15 ток/сек** обычно устраивает, а качество ощутимо выше.

---

Если хочешь, дальше могу:

- набросать конкретный `docker-compose.yml` под связку `llama.cpp server + Open WebUI` именно с GigaChat‑20B‑Q4_K_M,  
- или помочь подобрать настройки (контекст, температуры, top_k/p) именно под задачи ИТ/ИБ, чтобы она меньше «галлюцинировала» и лучше соблюдала формат ответов.
