Это **отличный выбор**. Если у вас есть доступ к `GigaChat-20B-A3B-instruct-v1.5`, это меняет ситуацию в корне.

По сравнению с устаревшей `ruGPT-3.5`:
1.  **Это Instruct-модель:** Она понимает команды ("Проанализируй", "Напиши скрипт", "Найди ошибку"), а не просто продолжает текст.
2.  **Актуальность:** Это свежая модель (2024/2025), обученная на огромном массиве русского корпоративного и технического текста.
3.  **Логика:** 20 миллиардов параметров (20B) — это "золотая середина". Она значительно умнее моделей 7B/8B (Llama 3, Mistral), но всё еще может работать на одной видеокарте (в сжатом виде).

### Поместится ли она на RTX 3090 (24 GB)?

В исходном виде (FP16) модель весит около **40 ГБ**. Она **не поместится**.
Однако, мы будем использовать квантование (сжатие) в формат **GGUF**.

| Формат сжатия | Размер модели (прибл.) | Место под контекст (из 24 ГБ) | Рекомендация |
| :--- | :--- | :--- | :--- |
| **Q4_K_M** (4-бит) | **~12-13 ГБ** | ~11 ГБ (Отлично) | **Лучший выбор.** Потери качества минимальны (<2%), влезает длинный контекст (8k+). |
| **Q5_K_M** (5-бит) | **~14-15 ГБ** | ~9 ГБ (Хорошо) | Можно попробовать, если контекст диалогов не очень длинный. |
| **Q6_K** (6-бит) | **~17 ГБ** | ~6 ГБ (Рискованно) | Может вылететь OOM (Out Of Memory) при длинных запросах RAG. |

**Вердикт:** Для задач RAG (где нужно загружать документы в память) берите версию **Q4_K_M**.

---

### Обновленный стек и инструкция по запуску

Стек остается прежним (**Ollama + Open WebUI**), так как он лучше всего работает с GGUF файлами на одной карте. vLLM с квантованием AWQ/GPTQ для 20B модели на 24ГБ памяти будет настроить сложнее (проблемы с фрагментацией памяти).

#### 1. Где взять модель (GGUF)
Поскольку вы используете локальную инфраструктуру, вам нужно скачать файл `.gguf`.
Ищите на HuggingFace по запросу: `GigaChat-20B-A3B-instruct-v1.5 GGUF`.
*Скорее всего, сообщество (например, mradermacher или подобные) уже сконвертировало её. Если нет — придется сконвертировать самостоятельно скриптом `llama.cpp` (но обычно GGUF появляется через пару дней после релиза).*

Предположим, вы скачали файл: `GigaChat-20B-A3B-instruct-v1.5.Q4_K_M.gguf`.

#### 2. Настройка Modelfile (Критически важно!)
GigaChat использует специфические теги для диалога. Если их не прописать, модель будет "бредить".

Создайте файл `Modelfile` рядом с вашим `docker-compose.yml`:

```dockerfile
# Указываем путь к файлу внутри контейнера (после монтирования)
FROM /root/.ollama/models/GigaChat-20B-A3B-instruct-v1.5.Q4_K_M.gguf

# Настройки генерации
PARAMETER TEMPERATURE 0.6
PARAMETER TOP_P 0.95
# Увеличиваем контекст до 8192 (важно для RAG)
PARAMETER N_CTX 8192

# Шаблон промпта под GigaChat (X-Instruction format)
# Важно проверить документацию конкретной квантованной версии, 
# но обычно для GigaChat v1.5/2 подходит формат, похожий на Llama-3 или специфичный Sber-овский:
# <s><|user|> ... <|model|> ...
TEMPLATE """<|user|>
{{ .System }}
{{ .Prompt }}<|model|>
"""

# Системное сообщение
SYSTEM """Ты — эксперт по IT-инфраструктуре и информационной безопасности. 
Твоя задача — помогать инженерам, писать безопасный код и анализировать логи. 
Отвечай точно, кратко и на русском языке."""
```

#### 3. Запуск в Docker (с правками под GigaChat)

Обновляем `docker-compose.yml`, чтобы пробросить файл модели внутрь.

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
      # ВАЖНО: Кладем скачанный GGUF файл в папку ./models на хосте
      - ./models:/models_host 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always

  # Остальные сервисы (Open-WebUI, Qdrant) без изменений, как в прошлом ответе
  # ...
```

#### 4. Регистрация модели
1.  Положите файл `GigaChat-20B...gguf` в папку `./models` на сервере.
2.  Запустите контейнеры: `docker-compose up -d`.
3.  Зайдите в контейнер: `docker exec -it ollama bash`.
4.  Создайте модель из Modelfile:
    ```bash
    # Создаем Modelfile внутри (или пробрасываем его тоже)
    echo "FROM /models_host/GigaChat-20B-A3B-instruct-v1.5.Q4_K_M.gguf" > Modelfile
    echo "PARAMETER N_CTX 8192" >> Modelfile
    echo 'TEMPLATE """<|user|>{{ .System }} {{ .Prompt }}<|model|>"""' >> Modelfile
    
    ollama create gigachat-20b -f Modelfile
    ```

### Почему это лучшее решение для вас?

1.  **Качество:** GigaChat-20B — это **единственная** открытая российская модель такого размера. Она на голову выше всего, что меньше 10B, и конкурирует с Llama-3-70B в задачах на русском языке.
2.  **Эффективность железа:** Вы утилизируете вашу RTX 3090 на 100%. 12 ГБ займет модель, 8-10 ГБ уйдет под кэш контекста (8000 токенов). Это максимально возможная производительность на одной карте.
3.  **Безопасность:** Полный on-premise, никаких облаков Сбера.

**Резюме:**
Берите **GigaChat-20B** в квантовании **Q4_K_M** (GGUF). Запускайте через **Ollama**. Это будет мощнейший инструмент для IT-отдела из доступных на одной видеокарте.
