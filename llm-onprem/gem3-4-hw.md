Учитывая строгие вводные — **один пользователь**, карта **RTX 3090 (24 GB VRAM)** и обязательное использование модели от **Сбера**, — оптимальный стек будет отличаться от enterprise-решения для 10 человек.

Для одного пользователя и одной карты vLLM будет избыточен (он сложнее в настройке и ест больше памяти под KV-cache). Нам важнее простота запуска и возможность вместить модель в память с максимальным контекстом.

Ниже приведен рекомендованный стек и конкретные настройки.

### 1. Выбор Стека

| Компонент | Рекомендация | Почему |
| :--- | :--- | :--- |
| **Inference Engine** | **Ollama** (на базе `llama.cpp`) | Идеально для 1 GPU. Поддерживает формат **GGUF**, который позволяет сжать модель Сбера без существенной потери качества, оставив место под длинный контекст (документы). vLLM здесь создаст лишний оверхед. |
| **Интерфейс (UI)** | **Open WebUI** | Лучший интерфейс "все-в-одном". Умеет работать с Ollama "из коробки", есть встроенная RAG-система, управление промптами и история. |
| **Векторная БД (RAG)** | **Qdrant** | Быстрее и легче, чем pgvector (PostgreSQL). Работает как отдельный легковесный контейнер. В Open WebUI отличная нативная интеграция с ним. |
| **Модель Embeddings** | **DeepPavlov/rubert** или **multilingual-e5** | Для поиска по русским документам нужна модель, понимающая семантику языка. Родная модель Ollama (nomic) плохо ищет по-русски. |

---

### 2. Выбор модели (Sber AI)

С моделями Сбера есть нюанс: их флагман *GigaChat* закрыт в облаке. В открытом доступе есть семейство **ruGPT-3.5**.

*   **Базовая модель:** `ai-forever/ruGPT-3.5-13B`.
*   **Проблема:** Оригинальная модель весит около 26 ГБ (FP16), что **не поместится** в RTX 3090 (24 ГБ) вместе с контекстом.
*   **Решение:** Использовать квантованную версию (сжатую) в формате **GGUF**.
*   **Конкретный файл:** `ruGPT-3.5-13B-GGUF` (рекомендую квантование **Q5_K_M** или **Q6_K** — это высокое качество, займет ~10-12 ГБ VRAM). Оставшиеся 12 ГБ пойдут под контекст документов.

> **Важное предупреждение:** ruGPT-3.5 — это классическая модель (completion), она хуже понимает инструкции типа "Сделай то-то", чем современные Llama-3 или Qwen. Она склонна продолжать текст, а не отвечать на вопрос.
> *Для IT-задач это может быть критично.* Если политика позволит, рассмотрите `Saiga Llama 3 8B` (она меньше, умнее и отлично говорит по-русски), но если строго Сбер — работаем с ruGPT-3.5.

---

### 3. Техническая реализация (Docker Compose)

Ниже готовая конфигурация `docker-compose.yml`, которая поднимет весь стек на одной машине с RTX 3090.

**Предварительные требования:**
1.  Установлены Docker и Docker Compose.
2.  Установлен **NVIDIA Container Toolkit** (чтобы докер видел видеокарту).

#### Файл `docker-compose.yml`

```yaml
version: '3.8'

services:
  # 1. Движок нейросети (Backend)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always

  # 2. Векторная база данных для RAG
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    restart: always

  # 3. Интерфейс (Frontend + RAG Orchestrator)
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # Настройки RAG
      - RAG_EMBEDDING_ENGINE=ollama 
      # Для русского языка лучше использовать специализированную модель эмбеддингов, см. ниже
      - RAG_VECTOR_DB_TYPE=qdrant
      - RAG_QDRANT_URL=http://qdrant:6333
    volumes:
      - open-webui_storage:/app/backend/data
    depends_on:
      - ollama
      - qdrant
    restart: always

volumes:
  ollama_storage:
  qdrant_storage:
  open-webui_storage:
```

### 4. Инструкция по запуску и настройке

#### Шаг 1: Загрузка модели Сбера
После запуска контейнеров (`docker-compose up -d`), нужно скачать и импортировать модель Сбера в Ollama. Поскольку официальной GGUF версии в библиотеке Ollama может не быть, берем с HuggingFace.

1.  Зайдите внутрь контейнера: `docker exec -it ollama bash`
2.  (Внутри) Установите wget (если нет) и скачайте модель.
    *Лучший вариант взять готовую сборку, например от пользователя "inkoziev" или сконвертировать самому. Допустим, мы нашли GGUF файл.*
    
    *Альтернативный (простой) путь:*
    В мире Ollama модель Сбера часто не лежит в стандартном реестре.
    **Рекомендация:** Используйте модель **Fred-T5** (тоже Сбер) или, если нужен именно GPT, найдите на HuggingFace файл `rugpt3.5-13b.Q5_K_M.gguf`.
    
    Создайте файл `Modelfile` в папке с `docker-compose.yml`:
    ```dockerfile
    FROM ./rugpt3.5-13b.Q5_K_M.gguf
    PARAMETER TEMPERATURE 0.6
    PARAMETER N_CTX 8192
    SYSTEM "Ты умный помощник IT-службы. Отвечай кратко и по делу."
    ```
    
    Затем создайте модель командой:
    `docker exec -it ollama ollama create rugpt-sber -f /path/to/Modelfile` (предварительно пробросив файл внутрь volume).

#### Шаг 2: Настройка Embeddings (Критично для RAG)
Модель `rugpt-3.5` сама по себе не умеет делать векторный поиск.
В интерфейсе Open WebUI (Админка -> Documents -> Embedding Model):
1.  Скачайте модель **sentence-transformers/paraphrase-multilingual-mpnet-base-v2** или **intfloat/multilingual-e5-large**.
2.  Open WebUI позволяет скачать их прямо из интерфейса (используя библиотеку `sentence-transformers` внутри контейнера, CPU будет достаточно для одного юзера).
3.  Это обеспечит качественный поиск по русскоязычной документации.

### 5. Итоговая калькуляция ресурсов (RTX 3090)

*   **Модель (ruGPT-3.5 13B Q5_K):** ~10 GB VRAM.
*   **KV Cache (Контекст 8192 токенов):** ~2-4 GB VRAM.
*   **Операционная система + дисплей:** ~1-2 GB VRAM.
*   **Свободно:** ~8-10 GB VRAM.

**Вывод:** Карты RTX 3090 достаточно с запасом. Система будет работать быстро (около 40-50 токенов/сек).

### Резюме для внедрения
1.  **Железо:** Вашей RTX 3090 — идеально достаточно.
2.  **ПО:** Docker Compose с **Ollama** + **Open WebUI** + **Qdrant**.
3.  **Модель:** Найдите GGUF версию **ruGPT-3.5 13B** (на HuggingFace ищите по запросу "rugpt3.5 gguf"). Не берите FP16 версию, она не влезет.
4.  **RAG:** Обязательно смените дефолтную embedding-модель в настройках Open WebUI на мультиязычную (`paraphrase-multilingual`), иначе поиск по базе знаний будет работать плохо.
